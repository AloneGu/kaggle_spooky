{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process  however  afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.487805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box  from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else  not even gold  the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process  however  afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box  from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else  not even gold  the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             25                 0   \n",
       "1         14                14         71             10                 0   \n",
       "2         36                32        200             17                 0   \n",
       "3         34                32        206             14                 0   \n",
       "4         27                25        174             14                 0   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.487805  \n",
       "1                0                1       4.071429  \n",
       "2                0                1       4.444444  \n",
       "3                0                4       4.970588  \n",
       "4                0                2       5.333333  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 550853) (8392, 550853)\n",
      "(19579, 20) (8392, 20)\n",
      "(19579, 164797) (8392, 164797)\n",
      "(19579, 20) (8392, 20)\n",
      "(19579, 550853) (8392, 550853)\n",
      "(19579, 1138259) (8392, 1138259)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_df=0.8)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(stop_words='english', ngram_range=(1,5), analyzer='char',max_df=0.8)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(stop_words='english',ngram_range=(1,3),max_df=0.8)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(stop_words='english',ngram_range=(1,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 8) (8392, 8)\n",
      "(19579, 48) (8392, 48)\n"
     ]
    }
   ],
   "source": [
    "# add tfidf to svd\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "print(train_X.shape, test_X.shape)\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2])\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 12) (8392, 12)\n",
      "[[ 0.673  0.151  0.176  0.99   0.001  0.008  1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.384  0.415  0.201  0.46   0.429  0.112  0.218  0.766  0.016  1.     0.\n",
      "   0.   ]\n",
      " [ 0.768  0.167  0.065  0.938  0.062  0.001  1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.072  0.072  0.856  0.     0.005  0.995  0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.379  0.422  0.2    0.781  0.184  0.035  0.25   0.748  0.002  1.     0.\n",
      "   0.   ]]\n",
      "[[ 0.246  0.13   0.624  0.011  0.002  0.987  0.001  0.     0.999  0.     0.\n",
      "   1.   ]\n",
      " [ 0.522  0.238  0.24   0.963  0.025  0.012  0.999  0.     0.001  1.     0.\n",
      "   0.   ]\n",
      " [ 0.339  0.517  0.144  0.033  0.967  0.001  0.03   0.97   0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.485  0.432  0.083  0.119  0.878  0.003  0.775  0.225  0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.588  0.247  0.165  0.712  0.059  0.229  0.967  0.027  0.005  0.993  0.\n",
      "   0.007]]\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=2017)\n",
    "for train_index, test_index in kf.split(train_tfidf):\n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    tmp_model = MultinomialNB(alpha=0.09,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "    help_tfidf_train[test_index] = tmp_train_feat\n",
    "    help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec to nb\n",
    "    X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "    tmp_model = MultinomialNB(0.09,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "    help_tfidf_train2[test_index] = tmp_train_feat\n",
    "    help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec to nb\n",
    "    X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "    tmp_model = MultinomialNB(0.09,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "    help_cnt1_train[test_index] = tmp_train_feat\n",
    "    help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec2 to nb \n",
    "    X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "    tmp_model = MultinomialNB(0.09,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "    help_cnt2_train[test_index] = tmp_train_feat\n",
    "    help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "help_train_feat = np.round(np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train]),3)\n",
    "help_test_feat = np.round(np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test]),3)\n",
    "\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "print(help_train_feat[:5])\n",
    "print(help_test_feat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 60) (8392, 60)\n"
     ]
    }
   ],
   "source": [
    "f_train_X = np.hstack([train_X,help_train_feat])\n",
    "f_test_X = np.hstack([test_X,help_test_feat])\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "def cv_test(k_cnt=3):\n",
    "    kf = KFold(n_splits=k_cnt, shuffle=True, random_state=42)\n",
    "    test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.1,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 1000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=50)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        \n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            test_pred += m.predict(d_test)\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}.csv\".format(k_cnt),index=False)\n",
    "    submiss.head(5)\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.999649\tvalid-mlogloss:1.00208\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.314006\tvalid-mlogloss:0.352407\n",
      "[100]\ttrain-mlogloss:0.277163\tvalid-mlogloss:0.340662\n",
      "[150]\ttrain-mlogloss:0.253733\tvalid-mlogloss:0.337905\n",
      "[200]\ttrain-mlogloss:0.234811\tvalid-mlogloss:0.336344\n",
      "[250]\ttrain-mlogloss:0.21825\tvalid-mlogloss:0.336483\n",
      "Stopping. Best iteration:\n",
      "[216]\ttrain-mlogloss:0.229244\tvalid-mlogloss:0.336252\n",
      "\n",
      "train log loss 0.213266469679 valid log loss 0.336685946195\n",
      "[0]\ttrain-mlogloss:0.999656\tvalid-mlogloss:0.999829\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.32167\tvalid-mlogloss:0.337108\n",
      "[100]\ttrain-mlogloss:0.285866\tvalid-mlogloss:0.323819\n",
      "[150]\ttrain-mlogloss:0.261959\tvalid-mlogloss:0.3202\n",
      "[200]\ttrain-mlogloss:0.242093\tvalid-mlogloss:0.319793\n",
      "[250]\ttrain-mlogloss:0.224277\tvalid-mlogloss:0.31973\n",
      "Stopping. Best iteration:\n",
      "[214]\ttrain-mlogloss:0.236797\tvalid-mlogloss:0.319466\n",
      "\n",
      "train log loss 0.219431200787 valid log loss 0.319820354577\n",
      "[0]\ttrain-mlogloss:1.00066\tvalid-mlogloss:0.999902\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.32373\tvalid-mlogloss:0.333884\n",
      "[100]\ttrain-mlogloss:0.287053\tvalid-mlogloss:0.321206\n",
      "[150]\ttrain-mlogloss:0.262062\tvalid-mlogloss:0.318757\n",
      "[200]\ttrain-mlogloss:0.242474\tvalid-mlogloss:0.318139\n",
      "[250]\ttrain-mlogloss:0.224379\tvalid-mlogloss:0.317822\n",
      "Stopping. Best iteration:\n",
      "[235]\ttrain-mlogloss:0.229433\tvalid-mlogloss:0.317536\n",
      "\n",
      "train log loss 0.213352771972 valid log loss 0.317696870687\n",
      "local average valid loss 0.324734390486\n",
      "train log loss 0.240219154291\n"
     ]
    }
   ],
   "source": [
    "cv_test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.999204\tvalid-mlogloss:1.00173\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.317775\tvalid-mlogloss:0.359228\n",
      "[100]\ttrain-mlogloss:0.284152\tvalid-mlogloss:0.34701\n",
      "[150]\ttrain-mlogloss:0.262571\tvalid-mlogloss:0.343418\n",
      "[200]\ttrain-mlogloss:0.245566\tvalid-mlogloss:0.342493\n",
      "[250]\ttrain-mlogloss:0.230436\tvalid-mlogloss:0.341858\n",
      "[300]\ttrain-mlogloss:0.215756\tvalid-mlogloss:0.342474\n",
      "Stopping. Best iteration:\n",
      "[250]\ttrain-mlogloss:0.230436\tvalid-mlogloss:0.341858\n",
      "\n",
      "train log loss 0.215756238041 valid log loss 0.342473591875\n",
      "[0]\ttrain-mlogloss:1.00003\tvalid-mlogloss:1.00015\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.323704\tvalid-mlogloss:0.338617\n",
      "[100]\ttrain-mlogloss:0.290002\tvalid-mlogloss:0.323899\n",
      "[150]\ttrain-mlogloss:0.267635\tvalid-mlogloss:0.320347\n",
      "[200]\ttrain-mlogloss:0.250481\tvalid-mlogloss:0.318672\n",
      "[250]\ttrain-mlogloss:0.235376\tvalid-mlogloss:0.318711\n",
      "Stopping. Best iteration:\n",
      "[206]\ttrain-mlogloss:0.2485\tvalid-mlogloss:0.318382\n",
      "\n",
      "train log loss 0.23345629441 valid log loss 0.318969718494\n",
      "[0]\ttrain-mlogloss:1.00009\tvalid-mlogloss:0.99959\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.324515\tvalid-mlogloss:0.331504\n",
      "[100]\ttrain-mlogloss:0.290964\tvalid-mlogloss:0.319039\n",
      "[150]\ttrain-mlogloss:0.269459\tvalid-mlogloss:0.31635\n",
      "[200]\ttrain-mlogloss:0.251096\tvalid-mlogloss:0.315883\n",
      "[250]\ttrain-mlogloss:0.235124\tvalid-mlogloss:0.316643\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-mlogloss:0.249749\tvalid-mlogloss:0.315779\n",
      "\n",
      "train log loss 0.233966752823 valid log loss 0.316599607947\n",
      "[0]\ttrain-mlogloss:0.999875\tvalid-mlogloss:1.00015\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.324152\tvalid-mlogloss:0.336187\n",
      "[100]\ttrain-mlogloss:0.291006\tvalid-mlogloss:0.322693\n",
      "[150]\ttrain-mlogloss:0.269258\tvalid-mlogloss:0.319247\n",
      "[200]\ttrain-mlogloss:0.251705\tvalid-mlogloss:0.318153\n",
      "[250]\ttrain-mlogloss:0.235703\tvalid-mlogloss:0.318806\n",
      "Stopping. Best iteration:\n",
      "[202]\ttrain-mlogloss:0.251024\tvalid-mlogloss:0.318001\n",
      "\n",
      "train log loss 0.235143135466 valid log loss 0.318770920632\n",
      "[0]\ttrain-mlogloss:1.00044\tvalid-mlogloss:1.00069\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.323282\tvalid-mlogloss:0.341483\n",
      "[100]\ttrain-mlogloss:0.288644\tvalid-mlogloss:0.329845\n",
      "[150]\ttrain-mlogloss:0.266655\tvalid-mlogloss:0.328054\n",
      "[200]\ttrain-mlogloss:0.248759\tvalid-mlogloss:0.328855\n",
      "Stopping. Best iteration:\n",
      "[185]\ttrain-mlogloss:0.254042\tvalid-mlogloss:0.327989\n",
      "\n",
      "train log loss 0.237703207938 valid log loss 0.3289464113\n",
      "local average valid loss 0.32515205005\n",
      "train log loss 0.243305026857\n"
     ]
    }
   ],
   "source": [
    "cv_test(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.999271\tvalid-mlogloss:1.0017\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-mlogloss:0.321969\tvalid-mlogloss:0.362134\n",
      "[100]\ttrain-mlogloss:0.290146\tvalid-mlogloss:0.348652\n",
      "[150]\ttrain-mlogloss:0.270389\tvalid-mlogloss:0.345614\n",
      "[200]\ttrain-mlogloss:0.254231\tvalid-mlogloss:0.343519\n"
     ]
    }
   ],
   "source": [
    "cv_test(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
