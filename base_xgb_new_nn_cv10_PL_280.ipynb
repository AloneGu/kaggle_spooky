{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:26<00:00,  1.03s/it]\n",
      "100%|██████████| 26/26 [01:05<00:00,  2.54s/it]\n",
      "100%|██████████| 26/26 [07:46<00:00, 17.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:11<00:00,  2.34it/s]\n",
      "100%|██████████| 26/26 [00:26<00:00,  1.02s/it]\n",
      "100%|██████████| 26/26 [03:42<00:00,  8.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 26685) (8392, 26685)\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(in_df, train_flag=False):\n",
    "    df = in_df.copy()\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "    \n",
    "    # Find numbers of different combinations\n",
    "    for c in tqdm(alphabet.upper()):\n",
    "        df['n_' + c] = df['text'].str.count(c)\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "        df['n_' + c + '?'] = df['text'].str.count(c + '\\?')\n",
    "        df['n_' + c + ';'] = df['text'].str.count(c + '\\;')\n",
    "        df['n_' + c + ':'] = df['text'].str.count(c + '\\:')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "            df['n_' + c + c2 + '?'] = df['text'].str.count(c + c2 + '\\?')\n",
    "            df['n_' + c + c2 + ';'] = df['text'].str.count(c + c2 + '\\;')\n",
    "            df['n_' + c + c2 + ':'] = df['text'].str.count(c + c2 + '\\:')\n",
    "            df['n_' + c + ', ' + c2] = df['text'].str.count(c + '\\, ' + c2)\n",
    "\n",
    "    # And now starting processing of cleaned text\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c] = df['text_cleaned'].str.count(c)\n",
    "        df['n_' + c + ' '] = df['text_cleaned'].str.count(c + ' ')\n",
    "        df['n_' + ' ' + c] = df['text_cleaned'].str.count(' ' + c)\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text_cleaned'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + ' '] = df['text_cleaned'].str.count(c + c2 + ' ')\n",
    "            df['n_' + ' ' + c + c2] = df['text_cleaned'].str.count(' ' + c + c2)\n",
    "            df['n_' + c + ' ' + c2] = df['text_cleaned'].str.count(c + ' ' + c2)\n",
    "\n",
    "            for c3 in alphabet:\n",
    "                df['n_' + c + c2 + c3] = df['text_cleaned'].str.count(c + c2 + c3)\n",
    "                \n",
    "    if train_flag:\n",
    "        df.drop(['text_cleaned','text','author','id'], axis=1, inplace=True)\n",
    "    else:\n",
    "        df.drop(['text_cleaned','text','id'], axis=1, inplace=True)\n",
    "    return df.values\n",
    "    \n",
    "print('Processing train...')\n",
    "train_hand_features = extract_features(train_df,train_flag=True)\n",
    "print('Processing test...')\n",
    "test_hand_features = extract_features(test_df)\n",
    "print(train_hand_features.shape,test_hand_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tag_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = [a[1] for a in nltk.pos_tag(words)]\n",
    "        res.append(' '.join(tag_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "def ne_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = nltk.pos_tag(words)\n",
    "        ne_tree = nltk.ne_chunk(tag_res)\n",
    "        list_res = nltk.tree2conlltags(ne_tree)\n",
    "        ne_res = [a[2] for a in list_res]\n",
    "        res.append(' '.join(ne_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "train_df['ne_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['ne_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "\n",
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False)\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False)\n",
    "c_vec4.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec6.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "full_cvec1 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "## add svd3\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_cvec1)\n",
    "train_svd3 = pd.DataFrame(svd_obj.transform(train_cvec))\n",
    "test_svd3 = pd.DataFrame(svd_obj.transform(test_cvec))\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "full_cvec2 = c_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "## add svd4\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_cvec2)\n",
    "train_svd4 = pd.DataFrame(svd_obj.transform(train_cvec2))\n",
    "test_svd4 = pd.DataFrame(svd_obj.transform(test_cvec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 15) (8392, 15)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 10\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "        \n",
    "        # hand feature to nb\n",
    "        X_train, X_test = train_hand_features[train_index], train_hand_features[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_hand_features)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train,hand_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test,hand_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture, GaussianMixture\n",
    "\n",
    "tmp_train_svd = np.hstack([train_svd,train_svd2,train_svd3,train_svd4])\n",
    "tmp_test_svd = np.hstack([test_svd,test_svd2,test_svd3,test_svd4])\n",
    "\n",
    "# svd to generative models\n",
    "\n",
    "def gen_by_gauss_feats(rnd=1):\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(tmp_train_svd):\n",
    "        # hand feature \n",
    "        X_train, X_test = tmp_train_svd[train_index], tmp_train_svd[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = BayesianGaussianMixture()\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(tmp_test_svd)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    return hand_train,hand_test\n",
    "\n",
    "help_train_feat4,help_test_feat4 = gen_by_gauss_feats(1)\n",
    "\n",
    "def gen_gauss_feats(rnd=1):\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(tmp_train_svd):\n",
    "        # hand feature \n",
    "        X_train, X_test = tmp_train_svd[train_index], tmp_train_svd[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = GaussianMixture()\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(tmp_test_svd)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    return hand_train,hand_test\n",
    "\n",
    "help_train_feat5,help_test_feat5 = gen_gauss_feats(1)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07162, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0856 - acc: 0.4025 - val_loss: 1.0716 - val_acc: 0.4123\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07162 to 0.97302, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0343 - acc: 0.4725 - val_loss: 0.9730 - val_acc: 0.5566\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.97302 to 0.78180, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8661 - acc: 0.6371 - val_loss: 0.7818 - val_acc: 0.6709\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.78180 to 0.62881, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6565 - acc: 0.7671 - val_loss: 0.6288 - val_acc: 0.7539\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.62881 to 0.54117, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4993 - acc: 0.8314 - val_loss: 0.5412 - val_acc: 0.7870\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54117 to 0.49098, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3942 - acc: 0.8688 - val_loss: 0.4910 - val_acc: 0.8042\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49098 to 0.45911, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3198 - acc: 0.8961 - val_loss: 0.4591 - val_acc: 0.8162\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45911 to 0.44144, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2660 - acc: 0.9181 - val_loss: 0.4414 - val_acc: 0.8213\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44144 to 0.43239, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2233 - acc: 0.9311 - val_loss: 0.4324 - val_acc: 0.8239\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43239 to 0.43198, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1906 - acc: 0.9461 - val_loss: 0.4320 - val_acc: 0.8249\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43198 to 0.42700, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1639 - acc: 0.9544 - val_loss: 0.4270 - val_acc: 0.8294\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1431 - acc: 0.9603 - val_loss: 0.4300 - val_acc: 0.8315\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1240 - acc: 0.9656 - val_loss: 0.4326 - val_acc: 0.8330\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1082 - acc: 0.9717 - val_loss: 0.4357 - val_acc: 0.8345\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0940 - acc: 0.9771 - val_loss: 0.4431 - val_acc: 0.8358\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0841 - acc: 0.9792 - val_loss: 0.4527 - val_acc: 0.8343\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0751 - acc: 0.9814 - val_loss: 0.4596 - val_acc: 0.8332\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0667 - acc: 0.9838 - val_loss: 0.4707 - val_acc: 0.8311\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0589 - acc: 0.9856 - val_loss: 0.4808 - val_acc: 0.8324\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0525 - acc: 0.9886 - val_loss: 0.4900 - val_acc: 0.8317\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07117, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0840 - acc: 0.4045 - val_loss: 1.0712 - val_acc: 0.4053\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07117 to 0.93728, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0165 - acc: 0.4766 - val_loss: 0.9373 - val_acc: 0.5638\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.93728 to 0.75857, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8273 - acc: 0.6627 - val_loss: 0.7586 - val_acc: 0.6938\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.75857 to 0.61735, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6365 - acc: 0.7784 - val_loss: 0.6174 - val_acc: 0.7694\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61735 to 0.53436, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4893 - acc: 0.8370 - val_loss: 0.5344 - val_acc: 0.7938\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.53436 to 0.48711, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3918 - acc: 0.8719 - val_loss: 0.4871 - val_acc: 0.8107\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48711 to 0.45947, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3232 - acc: 0.8960 - val_loss: 0.4595 - val_acc: 0.8179\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45947 to 0.44225, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2696 - acc: 0.9167 - val_loss: 0.4422 - val_acc: 0.8213\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44225 to 0.43280, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2270 - acc: 0.9304 - val_loss: 0.4328 - val_acc: 0.8239\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43280 to 0.42696, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1934 - acc: 0.9433 - val_loss: 0.4270 - val_acc: 0.8284\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1659 - acc: 0.9521 - val_loss: 0.4291 - val_acc: 0.8294\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1428 - acc: 0.9595 - val_loss: 0.4299 - val_acc: 0.8309\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1240 - acc: 0.9673 - val_loss: 0.4347 - val_acc: 0.8334\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1101 - acc: 0.9698 - val_loss: 0.4413 - val_acc: 0.8313\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0953 - acc: 0.9749 - val_loss: 0.4457 - val_acc: 0.8324\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0854 - acc: 0.9771 - val_loss: 0.4554 - val_acc: 0.8339\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0744 - acc: 0.9809 - val_loss: 0.4597 - val_acc: 0.8328\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0656 - acc: 0.9835 - val_loss: 0.4716 - val_acc: 0.8328\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0584 - acc: 0.9851 - val_loss: 0.4842 - val_acc: 0.8311\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0525 - acc: 0.9863 - val_loss: 0.4973 - val_acc: 0.8309\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07251, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0851 - acc: 0.4020 - val_loss: 1.0725 - val_acc: 0.4065\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07251 to 0.95271, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0270 - acc: 0.4639 - val_loss: 0.9527 - val_acc: 0.5519\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95271 to 0.75523, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8368 - acc: 0.6723 - val_loss: 0.7552 - val_acc: 0.7085\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.75523 to 0.61308, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6288 - acc: 0.7898 - val_loss: 0.6131 - val_acc: 0.7630\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61308 to 0.53376, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4845 - acc: 0.8421 - val_loss: 0.5338 - val_acc: 0.7918\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.53376 to 0.48623, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3882 - acc: 0.8722 - val_loss: 0.4862 - val_acc: 0.8088\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48623 to 0.45820, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3198 - acc: 0.8943 - val_loss: 0.4582 - val_acc: 0.8154\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45820 to 0.43996, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2657 - acc: 0.9132 - val_loss: 0.4400 - val_acc: 0.8216\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43996 to 0.42749, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2237 - acc: 0.9299 - val_loss: 0.4275 - val_acc: 0.8254\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1922 - acc: 0.9441 - val_loss: 0.4289 - val_acc: 0.8296\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42749 to 0.42496, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1646 - acc: 0.9513 - val_loss: 0.4250 - val_acc: 0.8326\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1437 - acc: 0.9596 - val_loss: 0.4312 - val_acc: 0.8315\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1260 - acc: 0.9657 - val_loss: 0.4377 - val_acc: 0.8309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1111 - acc: 0.9702 - val_loss: 0.4426 - val_acc: 0.8309\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0967 - acc: 0.9752 - val_loss: 0.4517 - val_acc: 0.8320\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0851 - acc: 0.9783 - val_loss: 0.4607 - val_acc: 0.8313\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0758 - acc: 0.9809 - val_loss: 0.4707 - val_acc: 0.8326\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0676 - acc: 0.9841 - val_loss: 0.4827 - val_acc: 0.8317\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0585 - acc: 0.9865 - val_loss: 0.4978 - val_acc: 0.8313\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0534 - acc: 0.9874 - val_loss: 0.5156 - val_acc: 0.8260\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07214, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0846 - acc: 0.4037 - val_loss: 1.0721 - val_acc: 0.4084\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07214 to 0.94369, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0237 - acc: 0.4676 - val_loss: 0.9437 - val_acc: 0.5686\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94369 to 0.74411, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8224 - acc: 0.6658 - val_loss: 0.7441 - val_acc: 0.7140\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74411 to 0.60220, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6161 - acc: 0.7882 - val_loss: 0.6022 - val_acc: 0.7677\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60220 to 0.52296, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4665 - acc: 0.8448 - val_loss: 0.5230 - val_acc: 0.7997\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52296 to 0.47732, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3680 - acc: 0.8814 - val_loss: 0.4773 - val_acc: 0.8137\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47732 to 0.44893, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2993 - acc: 0.9048 - val_loss: 0.4489 - val_acc: 0.8213\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44893 to 0.43541, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2491 - acc: 0.9192 - val_loss: 0.4354 - val_acc: 0.8245\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43541 to 0.42834, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2083 - acc: 0.9373 - val_loss: 0.4283 - val_acc: 0.8262\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42834 to 0.42579, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1771 - acc: 0.9485 - val_loss: 0.4258 - val_acc: 0.8303\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1513 - acc: 0.9559 - val_loss: 0.4259 - val_acc: 0.8309\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1303 - acc: 0.9647 - val_loss: 0.4313 - val_acc: 0.8311\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1145 - acc: 0.9681 - val_loss: 0.4332 - val_acc: 0.8334\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0998 - acc: 0.9735 - val_loss: 0.4455 - val_acc: 0.8307\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0873 - acc: 0.9779 - val_loss: 0.4513 - val_acc: 0.8305\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0765 - acc: 0.9825 - val_loss: 0.4655 - val_acc: 0.8303\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0685 - acc: 0.9821 - val_loss: 0.4746 - val_acc: 0.8307\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0603 - acc: 0.9846 - val_loss: 0.4909 - val_acc: 0.8292\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0529 - acc: 0.9870 - val_loss: 0.4961 - val_acc: 0.8315\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0475 - acc: 0.9877 - val_loss: 0.5173 - val_acc: 0.8283\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.06895, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0847 - acc: 0.4005 - val_loss: 1.0690 - val_acc: 0.4129\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.06895 to 0.93666, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0130 - acc: 0.4806 - val_loss: 0.9367 - val_acc: 0.5678\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.93666 to 0.76236, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8248 - acc: 0.6635 - val_loss: 0.7624 - val_acc: 0.6909\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.76236 to 0.62281, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6366 - acc: 0.7836 - val_loss: 0.6228 - val_acc: 0.7677\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.62281 to 0.54098, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4951 - acc: 0.8383 - val_loss: 0.5410 - val_acc: 0.7946\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54098 to 0.49380, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3951 - acc: 0.8701 - val_loss: 0.4938 - val_acc: 0.8071\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49380 to 0.46657, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3233 - acc: 0.8954 - val_loss: 0.4666 - val_acc: 0.8152\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46657 to 0.44688, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2719 - acc: 0.9137 - val_loss: 0.4469 - val_acc: 0.8233\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44688 to 0.43878, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2289 - acc: 0.9294 - val_loss: 0.4388 - val_acc: 0.8266\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43878 to 0.43136, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1947 - acc: 0.9438 - val_loss: 0.4314 - val_acc: 0.8283\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1656 - acc: 0.9514 - val_loss: 0.4318 - val_acc: 0.8296\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1469 - acc: 0.9574 - val_loss: 0.4368 - val_acc: 0.8301\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1294 - acc: 0.9645 - val_loss: 0.4361 - val_acc: 0.8324\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1113 - acc: 0.9693 - val_loss: 0.4445 - val_acc: 0.8334\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0975 - acc: 0.9744 - val_loss: 0.4536 - val_acc: 0.8334\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0866 - acc: 0.9771 - val_loss: 0.4653 - val_acc: 0.8326\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0760 - acc: 0.9811 - val_loss: 0.4719 - val_acc: 0.8356\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0675 - acc: 0.9844 - val_loss: 0.4889 - val_acc: 0.8337\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0606 - acc: 0.9860 - val_loss: 0.4982 - val_acc: 0.8336\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0540 - acc: 0.9869 - val_loss: 0.5152 - val_acc: 0.8311\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07400, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0851 - acc: 0.4027 - val_loss: 1.0740 - val_acc: 0.4034\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07400 to 0.94625, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0239 - acc: 0.4691 - val_loss: 0.9463 - val_acc: 0.5627\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94625 to 0.73870, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8232 - acc: 0.6768 - val_loss: 0.7387 - val_acc: 0.7140\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.73870 to 0.59602, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6016 - acc: 0.7963 - val_loss: 0.5960 - val_acc: 0.7691\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.59602 to 0.52123, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4584 - acc: 0.8468 - val_loss: 0.5212 - val_acc: 0.7986\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52123 to 0.47981, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3640 - acc: 0.8780 - val_loss: 0.4798 - val_acc: 0.8129\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47981 to 0.45059, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2986 - acc: 0.9045 - val_loss: 0.4506 - val_acc: 0.8194\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45059 to 0.43222, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2510 - acc: 0.9210 - val_loss: 0.4322 - val_acc: 0.8254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43222 to 0.42652, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2115 - acc: 0.9345 - val_loss: 0.4265 - val_acc: 0.8273\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42652 to 0.42287, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1787 - acc: 0.9477 - val_loss: 0.4229 - val_acc: 0.8300\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1529 - acc: 0.9569 - val_loss: 0.4234 - val_acc: 0.8315\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1337 - acc: 0.9630 - val_loss: 0.4265 - val_acc: 0.8332\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1156 - acc: 0.9690 - val_loss: 0.4306 - val_acc: 0.8336\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1010 - acc: 0.9736 - val_loss: 0.4451 - val_acc: 0.8322\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0893 - acc: 0.9773 - val_loss: 0.4496 - val_acc: 0.8326\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0790 - acc: 0.9803 - val_loss: 0.4616 - val_acc: 0.8324\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0692 - acc: 0.9838 - val_loss: 0.4664 - val_acc: 0.8320\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0621 - acc: 0.9837 - val_loss: 0.4826 - val_acc: 0.8313\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0552 - acc: 0.9869 - val_loss: 0.4927 - val_acc: 0.8311\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0492 - acc: 0.9882 - val_loss: 0.5052 - val_acc: 0.8301\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07264, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0852 - acc: 0.4015 - val_loss: 1.0726 - val_acc: 0.4067\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07264 to 0.94177, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0213 - acc: 0.4735 - val_loss: 0.9418 - val_acc: 0.5563\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94177 to 0.74705, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8260 - acc: 0.6630 - val_loss: 0.7471 - val_acc: 0.7049\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74705 to 0.60449, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6187 - acc: 0.7939 - val_loss: 0.6045 - val_acc: 0.7691\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60449 to 0.52740, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4712 - acc: 0.8433 - val_loss: 0.5274 - val_acc: 0.7946\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52740 to 0.48209, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3753 - acc: 0.8768 - val_loss: 0.4821 - val_acc: 0.8076\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48209 to 0.45628, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3083 - acc: 0.9006 - val_loss: 0.4563 - val_acc: 0.8150\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45628 to 0.44059, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2556 - acc: 0.9186 - val_loss: 0.4406 - val_acc: 0.8171\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44059 to 0.43510, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2169 - acc: 0.9337 - val_loss: 0.4351 - val_acc: 0.8201\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43510 to 0.43075, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1823 - acc: 0.9484 - val_loss: 0.4308 - val_acc: 0.8222\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43075 to 0.42943, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1584 - acc: 0.9539 - val_loss: 0.4294 - val_acc: 0.8252\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1364 - acc: 0.9623 - val_loss: 0.4379 - val_acc: 0.8247\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1204 - acc: 0.9665 - val_loss: 0.4392 - val_acc: 0.8271\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1043 - acc: 0.9738 - val_loss: 0.4449 - val_acc: 0.8269\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0921 - acc: 0.9772 - val_loss: 0.4559 - val_acc: 0.8256\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0812 - acc: 0.9803 - val_loss: 0.4623 - val_acc: 0.8288\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0716 - acc: 0.9823 - val_loss: 0.4765 - val_acc: 0.8283\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0636 - acc: 0.9844 - val_loss: 0.4869 - val_acc: 0.8283\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0573 - acc: 0.9865 - val_loss: 0.4989 - val_acc: 0.8313\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0501 - acc: 0.9892 - val_loss: 0.5163 - val_acc: 0.8286\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07480, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0854 - acc: 0.4024 - val_loss: 1.0748 - val_acc: 0.4019\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07480 to 0.95453, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0287 - acc: 0.4611 - val_loss: 0.9545 - val_acc: 0.5591\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95453 to 0.74629, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8335 - acc: 0.6641 - val_loss: 0.7463 - val_acc: 0.7140\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74629 to 0.60252, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6193 - acc: 0.7952 - val_loss: 0.6025 - val_acc: 0.7700\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60252 to 0.52632, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4733 - acc: 0.8431 - val_loss: 0.5263 - val_acc: 0.7935\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52632 to 0.48381, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3804 - acc: 0.8717 - val_loss: 0.4838 - val_acc: 0.8093\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48381 to 0.45479, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3118 - acc: 0.8987 - val_loss: 0.4548 - val_acc: 0.8194\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45479 to 0.43728, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2585 - acc: 0.9195 - val_loss: 0.4373 - val_acc: 0.8250\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43728 to 0.42887, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2206 - acc: 0.9306 - val_loss: 0.4289 - val_acc: 0.8275\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42887 to 0.42629, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1847 - acc: 0.9450 - val_loss: 0.4263 - val_acc: 0.8294\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42629 to 0.42534, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1592 - acc: 0.9548 - val_loss: 0.4253 - val_acc: 0.8328\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1368 - acc: 0.9632 - val_loss: 0.4277 - val_acc: 0.8347\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1192 - acc: 0.9684 - val_loss: 0.4309 - val_acc: 0.8330\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1035 - acc: 0.9722 - val_loss: 0.4352 - val_acc: 0.8330\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0914 - acc: 0.9761 - val_loss: 0.4436 - val_acc: 0.8305\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0794 - acc: 0.9806 - val_loss: 0.4553 - val_acc: 0.8330\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0719 - acc: 0.9811 - val_loss: 0.4608 - val_acc: 0.8334\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0634 - acc: 0.9841 - val_loss: 0.4705 - val_acc: 0.8324\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0549 - acc: 0.9872 - val_loss: 0.4873 - val_acc: 0.8317\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0500 - acc: 0.9882 - val_loss: 0.5039 - val_acc: 0.8300\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07194, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0851 - acc: 0.4017 - val_loss: 1.0719 - val_acc: 0.4074\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07194 to 0.94092, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0203 - acc: 0.4712 - val_loss: 0.9409 - val_acc: 0.5663\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94092 to 0.74278, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8237 - acc: 0.6701 - val_loss: 0.7428 - val_acc: 0.7172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74278 to 0.60012, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6161 - acc: 0.7937 - val_loss: 0.6001 - val_acc: 0.7725\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60012 to 0.52251, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4737 - acc: 0.8434 - val_loss: 0.5225 - val_acc: 0.7982\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52251 to 0.47522, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3780 - acc: 0.8760 - val_loss: 0.4752 - val_acc: 0.8127\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47522 to 0.44633, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3092 - acc: 0.9010 - val_loss: 0.4463 - val_acc: 0.8192\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44633 to 0.42941, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2570 - acc: 0.9199 - val_loss: 0.4294 - val_acc: 0.8245\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.42941 to 0.41967, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2164 - acc: 0.9353 - val_loss: 0.4197 - val_acc: 0.8281\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.41967 to 0.41887, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1827 - acc: 0.9458 - val_loss: 0.4189 - val_acc: 0.8307\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.41887 to 0.41642, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1553 - acc: 0.9542 - val_loss: 0.4164 - val_acc: 0.8328\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1345 - acc: 0.9622 - val_loss: 0.4195 - val_acc: 0.8349\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1170 - acc: 0.9687 - val_loss: 0.4266 - val_acc: 0.8368\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1031 - acc: 0.9718 - val_loss: 0.4320 - val_acc: 0.8377\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0896 - acc: 0.9765 - val_loss: 0.4406 - val_acc: 0.8379\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0785 - acc: 0.9789 - val_loss: 0.4493 - val_acc: 0.8370\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0688 - acc: 0.9829 - val_loss: 0.4596 - val_acc: 0.8360\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0608 - acc: 0.9850 - val_loss: 0.4712 - val_acc: 0.8347\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0548 - acc: 0.9871 - val_loss: 0.4831 - val_acc: 0.8349\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0477 - acc: 0.9894 - val_loss: 0.4941 - val_acc: 0.8337\n",
      "------------------\n",
      "Train on 12335 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07163, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0846 - acc: 0.4031 - val_loss: 1.0716 - val_acc: 0.4070\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07163 to 0.95355, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0266 - acc: 0.4609 - val_loss: 0.9536 - val_acc: 0.5595\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95355 to 0.76300, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8411 - acc: 0.6544 - val_loss: 0.7630 - val_acc: 0.6960\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.76300 to 0.61006, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6378 - acc: 0.7838 - val_loss: 0.6101 - val_acc: 0.7675\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61006 to 0.52656, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4835 - acc: 0.8402 - val_loss: 0.5266 - val_acc: 0.7997\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52656 to 0.48112, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3823 - acc: 0.8733 - val_loss: 0.4811 - val_acc: 0.8116\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48112 to 0.45183, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3133 - acc: 0.8978 - val_loss: 0.4518 - val_acc: 0.8179\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45183 to 0.44033, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2611 - acc: 0.9165 - val_loss: 0.4403 - val_acc: 0.8230\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44033 to 0.43145, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2191 - acc: 0.9328 - val_loss: 0.4314 - val_acc: 0.8252\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43145 to 0.43052, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1868 - acc: 0.9450 - val_loss: 0.4305 - val_acc: 0.8290\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1596 - acc: 0.9543 - val_loss: 0.4326 - val_acc: 0.8298\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1384 - acc: 0.9612 - val_loss: 0.4355 - val_acc: 0.8334\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1224 - acc: 0.9668 - val_loss: 0.4399 - val_acc: 0.8343\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1069 - acc: 0.9715 - val_loss: 0.4452 - val_acc: 0.8347\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0935 - acc: 0.9745 - val_loss: 0.4570 - val_acc: 0.8341\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0821 - acc: 0.9792 - val_loss: 0.4627 - val_acc: 0.8351\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0737 - acc: 0.9805 - val_loss: 0.4746 - val_acc: 0.8332\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0647 - acc: 0.9840 - val_loss: 0.4824 - val_acc: 0.8336\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0572 - acc: 0.9857 - val_loss: 0.5018 - val_acc: 0.8303\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0512 - acc: 0.9874 - val_loss: 0.5092 - val_acc: 0.8313\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 10\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.3,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_nn_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 283) (8392, 283)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id', 'text','tag_txt','ne_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "\n",
    "\n",
    "train_X = np.hstack([train_X,\n",
    "                     train_svd,train_svd2,train_svd3,train_svd4,\n",
    "                     train_cvec3,train_cvec4,train_tf5,train_tf6])\n",
    "test_X = np.hstack([test_X,\n",
    "                    test_svd,test_svd2,test_svd4,test_svd4,\n",
    "                    test_cvec3,test_cvec4,test_tf5,test_tf6])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,\n",
    "                       help_train_feat4,help_train_feat5,\n",
    "                       cnn_train1, cnn_train2\n",
    "                       ])\n",
    "f_train_X = np.round(f_train_X,4)\n",
    "\n",
    "\n",
    "f_test_X = np.hstack([test_X, help_test_feat,\n",
    "                      help_test_feat4,help_test_feat5,\n",
    "                      cnn_test1, cnn_test2\n",
    "                     ])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('feat.pkl','wb') as fout:\n",
    "#     pickle.dump([f_train_X,f_test_X],fout)\n",
    "# print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05517\tvalid-mlogloss:1.05543\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.242503\tvalid-mlogloss:0.274426\n",
      "[400]\ttrain-mlogloss:0.203594\tvalid-mlogloss:0.264512\n",
      "[600]\ttrain-mlogloss:0.175479\tvalid-mlogloss:0.263147\n",
      "Stopping. Best iteration:\n",
      "[553]\ttrain-mlogloss:0.181621\tvalid-mlogloss:0.262918\n",
      "\n",
      "train log loss 0.175108068374 valid log loss 0.263204350169\n",
      "rev 3.79932930196\n",
      "[0]\ttrain-mlogloss:1.05498\tvalid-mlogloss:1.05625\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.238677\tvalid-mlogloss:0.304704\n",
      "[400]\ttrain-mlogloss:0.199766\tvalid-mlogloss:0.297094\n",
      "[600]\ttrain-mlogloss:0.171774\tvalid-mlogloss:0.295971\n",
      "Stopping. Best iteration:\n",
      "[555]\ttrain-mlogloss:0.177508\tvalid-mlogloss:0.295648\n",
      "\n",
      "train log loss 0.171196982505 valid log loss 0.295903977376\n",
      "rev 3.37947468252\n",
      "[0]\ttrain-mlogloss:1.05517\tvalid-mlogloss:1.05572\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.241272\tvalid-mlogloss:0.27862\n",
      "[400]\ttrain-mlogloss:0.201206\tvalid-mlogloss:0.273456\n",
      "Stopping. Best iteration:\n",
      "[434]\ttrain-mlogloss:0.195953\tvalid-mlogloss:0.272979\n",
      "\n",
      "train log loss 0.188699077346 valid log loss 0.273048552339\n",
      "rev 3.66235232318\n",
      "[0]\ttrain-mlogloss:1.05522\tvalid-mlogloss:1.05549\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.241386\tvalid-mlogloss:0.283752\n",
      "[400]\ttrain-mlogloss:0.201762\tvalid-mlogloss:0.275983\n",
      "[600]\ttrain-mlogloss:0.174534\tvalid-mlogloss:0.273511\n",
      "Stopping. Best iteration:\n",
      "[614]\ttrain-mlogloss:0.172883\tvalid-mlogloss:0.273331\n",
      "\n",
      "train log loss 0.166983756338 valid log loss 0.273503758811\n",
      "rev 3.65625688051\n",
      "[0]\ttrain-mlogloss:1.05525\tvalid-mlogloss:1.05453\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.244938\tvalid-mlogloss:0.24587\n",
      "[400]\ttrain-mlogloss:0.205613\tvalid-mlogloss:0.238622\n",
      "[600]\ttrain-mlogloss:0.177387\tvalid-mlogloss:0.236587\n",
      "Stopping. Best iteration:\n",
      "[577]\ttrain-mlogloss:0.180195\tvalid-mlogloss:0.236485\n",
      "\n",
      "train log loss 0.174210541763 valid log loss 0.236695534292\n",
      "rev 4.22483678449\n",
      "[0]\ttrain-mlogloss:1.05534\tvalid-mlogloss:1.05546\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.244664\tvalid-mlogloss:0.257847\n",
      "[400]\ttrain-mlogloss:0.205633\tvalid-mlogloss:0.245963\n",
      "[600]\ttrain-mlogloss:0.177384\tvalid-mlogloss:0.241867\n",
      "Stopping. Best iteration:\n",
      "[645]\ttrain-mlogloss:0.171853\tvalid-mlogloss:0.240973\n",
      "\n",
      "train log loss 0.165892744868 valid log loss 0.241122106912\n",
      "rev 4.14727630247\n",
      "[0]\ttrain-mlogloss:1.05525\tvalid-mlogloss:1.0552\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.241916\tvalid-mlogloss:0.272896\n",
      "[400]\ttrain-mlogloss:0.202889\tvalid-mlogloss:0.266904\n",
      "Stopping. Best iteration:\n",
      "[444]\ttrain-mlogloss:0.196071\tvalid-mlogloss:0.265848\n",
      "\n",
      "train log loss 0.188927029661 valid log loss 0.265886053614\n",
      "rev 3.76100959943\n",
      "[0]\ttrain-mlogloss:1.05517\tvalid-mlogloss:1.05561\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.242178\tvalid-mlogloss:0.276796\n",
      "[400]\ttrain-mlogloss:0.203051\tvalid-mlogloss:0.266225\n",
      "Stopping. Best iteration:\n",
      "[514]\ttrain-mlogloss:0.185854\tvalid-mlogloss:0.264619\n",
      "\n",
      "train log loss 0.179168588324 valid log loss 0.264967816985\n",
      "rev 3.77404324562\n",
      "[0]\ttrain-mlogloss:1.05534\tvalid-mlogloss:1.05474\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.244319\tvalid-mlogloss:0.260466\n",
      "[400]\ttrain-mlogloss:0.205248\tvalid-mlogloss:0.253105\n",
      "[600]\ttrain-mlogloss:0.177048\tvalid-mlogloss:0.251202\n",
      "Stopping. Best iteration:\n",
      "[603]\ttrain-mlogloss:0.176707\tvalid-mlogloss:0.251047\n",
      "\n",
      "train log loss 0.170530277975 valid log loss 0.25149128419\n",
      "rev 3.9762809404\n",
      "[0]\ttrain-mlogloss:1.0552\tvalid-mlogloss:1.05584\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.241363\tvalid-mlogloss:0.286001\n",
      "[400]\ttrain-mlogloss:0.202773\tvalid-mlogloss:0.277642\n",
      "Stopping. Best iteration:\n",
      "[544]\ttrain-mlogloss:0.181803\tvalid-mlogloss:0.275509\n",
      "\n",
      "train log loss 0.175323652256 valid log loss 0.275866931738\n",
      "rev 3.62493610126\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0105  0.0031  0.9863\n",
      "1  id24541  0.9994  0.0003  0.0003\n",
      "2  id00134  0.0014  0.9982  0.0004\n",
      "3  id27757  0.7615  0.2335  0.0050\n",
      "4  id04081  0.7801  0.1485  0.0714\n",
      "--------------\n",
      "38.0057961618\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0105  0.0031  0.9863\n",
      "1  id24541  0.9994  0.0003  0.0003\n",
      "2  id00134  0.0014  0.9982  0.0004\n",
      "3  id27757  0.7607  0.2342  0.0050\n",
      "4  id04081  0.7803  0.1482  0.0715\n",
      "---------------\n",
      "local average valid loss 0.264169036643\n",
      "train log loss 0.180801396689\n"
     ]
    }
   ],
   "source": [
    "cv_test(10, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
