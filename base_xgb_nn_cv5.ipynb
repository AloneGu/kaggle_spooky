{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 8) (8392, 8)\n",
      "(19579, 68) (8392, 68)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "# add tfidf to svd\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "print(train_X.shape, test_X.shape)\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2])\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 12) (8392, 12)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.02,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05253, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0822 - acc: 0.4060 - val_loss: 1.0525 - val_acc: 0.4786\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05253 to 0.76502, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9131 - acc: 0.6032 - val_loss: 0.7650 - val_acc: 0.7077\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76502 to 0.54395, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6010 - acc: 0.7710 - val_loss: 0.5440 - val_acc: 0.7869\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.54395 to 0.45176, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4088 - acc: 0.8522 - val_loss: 0.4518 - val_acc: 0.8226\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.45176 to 0.41749, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2953 - acc: 0.8983 - val_loss: 0.4175 - val_acc: 0.8385\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41749 to 0.40857, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2208 - acc: 0.9281 - val_loss: 0.4086 - val_acc: 0.8462\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1733 - acc: 0.9454 - val_loss: 0.4137 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1360 - acc: 0.9601 - val_loss: 0.4260 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1081 - acc: 0.9679 - val_loss: 0.4512 - val_acc: 0.8488\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0926 - acc: 0.9722 - val_loss: 0.4756 - val_acc: 0.8456\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04937, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0811 - acc: 0.4127 - val_loss: 1.0494 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04937 to 0.75567, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9147 - acc: 0.5980 - val_loss: 0.7557 - val_acc: 0.6956\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75567 to 0.51342, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5690 - acc: 0.7990 - val_loss: 0.5134 - val_acc: 0.8066\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51342 to 0.43411, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3673 - acc: 0.8769 - val_loss: 0.4341 - val_acc: 0.8290\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43411 to 0.41291, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2654 - acc: 0.9133 - val_loss: 0.4129 - val_acc: 0.8379\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41291 to 0.41225, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2023 - acc: 0.9359 - val_loss: 0.4123 - val_acc: 0.8417\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1607 - acc: 0.9489 - val_loss: 0.4280 - val_acc: 0.8405\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1280 - acc: 0.9612 - val_loss: 0.4472 - val_acc: 0.8392\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1046 - acc: 0.9688 - val_loss: 0.4729 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0901 - acc: 0.9742 - val_loss: 0.4970 - val_acc: 0.8290\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03613, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0782 - acc: 0.4222 - val_loss: 1.0361 - val_acc: 0.4850\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03613 to 0.79671, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9225 - acc: 0.5841 - val_loss: 0.7967 - val_acc: 0.6860\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.79671 to 0.53739, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6095 - acc: 0.7809 - val_loss: 0.5374 - val_acc: 0.7920\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53739 to 0.44627, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3836 - acc: 0.8696 - val_loss: 0.4463 - val_acc: 0.8200\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44627 to 0.41941, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2737 - acc: 0.9076 - val_loss: 0.4194 - val_acc: 0.8392\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2072 - acc: 0.9330 - val_loss: 0.4273 - val_acc: 0.8354\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9502 - val_loss: 0.4290 - val_acc: 0.8379\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1299 - acc: 0.9596 - val_loss: 0.4505 - val_acc: 0.8411\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1050 - acc: 0.9689 - val_loss: 0.4776 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0845 - acc: 0.9763 - val_loss: 0.5092 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04157, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0816 - acc: 0.4117 - val_loss: 1.0416 - val_acc: 0.4767\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04157 to 0.77121, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9160 - acc: 0.5797 - val_loss: 0.7712 - val_acc: 0.7096\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77121 to 0.51704, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6068 - acc: 0.7812 - val_loss: 0.5170 - val_acc: 0.8079\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51704 to 0.43099, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3906 - acc: 0.8651 - val_loss: 0.4310 - val_acc: 0.8328\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43099 to 0.40488, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2841 - acc: 0.9052 - val_loss: 0.4049 - val_acc: 0.8417\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40488 to 0.39438, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2178 - acc: 0.9291 - val_loss: 0.3944 - val_acc: 0.8456\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1726 - acc: 0.9459 - val_loss: 0.3991 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1385 - acc: 0.9581 - val_loss: 0.4071 - val_acc: 0.8507\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1132 - acc: 0.9679 - val_loss: 0.4312 - val_acc: 0.8494\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0951 - acc: 0.9715 - val_loss: 0.4405 - val_acc: 0.8468\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03782, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0799 - acc: 0.4170 - val_loss: 1.0378 - val_acc: 0.4888\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03782 to 0.71331, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8909 - acc: 0.6091 - val_loss: 0.7133 - val_acc: 0.7377\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71331 to 0.48531, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5550 - acc: 0.8069 - val_loss: 0.4853 - val_acc: 0.8226\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48531 to 0.41311, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3630 - acc: 0.8751 - val_loss: 0.4131 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41311 to 0.39431, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2664 - acc: 0.9103 - val_loss: 0.3943 - val_acc: 0.8532\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39431 to 0.39227, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2052 - acc: 0.9320 - val_loss: 0.3923 - val_acc: 0.8558\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1576 - acc: 0.9495 - val_loss: 0.4060 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1288 - acc: 0.9613 - val_loss: 0.4334 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1041 - acc: 0.9686 - val_loss: 0.4556 - val_acc: 0.8411\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0891 - acc: 0.9744 - val_loss: 0.4857 - val_acc: 0.8385\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04611, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0805 - acc: 0.4164 - val_loss: 1.0461 - val_acc: 0.4678\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04611 to 0.80328, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9340 - acc: 0.5626 - val_loss: 0.8033 - val_acc: 0.6637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80328 to 0.55291, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6349 - acc: 0.7621 - val_loss: 0.5529 - val_acc: 0.7862\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.55291 to 0.44454, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4055 - acc: 0.8611 - val_loss: 0.4445 - val_acc: 0.8315\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44454 to 0.41441, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2821 - acc: 0.9067 - val_loss: 0.4144 - val_acc: 0.8360\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41441 to 0.41062, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2103 - acc: 0.9319 - val_loss: 0.4106 - val_acc: 0.8398\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1596 - acc: 0.9535 - val_loss: 0.4242 - val_acc: 0.8379\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1252 - acc: 0.9626 - val_loss: 0.4418 - val_acc: 0.8366\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1035 - acc: 0.9686 - val_loss: 0.4703 - val_acc: 0.8373\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0859 - acc: 0.9738 - val_loss: 0.4905 - val_acc: 0.8328\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04152, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0805 - acc: 0.4142 - val_loss: 1.0415 - val_acc: 0.4844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04152 to 0.80166, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9352 - acc: 0.5692 - val_loss: 0.8017 - val_acc: 0.6707\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80166 to 0.52774, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6116 - acc: 0.7793 - val_loss: 0.5277 - val_acc: 0.7964\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52774 to 0.42653, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3810 - acc: 0.8688 - val_loss: 0.4265 - val_acc: 0.8360\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42653 to 0.39541, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2690 - acc: 0.9105 - val_loss: 0.3954 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2005 - acc: 0.9358 - val_loss: 0.3965 - val_acc: 0.8519\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1563 - acc: 0.9498 - val_loss: 0.4119 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1249 - acc: 0.9601 - val_loss: 0.4243 - val_acc: 0.8417\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1017 - acc: 0.9705 - val_loss: 0.4487 - val_acc: 0.8341\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0841 - acc: 0.9757 - val_loss: 0.4806 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03952, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0807 - acc: 0.4157 - val_loss: 1.0395 - val_acc: 0.4780\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03952 to 0.77772, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9167 - acc: 0.5901 - val_loss: 0.7777 - val_acc: 0.7058\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77772 to 0.51627, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5962 - acc: 0.7853 - val_loss: 0.5163 - val_acc: 0.8041\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51627 to 0.42245, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3794 - acc: 0.8722 - val_loss: 0.4224 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42245 to 0.39394, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2648 - acc: 0.9117 - val_loss: 0.3939 - val_acc: 0.8507\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2023 - acc: 0.9354 - val_loss: 0.3950 - val_acc: 0.8513\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1581 - acc: 0.9508 - val_loss: 0.4064 - val_acc: 0.8507\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1279 - acc: 0.9607 - val_loss: 0.4225 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1027 - acc: 0.9696 - val_loss: 0.4513 - val_acc: 0.8513\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0860 - acc: 0.9739 - val_loss: 0.4734 - val_acc: 0.8494\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05783, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0830 - acc: 0.4111 - val_loss: 1.0578 - val_acc: 0.4378\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05783 to 0.72791, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9030 - acc: 0.6042 - val_loss: 0.7279 - val_acc: 0.7543\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72791 to 0.50289, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5502 - acc: 0.8116 - val_loss: 0.5029 - val_acc: 0.8098\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50289 to 0.42709, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3634 - acc: 0.8741 - val_loss: 0.4271 - val_acc: 0.8392\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42709 to 0.40509, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2635 - acc: 0.9140 - val_loss: 0.4051 - val_acc: 0.8456\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2013 - acc: 0.9349 - val_loss: 0.4134 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1598 - acc: 0.9497 - val_loss: 0.4212 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1299 - acc: 0.9606 - val_loss: 0.4429 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1064 - acc: 0.9687 - val_loss: 0.4630 - val_acc: 0.8462\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0887 - acc: 0.9745 - val_loss: 0.4960 - val_acc: 0.8385\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04479, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0818 - acc: 0.4111 - val_loss: 1.0448 - val_acc: 0.4684\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04479 to 0.76304, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9110 - acc: 0.5908 - val_loss: 0.7630 - val_acc: 0.7198\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76304 to 0.52396, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5957 - acc: 0.7859 - val_loss: 0.5240 - val_acc: 0.8041\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52396 to 0.44225, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3840 - acc: 0.8669 - val_loss: 0.4423 - val_acc: 0.8322\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44225 to 0.41637, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2751 - acc: 0.9089 - val_loss: 0.4164 - val_acc: 0.8481\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41637 to 0.41051, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2120 - acc: 0.9294 - val_loss: 0.4105 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1671 - acc: 0.9456 - val_loss: 0.4193 - val_acc: 0.8462\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1331 - acc: 0.9585 - val_loss: 0.4377 - val_acc: 0.8398\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1109 - acc: 0.9660 - val_loss: 0.4597 - val_acc: 0.8385\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0910 - acc: 0.9737 - val_loss: 0.4884 - val_acc: 0.8354\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04511, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0813 - acc: 0.4127 - val_loss: 1.0451 - val_acc: 0.4588\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04511 to 0.72071, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8904 - acc: 0.6136 - val_loss: 0.7207 - val_acc: 0.7192\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72071 to 0.48979, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5503 - acc: 0.8073 - val_loss: 0.4898 - val_acc: 0.8149\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48979 to 0.41715, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3610 - acc: 0.8788 - val_loss: 0.4172 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41715 to 0.39169, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2597 - acc: 0.9147 - val_loss: 0.3917 - val_acc: 0.8500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2017 - acc: 0.9344 - val_loss: 0.3927 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1557 - acc: 0.9520 - val_loss: 0.4071 - val_acc: 0.8468\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1281 - acc: 0.9619 - val_loss: 0.4253 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1037 - acc: 0.9672 - val_loss: 0.4505 - val_acc: 0.8417\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0875 - acc: 0.9750 - val_loss: 0.4779 - val_acc: 0.8405\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04913, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0822 - acc: 0.4104 - val_loss: 1.0491 - val_acc: 0.4671\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04913 to 0.74199, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9068 - acc: 0.5974 - val_loss: 0.7420 - val_acc: 0.7332\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.74199 to 0.50354, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5778 - acc: 0.7924 - val_loss: 0.5035 - val_acc: 0.8143\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50354 to 0.42511, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3784 - acc: 0.8712 - val_loss: 0.4251 - val_acc: 0.8354\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42511 to 0.39840, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2734 - acc: 0.9103 - val_loss: 0.3984 - val_acc: 0.8456\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39840 to 0.39473, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2072 - acc: 0.9316 - val_loss: 0.3947 - val_acc: 0.8500\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1617 - acc: 0.9471 - val_loss: 0.4019 - val_acc: 0.8526\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1340 - acc: 0.9572 - val_loss: 0.4177 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1073 - acc: 0.9676 - val_loss: 0.4406 - val_acc: 0.8475\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0879 - acc: 0.9742 - val_loss: 0.4649 - val_acc: 0.8488\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03668, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0795 - acc: 0.4188 - val_loss: 1.0367 - val_acc: 0.4805\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03668 to 0.77876, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9068 - acc: 0.5850 - val_loss: 0.7788 - val_acc: 0.6796\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77876 to 0.53759, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6099 - acc: 0.7783 - val_loss: 0.5376 - val_acc: 0.7983\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53759 to 0.43927, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3930 - acc: 0.8652 - val_loss: 0.4393 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43927 to 0.40432, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2783 - acc: 0.9046 - val_loss: 0.4043 - val_acc: 0.8437\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40432 to 0.39979, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2099 - acc: 0.9322 - val_loss: 0.3998 - val_acc: 0.8488\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1636 - acc: 0.9486 - val_loss: 0.4089 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1328 - acc: 0.9597 - val_loss: 0.4389 - val_acc: 0.8462\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1052 - acc: 0.9698 - val_loss: 0.4682 - val_acc: 0.8468\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0879 - acc: 0.9753 - val_loss: 0.4975 - val_acc: 0.8417\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04088, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0803 - acc: 0.4178 - val_loss: 1.0409 - val_acc: 0.4799\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04088 to 0.86945, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9495 - acc: 0.5406 - val_loss: 0.8694 - val_acc: 0.5737\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.86945 to 0.76621, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7859 - acc: 0.6196 - val_loss: 0.7662 - val_acc: 0.6190\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.76621 to 0.72650, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6811 - acc: 0.6489 - val_loss: 0.7265 - val_acc: 0.6286\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72650 to 0.72355, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6163 - acc: 0.6723 - val_loss: 0.7236 - val_acc: 0.6503\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.72355 to 0.72231, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5546 - acc: 0.7307 - val_loss: 0.7223 - val_acc: 0.6586\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.4741 - acc: 0.8016 - val_loss: 0.7342 - val_acc: 0.6899\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.3814 - acc: 0.8587 - val_loss: 0.7280 - val_acc: 0.7128\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.3006 - acc: 0.8963 - val_loss: 0.7245 - val_acc: 0.7250\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.2394 - acc: 0.9200 - val_loss: 0.7340 - val_acc: 0.7390\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03563, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0791 - acc: 0.4204 - val_loss: 1.0356 - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03563 to 0.75442, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9084 - acc: 0.5951 - val_loss: 0.7544 - val_acc: 0.7192\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75442 to 0.51868, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5841 - acc: 0.7923 - val_loss: 0.5187 - val_acc: 0.8015\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51868 to 0.43090, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3770 - acc: 0.8720 - val_loss: 0.4309 - val_acc: 0.8385\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43090 to 0.40348, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2686 - acc: 0.9090 - val_loss: 0.4035 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40348 to 0.40105, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2069 - acc: 0.9312 - val_loss: 0.4010 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1621 - acc: 0.9495 - val_loss: 0.4083 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1306 - acc: 0.9596 - val_loss: 0.4311 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1089 - acc: 0.9663 - val_loss: 0.4494 - val_acc: 0.8417\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0914 - acc: 0.9723 - val_loss: 0.4762 - val_acc: 0.8398\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.85093, saving model to /tmp/nn_model.h5\n",
      " - 21s - loss: 1.0305 - acc: 0.4597 - val_loss: 0.8509 - val_acc: 0.6388\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.85093 to 0.50960, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.6311 - acc: 0.7401 - val_loss: 0.5096 - val_acc: 0.7983\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.50960 to 0.44154, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3345 - acc: 0.8749 - val_loss: 0.4415 - val_acc: 0.8277\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.2037 - acc: 0.9264 - val_loss: 0.5233 - val_acc: 0.8181\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.85117, saving model to /tmp/nn_model.h5\n",
      " - 21s - loss: 1.0175 - acc: 0.4720 - val_loss: 0.8512 - val_acc: 0.6324\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.85117 to 0.51954, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.6005 - acc: 0.7536 - val_loss: 0.5195 - val_acc: 0.7971\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.51954 to 0.49107, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3145 - acc: 0.8807 - val_loss: 0.4911 - val_acc: 0.8175\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.2025 - acc: 0.9256 - val_loss: 0.5305 - val_acc: 0.8277\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.82940, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0111 - acc: 0.4777 - val_loss: 0.8294 - val_acc: 0.6177\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.82940 to 0.53152, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.6374 - acc: 0.7280 - val_loss: 0.5315 - val_acc: 0.7920\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.53152 to 0.47127, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3381 - acc: 0.8702 - val_loss: 0.4713 - val_acc: 0.8245\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.2049 - acc: 0.9234 - val_loss: 0.5254 - val_acc: 0.8162\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.82428, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0171 - acc: 0.4709 - val_loss: 0.8243 - val_acc: 0.6260\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.82428 to 0.52889, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.6261 - acc: 0.7398 - val_loss: 0.5289 - val_acc: 0.7888\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.52889 to 0.48465, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3310 - acc: 0.8720 - val_loss: 0.4847 - val_acc: 0.8168\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.2031 - acc: 0.9247 - val_loss: 0.5284 - val_acc: 0.8315\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/4\n",
      "Epoch 00001: val_loss improved from inf to 0.78653, saving model to /tmp/nn_model.h5\n",
      " - 23s - loss: 1.0115 - acc: 0.4839 - val_loss: 0.7865 - val_acc: 0.6809\n",
      "Epoch 2/4\n",
      "Epoch 00002: val_loss improved from 0.78653 to 0.47220, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.5514 - acc: 0.7797 - val_loss: 0.4722 - val_acc: 0.8200\n",
      "Epoch 3/4\n",
      "Epoch 00003: val_loss improved from 0.47220 to 0.44849, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2980 - acc: 0.8859 - val_loss: 0.4485 - val_acc: 0.8251\n",
      "Epoch 4/4\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.1925 - acc: 0.9284 - val_loss: 0.5326 - val_acc: 0.8277\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=256, epochs=4, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 128) (8392, 128)\n",
      "def done\n"
     ]
    }
   ],
   "source": [
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,\n",
    "                       help_train_feat3,lstm_train1, lstm_train2, cnn_train1, cnn_train2,\n",
    "                       cnn_train3, cnn_train4,cnn_train5, cnn_train6])\n",
    "f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,\n",
    "                      lstm_test1, lstm_test2, cnn_test1, cnn_test2,\n",
    "                      cnn_test3, cnn_test4,cnn_test5, cnn_test6\n",
    "                     ])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "\n",
    "\n",
    "print(f_train_X.shape, f_test_X.shape)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.02,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07657\tvalid-mlogloss:1.07711\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.270211\tvalid-mlogloss:0.307651\n",
      "[400]\ttrain-mlogloss:0.232468\tvalid-mlogloss:0.287499\n",
      "[600]\ttrain-mlogloss:0.210755\tvalid-mlogloss:0.28265\n",
      "[800]\ttrain-mlogloss:0.193397\tvalid-mlogloss:0.28105\n",
      "[1000]\ttrain-mlogloss:0.178489\tvalid-mlogloss:0.280444\n",
      "[1200]\ttrain-mlogloss:0.165569\tvalid-mlogloss:0.280592\n",
      "Stopping. Best iteration:\n",
      "[1103]\ttrain-mlogloss:0.171741\tvalid-mlogloss:0.280343\n",
      "\n",
      "train log loss 0.165382834046 valid log loss 0.280562771671\n",
      "rev 3.56426475988\n",
      "[0]\ttrain-mlogloss:1.07669\tvalid-mlogloss:1.07706\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.272613\tvalid-mlogloss:0.300226\n",
      "[400]\ttrain-mlogloss:0.234569\tvalid-mlogloss:0.28105\n",
      "[600]\ttrain-mlogloss:0.212049\tvalid-mlogloss:0.277286\n",
      "[800]\ttrain-mlogloss:0.194889\tvalid-mlogloss:0.276168\n",
      "[1000]\ttrain-mlogloss:0.180149\tvalid-mlogloss:0.275798\n",
      "Stopping. Best iteration:\n",
      "[974]\ttrain-mlogloss:0.181926\tvalid-mlogloss:0.275579\n",
      "\n",
      "train log loss 0.175051240244 valid log loss 0.275902870706\n",
      "rev 3.62446391892\n",
      "[0]\ttrain-mlogloss:1.07685\tvalid-mlogloss:1.07672\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.279346\tvalid-mlogloss:0.276304\n",
      "[400]\ttrain-mlogloss:0.241636\tvalid-mlogloss:0.253304\n",
      "[600]\ttrain-mlogloss:0.219404\tvalid-mlogloss:0.247173\n",
      "[800]\ttrain-mlogloss:0.201773\tvalid-mlogloss:0.244628\n",
      "[1000]\ttrain-mlogloss:0.186778\tvalid-mlogloss:0.243537\n",
      "[1200]\ttrain-mlogloss:0.173108\tvalid-mlogloss:0.242787\n",
      "[1400]\ttrain-mlogloss:0.161076\tvalid-mlogloss:0.242586\n",
      "Stopping. Best iteration:\n",
      "[1393]\ttrain-mlogloss:0.161491\tvalid-mlogloss:0.242509\n",
      "\n",
      "train log loss 0.155807985297 valid log loss 0.242752350767\n",
      "rev 4.11942457752\n",
      "[0]\ttrain-mlogloss:1.07676\tvalid-mlogloss:1.07687\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.275297\tvalid-mlogloss:0.29132\n",
      "[400]\ttrain-mlogloss:0.237045\tvalid-mlogloss:0.270053\n",
      "[600]\ttrain-mlogloss:0.214552\tvalid-mlogloss:0.265482\n",
      "[800]\ttrain-mlogloss:0.197144\tvalid-mlogloss:0.263937\n",
      "[1000]\ttrain-mlogloss:0.182044\tvalid-mlogloss:0.262717\n",
      "Stopping. Best iteration:\n",
      "[1004]\ttrain-mlogloss:0.181749\tvalid-mlogloss:0.262688\n",
      "\n",
      "train log loss 0.174921513448 valid log loss 0.263016071594\n",
      "rev 3.80204903046\n",
      "[0]\ttrain-mlogloss:1.07677\tvalid-mlogloss:1.07684\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.273657\tvalid-mlogloss:0.295882\n",
      "[400]\ttrain-mlogloss:0.235446\tvalid-mlogloss:0.276732\n",
      "[600]\ttrain-mlogloss:0.213253\tvalid-mlogloss:0.271313\n",
      "[800]\ttrain-mlogloss:0.195984\tvalid-mlogloss:0.269418\n",
      "Stopping. Best iteration:\n",
      "[783]\ttrain-mlogloss:0.197334\tvalid-mlogloss:0.269355\n",
      "\n",
      "train log loss 0.18958232793 valid log loss 0.269421625376\n",
      "rev 3.71165454371\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0138  0.0028  0.9834\n",
      "1  id24541  0.9989  0.0007  0.0004\n",
      "2  id00134  0.0030  0.9963  0.0008\n",
      "3  id27757  0.7270  0.2686  0.0044\n",
      "4  id04081  0.8181  0.1154  0.0665\n",
      "--------------\n",
      "18.8218568305\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0139  0.0027  0.9833\n",
      "1  id24541  0.9989  0.0007  0.0004\n",
      "2  id00134  0.0030  0.9963  0.0008\n",
      "3  id27757  0.7242  0.2715  0.0044\n",
      "4  id04081  0.8177  0.1153  0.0670\n",
      "---------------\n",
      "local average valid loss 0.266331138023\n",
      "train log loss 0.183814419096\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
