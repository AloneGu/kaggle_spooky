{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n"
     ]
    }
   ],
   "source": [
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8,lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 8) (8392, 8)\n",
      "(19579, 68) (8392, 68)\n"
     ]
    }
   ],
   "source": [
    "# add tfidf to svd\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "print(train_X.shape, test_X.shape)\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2])\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 12) (8392, 12)\n",
      "[[ 0.835  0.036  0.129  1.     0.     0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.457  0.31   0.233  0.715  0.227  0.058  0.811  0.167  0.022  0.933\n",
      "   0.067  0.   ]\n",
      " [ 0.747  0.225  0.028  0.992  0.008  0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.029  0.069  0.902  0.     0.002  0.998  0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.334  0.387  0.279  0.949  0.032  0.019  0.215  0.402  0.383  1.     0.\n",
      "   0.   ]]\n",
      "[[ 0.192  0.087  0.721  0.001  0.     0.999  0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.753  0.166  0.081  0.999  0.     0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.048  0.895  0.056  0.006  0.993  0.     0.     1.     0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.518  0.435  0.047  0.324  0.676  0.     0.677  0.323  0.     0.2    0.8\n",
      "   0.   ]\n",
      " [ 0.557  0.259  0.184  0.772  0.034  0.194  0.881  0.116  0.003  0.987  0.\n",
      "   0.013]]\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23)\n",
    "for train_index, test_index in kf.split(train_tfidf):\n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    tmp_model = MultinomialNB(alpha=0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "    help_tfidf_train[test_index] = tmp_train_feat\n",
    "    help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "    help_tfidf_train2[test_index] = tmp_train_feat\n",
    "    help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec to nb\n",
    "    X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "    help_cnt1_train[test_index] = tmp_train_feat\n",
    "    help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec2 to nb \n",
    "    X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "    help_cnt2_train[test_index] = tmp_train_feat\n",
    "    help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "help_train_feat = np.round(np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train]),3)\n",
    "help_test_feat = np.round(np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test]),3)\n",
    "\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "print(help_train_feat[:5])\n",
    "print(help_test_feat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Epoch 00001: val_loss improved from inf to 1.04320, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.04320 to 0.79774, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.79774 to 0.71137, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.71137 to 0.68705, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.68705 to 0.65715, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.65715 to 0.61462, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.61462 to 0.57192, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.57192 to 0.54677, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.54677 to 0.54507, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 1.04943, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.04943 to 0.77107, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.77107 to 0.51991, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.51991 to 0.43604, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.43604 to 0.41317, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 1.03694, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.03694 to 0.79253, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.79253 to 0.53844, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.53844 to 0.44662, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.44662 to 0.41900, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.41900 to 0.41871, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 1.04188, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.04188 to 0.77874, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.77874 to 0.52436, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.52436 to 0.43428, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.43428 to 0.40097, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.40097 to 0.39368, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 1.03804, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.03804 to 0.73262, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.73262 to 0.48929, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.48929 to 0.41364, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.41364 to 0.39244, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.39244 to 0.39020, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 00010: val_loss did not improve\n",
      "[[ 0.999  0.001  0.   ]\n",
      " [ 0.514  0.303  0.182]\n",
      " [ 0.987  0.01   0.002]\n",
      " [ 0.     0.     1.   ]\n",
      " [ 0.919  0.056  0.025]]\n",
      "[[ 0.096  0.034  0.869]\n",
      " [ 0.999  0.001  0.   ]\n",
      " [ 0.203  0.78   0.017]\n",
      " [ 0.838  0.153  0.01 ]\n",
      " [ 0.834  0.057  0.109]]\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=0,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "\n",
    "        model = load_model(MODEL_P)\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "cnn_train,cnn_test = get_cnn_feats()\n",
    "cnn_train = np.round(cnn_train,3)\n",
    "cnn_test = np.round(cnn_test,3)\n",
    "print(cnn_train[:5])\n",
    "print(cnn_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Epoch 00001: val_loss improved from inf to 0.82915, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.82915 to 0.62665, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.62665 to 0.51120, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 0.67931, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.67931 to 0.49655, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.49655 to 0.46448, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 0.65883, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.65883 to 0.47803, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.47803 to 0.44786, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 0.67058, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.67058 to 0.47559, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.47559 to 0.44438, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00001: val_loss improved from inf to 0.66736, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.66736 to 0.47500, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.47500 to 0.44136, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 00005: val_loss did not improve\n",
      "[[ 0.996  0.003  0.001]\n",
      " [ 0.548  0.318  0.134]\n",
      " [ 0.95   0.046  0.004]\n",
      " [ 0.     0.001  0.999]\n",
      " [ 0.914  0.077  0.009]]\n",
      "[[ 0.092  0.03   0.877]\n",
      " [ 0.993  0.006  0.001]\n",
      " [ 0.038  0.957  0.005]\n",
      " [ 0.769  0.211  0.02 ]\n",
      " [ 0.894  0.034  0.072]]\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        #model.add(CuDNNLSTM(N,return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=5, \n",
    "                  verbose=0,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "\n",
    "        model = load_model(MODEL_P)\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train,lstm_test = get_lstm_feats()\n",
    "lstm_train = np.round(lstm_train,3)\n",
    "lstm_test = np.round(lstm_test,3)\n",
    "print(lstm_train[:5])\n",
    "print(lstm_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 86) (8392, 86)\n"
     ]
    }
   ],
   "source": [
    "f_train_X = np.hstack([train_X,help_train_feat,cnn_train,lstm_train])\n",
    "f_test_X = np.hstack([test_X,help_test_feat,cnn_test,lstm_test])\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=42)\n",
    "    test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.1,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 1000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=100)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        \n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            test_pred += m.predict(d_test)\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}.csv\".format(k_cnt),index=False)\n",
    "    submiss.head(5)\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.993036\tvalid-mlogloss:0.995346\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.228869\tvalid-mlogloss:0.298382\n",
      "[200]\ttrain-mlogloss:0.182353\tvalid-mlogloss:0.295783\n",
      "Stopping. Best iteration:\n",
      "[204]\ttrain-mlogloss:0.180595\tvalid-mlogloss:0.295603\n",
      "\n",
      "train log loss 0.163078433848 valid log loss 0.296552561864\n",
      "[0]\ttrain-mlogloss:0.995848\tvalid-mlogloss:0.99514\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.241766\tvalid-mlogloss:0.271849\n",
      "[200]\ttrain-mlogloss:0.195486\tvalid-mlogloss:0.265985\n",
      "Stopping. Best iteration:\n",
      "[231]\ttrain-mlogloss:0.183731\tvalid-mlogloss:0.265108\n",
      "\n",
      "train log loss 0.16702808401 valid log loss 0.265636695239\n",
      "[0]\ttrain-mlogloss:0.993556\tvalid-mlogloss:0.994657\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.234577\tvalid-mlogloss:0.288876\n",
      "[200]\ttrain-mlogloss:0.189129\tvalid-mlogloss:0.285486\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.187232\tvalid-mlogloss:0.285179\n",
      "\n",
      "train log loss 0.169479358411 valid log loss 0.285797424583\n",
      "local average valid loss 0.282662227229\n",
      "train log loss 0.191631917367\n"
     ]
    }
   ],
   "source": [
    "cv_test(3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.992988\tvalid-mlogloss:0.994817\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.229215\tvalid-mlogloss:0.297556\n",
      "[200]\ttrain-mlogloss:0.182404\tvalid-mlogloss:0.296828\n",
      "Stopping. Best iteration:\n",
      "[166]\ttrain-mlogloss:0.196088\tvalid-mlogloss:0.295635\n",
      "\n",
      "train log loss 0.176905550816 valid log loss 0.296998004456\n",
      "[0]\ttrain-mlogloss:0.995105\tvalid-mlogloss:0.99573\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.236688\tvalid-mlogloss:0.283001\n",
      "[200]\ttrain-mlogloss:0.189566\tvalid-mlogloss:0.279869\n",
      "Stopping. Best iteration:\n",
      "[173]\ttrain-mlogloss:0.200992\tvalid-mlogloss:0.278913\n",
      "\n",
      "train log loss 0.181318739978 valid log loss 0.279880517916\n",
      "[0]\ttrain-mlogloss:0.995587\tvalid-mlogloss:0.994662\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.239283\tvalid-mlogloss:0.278334\n",
      "[200]\ttrain-mlogloss:0.193483\tvalid-mlogloss:0.272766\n",
      "[300]\ttrain-mlogloss:0.159292\tvalid-mlogloss:0.272915\n",
      "Stopping. Best iteration:\n",
      "[273]\ttrain-mlogloss:0.167301\tvalid-mlogloss:0.272143\n",
      "\n",
      "train log loss 0.152443591303 valid log loss 0.273512771663\n",
      "local average valid loss 0.283463764678\n",
      "train log loss 0.194379044469\n"
     ]
    }
   ],
   "source": [
    "cv_test(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.993344\tvalid-mlogloss:0.995987\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.237631\tvalid-mlogloss:0.300001\n",
      "[200]\ttrain-mlogloss:0.195839\tvalid-mlogloss:0.295173\n",
      "Stopping. Best iteration:\n",
      "[196]\ttrain-mlogloss:0.197322\tvalid-mlogloss:0.29487\n",
      "\n",
      "train log loss 0.18053755178 valid log loss 0.295829625781\n",
      "[0]\ttrain-mlogloss:0.993888\tvalid-mlogloss:0.995256\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.239652\tvalid-mlogloss:0.292522\n",
      "[200]\ttrain-mlogloss:0.196984\tvalid-mlogloss:0.289627\n",
      "Stopping. Best iteration:\n",
      "[205]\ttrain-mlogloss:0.195372\tvalid-mlogloss:0.289553\n",
      "\n",
      "train log loss 0.179178705455 valid log loss 0.291050927926\n",
      "[0]\ttrain-mlogloss:0.994804\tvalid-mlogloss:0.992874\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.247764\tvalid-mlogloss:0.261061\n",
      "[200]\ttrain-mlogloss:0.205217\tvalid-mlogloss:0.253488\n",
      "Stopping. Best iteration:\n",
      "[239]\ttrain-mlogloss:0.192318\tvalid-mlogloss:0.253105\n",
      "\n",
      "train log loss 0.177165889631 valid log loss 0.253698303278\n",
      "[0]\ttrain-mlogloss:0.994146\tvalid-mlogloss:0.994966\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.242527\tvalid-mlogloss:0.28294\n",
      "[200]\ttrain-mlogloss:0.199764\tvalid-mlogloss:0.279687\n",
      "[300]\ttrain-mlogloss:0.168222\tvalid-mlogloss:0.277951\n",
      "Stopping. Best iteration:\n",
      "[272]\ttrain-mlogloss:0.176605\tvalid-mlogloss:0.277456\n",
      "\n",
      "train log loss 0.162506408971 valid log loss 0.277847068331\n",
      "[0]\ttrain-mlogloss:0.994039\tvalid-mlogloss:0.994365\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.240942\tvalid-mlogloss:0.287071\n",
      "[200]\ttrain-mlogloss:0.198565\tvalid-mlogloss:0.283039\n",
      "Stopping. Best iteration:\n",
      "[224]\ttrain-mlogloss:0.190381\tvalid-mlogloss:0.282019\n",
      "\n",
      "train log loss 0.174328028487 valid log loss 0.282416101189\n",
      "local average valid loss 0.280168405301\n",
      "train log loss 0.187474318816\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cv_test(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
