{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 591993) (8392, 591993)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(stop_words='english', ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 591993) (8392, 591993)\n",
      "(19579, 1354551) (8392, 1354551)\n"
     ]
    }
   ],
   "source": [
    "## add cnt vec\n",
    "c_vec = CountVectorizer(stop_words='english',ngram_range=(1,3),max_df=0.8,lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(stop_words='english',ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 8) (8392, 8)\n",
      "(19579, 68) (8392, 68)\n"
     ]
    }
   ],
   "source": [
    "# add tfidf to svd\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "print(train_X.shape, test_X.shape)\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2])\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 12) (8392, 12)\n",
      "[[ 0.659  0.153  0.189  0.999  0.     0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.422  0.453  0.126  0.753  0.204  0.043  0.197  0.802  0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.78   0.167  0.053  0.994  0.006  0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.041  0.054  0.905  0.     0.     1.     0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.322  0.416  0.262  0.918  0.066  0.016  0.162  0.792  0.046  1.     0.\n",
      "   0.   ]]\n",
      "[[ 0.208  0.114  0.678  0.001  0.     0.999  0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.488  0.243  0.27   1.     0.     0.     0.987  0.     0.013  1.     0.\n",
      "   0.   ]\n",
      " [ 0.321  0.544  0.135  0.008  0.992  0.     0.162  0.838  0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.531  0.385  0.084  0.32   0.68   0.     0.799  0.201  0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.583  0.255  0.162  0.777  0.036  0.187  0.963  0.031  0.006  0.887  0.\n",
      "   0.113]]\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=2017)\n",
    "for train_index, test_index in kf.split(train_tfidf):\n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    tmp_model = MultinomialNB(alpha=0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "    help_tfidf_train[test_index] = tmp_train_feat\n",
    "    help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "    help_tfidf_train2[test_index] = tmp_train_feat\n",
    "    help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec to nb\n",
    "    X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "    help_cnt1_train[test_index] = tmp_train_feat\n",
    "    help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec2 to nb \n",
    "    X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "    tmp_model = MultinomialNB(0.05,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "    help_cnt2_train[test_index] = tmp_train_feat\n",
    "    help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "help_train_feat = np.round(np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train]),3)\n",
    "help_test_feat = np.round(np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test]),3)\n",
    "\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "print(help_train_feat[:5])\n",
    "print(help_test_feat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 10)            200000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 78, 16)            496       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 200,819\n",
      "Trainable params: 200,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 1.08164, saving model to /tmp/lstm.h5\n",
      "0s - loss: 1.0883 - acc: 0.4022 - val_loss: 1.0816 - val_acc: 0.3999\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 1.08164 to 1.02244, saving model to /tmp/lstm.h5\n",
      "0s - loss: 1.0610 - acc: 0.4348 - val_loss: 1.0224 - val_acc: 0.5087\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 1.02244 to 0.88316, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.9566 - acc: 0.5490 - val_loss: 0.8832 - val_acc: 0.5907\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.88316 to 0.76381, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.8067 - acc: 0.6261 - val_loss: 0.7638 - val_acc: 0.6430\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.76381 to 0.68570, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.6822 - acc: 0.7045 - val_loss: 0.6857 - val_acc: 0.7074\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.68570 to 0.60659, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.5727 - acc: 0.7815 - val_loss: 0.6066 - val_acc: 0.7610\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.60659 to 0.53924, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.4759 - acc: 0.8325 - val_loss: 0.5392 - val_acc: 0.7929\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.53924 to 0.49681, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.3922 - acc: 0.8708 - val_loss: 0.4968 - val_acc: 0.8177\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.49681 to 0.46953, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.3283 - acc: 0.8956 - val_loss: 0.4695 - val_acc: 0.8243\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.46953 to 0.45601, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2806 - acc: 0.9101 - val_loss: 0.4560 - val_acc: 0.8276\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.45601 to 0.44964, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2413 - acc: 0.9245 - val_loss: 0.4496 - val_acc: 0.8292\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.44964 to 0.44891, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2046 - acc: 0.9390 - val_loss: 0.4489 - val_acc: 0.8297\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.1793 - acc: 0.9452 - val_loss: 0.4515 - val_acc: 0.8355\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.1622 - acc: 0.9521 - val_loss: 0.4619 - val_acc: 0.8279\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.1441 - acc: 0.9577 - val_loss: 0.4670 - val_acc: 0.8276\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.1314 - acc: 0.9604 - val_loss: 0.4907 - val_acc: 0.8228\n",
      "Epoch 17/20\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.1214 - acc: 0.9632 - val_loss: 0.5041 - val_acc: 0.8287\n",
      "Epoch 18/20\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.1103 - acc: 0.9698 - val_loss: 0.5181 - val_acc: 0.8327\n",
      "Epoch 19/20\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.0971 - acc: 0.9732 - val_loss: 0.5231 - val_acc: 0.8307\n",
      "Epoch 20/20\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.0916 - acc: 0.9738 - val_loss: 0.5348 - val_acc: 0.8304\n",
      "0.211023392491\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = 20000\n",
    "    N = 10\n",
    "    MAX_LEN = 80\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(Conv1D(16,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    model.fit(ttrain_x, ttrain_y, \n",
    "              validation_split=0.2,\n",
    "              batch_size=256, epochs=20, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk]\n",
    "             )\n",
    "    \n",
    "    model = load_model(MODEL_P)\n",
    "    train_pred = model.predict(ttrain_x)\n",
    "    test_pred = model.predict(ttest_x)\n",
    "    print(log_loss(ttrain_y,train_pred))\n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "cnn_train,cnn_test = get_cnn_feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 83) (8392, 83)\n"
     ]
    }
   ],
   "source": [
    "f_train_X = np.hstack([train_X,help_train_feat,cnn_train])\n",
    "f_test_X = np.hstack([test_X,help_test_feat,cnn_test])\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=42)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=42)\n",
    "    test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.1,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 1000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=100)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        \n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            test_pred += m.predict(d_test)\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}.csv\".format(k_cnt),index=False)\n",
    "    submiss.head(5)\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.974259\tvalid-mlogloss:0.975488\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.116905\tvalid-mlogloss:0.171554\n",
      "Stopping. Best iteration:\n",
      "[120]\ttrain-mlogloss:0.107818\tvalid-mlogloss:0.171409\n",
      "\n",
      "train log loss 0.0893385395499 valid log loss 0.172277056131\n",
      "[0]\ttrain-mlogloss:0.987313\tvalid-mlogloss:0.986092\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.124784\tvalid-mlogloss:0.153872\n",
      "Stopping. Best iteration:\n",
      "[134]\ttrain-mlogloss:0.110327\tvalid-mlogloss:0.152211\n",
      "\n",
      "train log loss 0.0925379553918 valid log loss 0.152935727921\n",
      "[0]\ttrain-mlogloss:0.974487\tvalid-mlogloss:0.975289\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.119346\tvalid-mlogloss:0.164631\n",
      "[200]\ttrain-mlogloss:0.083467\tvalid-mlogloss:0.163017\n",
      "Stopping. Best iteration:\n",
      "[219]\ttrain-mlogloss:0.078177\tvalid-mlogloss:0.162546\n",
      "\n",
      "train log loss 0.065920233975 valid log loss 0.163841501602\n",
      "local average valid loss 0.163018095218\n",
      "train log loss 0.0977589054847\n"
     ]
    }
   ],
   "source": [
    "cv_test(3, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.978122\tvalid-mlogloss:0.979576\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.117467\tvalid-mlogloss:0.169833\n",
      "Stopping. Best iteration:\n",
      "[129]\ttrain-mlogloss:0.104507\tvalid-mlogloss:0.168945\n",
      "\n",
      "train log loss 0.0875010168935 valid log loss 0.169749255099\n",
      "[0]\ttrain-mlogloss:0.986673\tvalid-mlogloss:0.987543\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.12001\tvalid-mlogloss:0.163803\n",
      "[200]\ttrain-mlogloss:0.083858\tvalid-mlogloss:0.162445\n",
      "Stopping. Best iteration:\n",
      "[170]\ttrain-mlogloss:0.093099\tvalid-mlogloss:0.162322\n",
      "\n",
      "train log loss 0.0786229854767 valid log loss 0.162613398778\n",
      "[0]\ttrain-mlogloss:0.986986\tvalid-mlogloss:0.986713\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.123536\tvalid-mlogloss:0.158924\n",
      "Stopping. Best iteration:\n",
      "[124]\ttrain-mlogloss:0.112964\tvalid-mlogloss:0.157712\n",
      "\n",
      "train log loss 0.0952253295258 valid log loss 0.157821063087\n",
      "local average valid loss 0.163394572321\n",
      "train log loss 0.10208872952\n"
     ]
    }
   ],
   "source": [
    "cv_test(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.978946\tvalid-mlogloss:0.980699\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.122856\tvalid-mlogloss:0.17497\n",
      "Stopping. Best iteration:\n",
      "[136]\ttrain-mlogloss:0.109059\tvalid-mlogloss:0.17452\n",
      "\n",
      "train log loss 0.0932692716897 valid log loss 0.176244603204\n",
      "[0]\ttrain-mlogloss:0.979369\tvalid-mlogloss:0.979721\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.124859\tvalid-mlogloss:0.166515\n",
      "Stopping. Best iteration:\n",
      "[139]\ttrain-mlogloss:0.110632\tvalid-mlogloss:0.16593\n",
      "\n",
      "train log loss 0.0941380485553 valid log loss 0.167361436162\n",
      "[0]\ttrain-mlogloss:0.979851\tvalid-mlogloss:0.979445\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.129696\tvalid-mlogloss:0.146146\n",
      "[200]\ttrain-mlogloss:0.095807\tvalid-mlogloss:0.142959\n",
      "[300]\ttrain-mlogloss:0.072912\tvalid-mlogloss:0.142714\n",
      "Stopping. Best iteration:\n",
      "[306]\ttrain-mlogloss:0.071798\tvalid-mlogloss:0.142367\n",
      "\n",
      "train log loss 0.0626418056477 valid log loss 0.143362207176\n",
      "[0]\ttrain-mlogloss:0.979365\tvalid-mlogloss:0.980411\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.124837\tvalid-mlogloss:0.167345\n",
      "[200]\ttrain-mlogloss:0.091652\tvalid-mlogloss:0.167199\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-mlogloss:0.102452\tvalid-mlogloss:0.166447\n",
      "\n",
      "train log loss 0.0883167662528 valid log loss 0.167007810332\n",
      "[0]\ttrain-mlogloss:0.981165\tvalid-mlogloss:0.982029\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-mlogloss:0.125927\tvalid-mlogloss:0.161704\n",
      "[200]\ttrain-mlogloss:0.092507\tvalid-mlogloss:0.159788\n",
      "Stopping. Best iteration:\n",
      "[246]\ttrain-mlogloss:0.081246\tvalid-mlogloss:0.158733\n",
      "\n",
      "train log loss 0.0706810233661 valid log loss 0.159799831059\n",
      "local average valid loss 0.162755177587\n",
      "train log loss 0.089395100705\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#cv_test(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
