{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:29<00:00,  1.14s/it]\n",
      "100%|██████████| 26/26 [01:11<00:00,  2.76s/it]\n",
      "100%|██████████| 26/26 [08:55<00:00, 20.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:13<00:00,  1.93it/s]\n",
      "100%|██████████| 26/26 [00:32<00:00,  1.26s/it]\n",
      "100%|██████████| 26/26 [04:02<00:00,  9.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 26685) (8392, 26685)\n",
      "Index(['id', 'text', 'author', 'n_.', 'n_...', 'n_,', 'n_:', 'n_;', 'n_-',\n",
      "       'n_?', 'n_!', 'n_'', 'n_\"', 'n_The ', 'n_I ', 'n_It ', 'n_He ', 'n_Me ',\n",
      "       'n_She ', 'n_We ', 'n_They ', 'n_You ', 'n_the', 'n_ a ', 'n_appear',\n",
      "       'n_little', 'n_was ', 'n_one ', 'n_two ', 'n_three ', 'n_ten ', 'n_is ',\n",
      "       'n_are ', 'n_ed', 'n_however', 'n_ to ', 'n_into', 'n_about ', 'n_th',\n",
      "       'n_er', 'n_ex', 'n_an ', 'n_ground', 'n_any', 'n_silence', 'n_wall'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "def extract_features(df, train_flag=False):\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    # Find numbers of different combinations\n",
    "    for c in tqdm(alphabet.upper()):\n",
    "        new_df['n_' + c] = new_df['text'].str.count(c)\n",
    "        new_df['n_' + c + '.'] = new_df['text'].str.count(c + '\\.')\n",
    "        new_df['n_' + c + ','] = new_df['text'].str.count(c + '\\,')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2] = new_df['text'].str.count(c + c2)\n",
    "            new_df['n_' + c + c2 + '.'] = new_df['text'].str.count(c + c2 + '\\.')\n",
    "            new_df['n_' + c + c2 + ','] = new_df['text'].str.count(c + c2 + '\\,')\n",
    "\n",
    "    for c in tqdm(alphabet):\n",
    "        new_df['n_' + c + '.'] = new_df['text'].str.count(c + '\\.')\n",
    "        new_df['n_' + c + ','] = new_df['text'].str.count(c + '\\,')\n",
    "        new_df['n_' + c + '?'] = new_df['text'].str.count(c + '\\?')\n",
    "        new_df['n_' + c + ';'] = new_df['text'].str.count(c + '\\;')\n",
    "        new_df['n_' + c + ':'] = new_df['text'].str.count(c + '\\:')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2 + '.'] = new_df['text'].str.count(c + c2 + '\\.')\n",
    "            new_df['n_' + c + c2 + ','] = new_df['text'].str.count(c + c2 + '\\,')\n",
    "            new_df['n_' + c + c2 + '?'] = new_df['text'].str.count(c + c2 + '\\?')\n",
    "            new_df['n_' + c + c2 + ';'] = new_df['text'].str.count(c + c2 + '\\;')\n",
    "            new_df['n_' + c + c2 + ':'] = new_df['text'].str.count(c + c2 + '\\:')\n",
    "            new_df['n_' + c + ', ' + c2] = new_df['text'].str.count(c + '\\, ' + c2)\n",
    "\n",
    "    # And now starting processing of cleaned text\n",
    "    for c in tqdm(alphabet):\n",
    "        new_df['n_' + c] = new_df['text_cleaned'].str.count(c)\n",
    "        new_df['n_' + c + ' '] = new_df['text_cleaned'].str.count(c + ' ')\n",
    "        new_df['n_' + ' ' + c] = new_df['text_cleaned'].str.count(' ' + c)\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2] = new_df['text_cleaned'].str.count(c + c2)\n",
    "            new_df['n_' + c + c2 + ' '] = new_df['text_cleaned'].str.count(c + c2 + ' ')\n",
    "            new_df['n_' + ' ' + c + c2] = new_df['text_cleaned'].str.count(' ' + c + c2)\n",
    "            new_df['n_' + c + ' ' + c2] = new_df['text_cleaned'].str.count(c + ' ' + c2)\n",
    "\n",
    "            for c3 in alphabet:\n",
    "                new_df['n_' + c + c2 + c3] = new_df['text_cleaned'].str.count(c + c2 + c3)\n",
    "                \n",
    "    if train_flag:\n",
    "        new_df.drop(['text_cleaned','text','author','id'], axis=1, inplace=True)\n",
    "    else:\n",
    "        new_df.drop(['text_cleaned','text','id'], axis=1, inplace=True)\n",
    "        \n",
    "    df.drop(['text_cleaned'],axis=1,inplace=True)\n",
    "    return new_df.values\n",
    "    \n",
    "print('Processing train...')\n",
    "train_hand_features = extract_features(train_df,train_flag=True)\n",
    "print('Processing test...')\n",
    "test_hand_features = extract_features(test_df)\n",
    "print(train_hand_features.shape,test_hand_features.shape)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' DET VERB NOUN ADJ NOUN PUNCT', ' DT VBZ NN JJ NNS .', ' nsubj ROOT nmod amod attr punct')\n",
      "train done 396.3616397380829\n",
      "test done 188.14431023597717\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/models#usage-import\n",
    "# https://spacy.io/usage/models\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# change ne to tag\n",
    "def get_spacy_text(s):\n",
    "    pos,tag,dep = '','',''\n",
    "    for token in spacy_nlp(s):\n",
    "        pos = pos + ' ' + token.pos_\n",
    "        tag = tag + ' ' + token.tag_\n",
    "        dep = dep + ' ' + token.dep_\n",
    "\n",
    "    return pos,tag,dep\n",
    "\n",
    "print(get_spacy_text('this is kaggle spooky games.'))\n",
    "\n",
    "import time\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in train_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "train_df['pos_txt'],train_df['tag_txt'],train_df['dep_txt'] = poss, tags, deps\n",
    "print('train done',time.time() - start_t)\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in test_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "test_df['pos_txt'],test_df['tag_txt'],test_df['dep_txt'] = poss, tags, deps\n",
    "print('test done', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n",
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n"
     ]
    }
   ],
   "source": [
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "c_vec4.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on dep\n",
    "c_vec7 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec7.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_cvec7 = c_vec7.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_cvec7 = c_vec7.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_cvec7.shape,test_cvec7.shape)\n",
    "\n",
    "# tfidf on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "# tfidf on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "tf_vec6.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)\n",
    "\n",
    "# tfidf on dep\n",
    "tf_vec8 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec8.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_tf8 = tf_vec8.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_tf8 = tf_vec8.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_tf8.shape,test_tf8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp feat done\n"
     ]
    }
   ],
   "source": [
    "all_nlp_train = np.hstack([train_cvec3,train_cvec4,train_tf5,train_tf6,train_cvec7, train_tf8]) \n",
    "all_nlp_test = np.hstack([test_cvec3,test_cvec4,test_tf5,test_tf6, test_cvec7, test_tf8]) \n",
    "print('nlp feat done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'author', 'n_.', 'n_...', 'n_,', 'n_:', 'n_;', 'n_-',\n",
      "       'n_?', 'n_!', 'n_'', 'n_\"', 'n_The ', 'n_I ', 'n_It ', 'n_He ', 'n_Me ',\n",
      "       'n_She ', 'n_We ', 'n_They ', 'n_You ', 'n_the', 'n_ a ', 'n_appear',\n",
      "       'n_little', 'n_was ', 'n_one ', 'n_two ', 'n_three ', 'n_ten ', 'n_is ',\n",
      "       'n_are ', 'n_ed', 'n_however', 'n_ to ', 'n_into', 'n_about ', 'n_th',\n",
      "       'n_er', 'n_ex', 'n_an ', 'n_ground', 'n_any', 'n_silence', 'n_wall',\n",
      "       'pos_txt', 'tag_txt', 'dep_txt', 'num_words', 'num_unique_words',\n",
      "       'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper',\n",
      "       'num_words_title', 'mean_word_len', 'unique_r', 'w_p', 'w_p_r',\n",
      "       'stop_r', 'w_p_stop', 'w_p_stop_r', 'num_words_upper_r',\n",
      "       'num_words_title_r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# add features\n",
    "def add_feat(df):\n",
    "    df['unique_r'] = df['num_unique_words'] / df['num_words']\n",
    "    df['w_p'] = df['num_words'] - df['num_punctuations']\n",
    "    df['w_p_r'] = df['w_p'] / df['num_words']\n",
    "    df['stop_r'] = df['num_stopwords'] / df['num_words']\n",
    "    df['w_p_stop'] = df['w_p'] - df['num_stopwords']\n",
    "    df['w_p_stop_r'] = df['w_p_stop'] / df['num_words']\n",
    "    df['num_words_upper_r'] = df['num_words_upper'] / df['num_words']\n",
    "    df['num_words_title_r'] = df['num_words_title'] / df['num_words']\n",
    "\n",
    "add_feat(train_df)\n",
    "add_feat(test_df)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "full_cvec1 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "## add svd3\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec1)\n",
    "train_svd3 = pd.DataFrame(svd_obj.transform(train_cvec))\n",
    "test_svd3 = pd.DataFrame(svd_obj.transform(test_cvec))\n",
    "\n",
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "full_cvec2 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "## add svd4\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec2)\n",
    "train_svd4 = pd.DataFrame(svd_obj.transform(train_cvec2))\n",
    "test_svd4 = pd.DataFrame(svd_obj.transform(test_cvec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 1068) (8392, 1068)\n"
     ]
    }
   ],
   "source": [
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(1,2), analyzer='char',max_df=0.8)\n",
    "full_cvec3 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec3 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec3 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec3.shape,test_cvec3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_svd_train = np.hstack([train_svd,train_svd2,train_svd3,train_svd4,train_cvec3.toarray()])\n",
    "all_svd_test = np.hstack([test_svd,test_svd2,test_svd3,test_svd4,test_cvec3.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 15) (8392, 15)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "        \n",
    "        # hand feature to nb\n",
    "        X_train, X_test = train_hand_features[train_index], train_hand_features[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_hand_features)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train,hand_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test,hand_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "\n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('nn_feat.pkl','rb') as fin:\n",
    "    nn_train1,nn_test1 = pickle.load(fin)\n",
    "with open('nn_2gram_feat.pkl','rb') as fin:\n",
    "    nn_train2,nn_test2 = pickle.load(fin)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 1866) (8392, 1866)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id','text','tag_txt','pos_txt','dep_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X, all_svd_train, all_nlp_train])\n",
    "test_X = np.hstack([test_X, all_svd_test, all_nlp_test])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,nn_train1,nn_train2])\n",
    "#f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,nn_test1,nn_test2])\n",
    "#f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 420\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=200,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    #test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    #weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=best_single_pred[:,0]\n",
    "    submiss['HPL']=best_single_pred[:,1]\n",
    "    submiss['MWS']=best_single_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05529\tvalid-mlogloss:1.05497\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.225782\tvalid-mlogloss:0.263446\n",
      "[400]\ttrain-mlogloss:0.180428\tvalid-mlogloss:0.256579\n",
      "[600]\ttrain-mlogloss:0.149666\tvalid-mlogloss:0.256031\n",
      "[800]\ttrain-mlogloss:0.125709\tvalid-mlogloss:0.256907\n",
      "Stopping. Best iteration:\n",
      "[693]\ttrain-mlogloss:0.13776\tvalid-mlogloss:0.255587\n",
      "\n",
      "train log loss 0.116297053862 valid log loss 0.257254700976\n",
      "rev 3.8871981589\n",
      "[0]\ttrain-mlogloss:1.05496\tvalid-mlogloss:1.05548\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.223538\tvalid-mlogloss:0.271343\n",
      "[400]\ttrain-mlogloss:0.179055\tvalid-mlogloss:0.262164\n",
      "[600]\ttrain-mlogloss:0.148997\tvalid-mlogloss:0.260695\n",
      "[800]\ttrain-mlogloss:0.12562\tvalid-mlogloss:0.260974\n",
      "Stopping. Best iteration:\n",
      "[664]\ttrain-mlogloss:0.140845\tvalid-mlogloss:0.260461\n",
      "\n",
      "train log loss 0.119141854924 valid log loss 0.261095673293\n",
      "rev 3.83001367808\n",
      "[0]\ttrain-mlogloss:1.05522\tvalid-mlogloss:1.0557\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.224789\tvalid-mlogloss:0.266494\n",
      "[400]\ttrain-mlogloss:0.180454\tvalid-mlogloss:0.258107\n",
      "[600]\ttrain-mlogloss:0.150132\tvalid-mlogloss:0.257214\n",
      "Stopping. Best iteration:\n",
      "[550]\ttrain-mlogloss:0.157095\tvalid-mlogloss:0.256789\n",
      "\n",
      "train log loss 0.131708644656 valid log loss 0.257359158838\n",
      "rev 3.88562040891\n",
      "[0]\ttrain-mlogloss:1.0553\tvalid-mlogloss:1.05587\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.224292\tvalid-mlogloss:0.268386\n",
      "[400]\ttrain-mlogloss:0.179969\tvalid-mlogloss:0.259415\n",
      "[600]\ttrain-mlogloss:0.149489\tvalid-mlogloss:0.256462\n",
      "[800]\ttrain-mlogloss:0.126069\tvalid-mlogloss:0.256339\n",
      "[1000]\ttrain-mlogloss:0.107392\tvalid-mlogloss:0.25664\n",
      "Stopping. Best iteration:\n",
      "[857]\ttrain-mlogloss:0.120467\tvalid-mlogloss:0.25599\n",
      "\n",
      "train log loss 0.102543139954 valid log loss 0.256810266405\n",
      "rev 3.89392532471\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05515\tvalid-mlogloss:1.05534\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.226992\tvalid-mlogloss:0.261396\n",
      "[400]\ttrain-mlogloss:0.182558\tvalid-mlogloss:0.249545\n",
      "[600]\ttrain-mlogloss:0.151893\tvalid-mlogloss:0.245761\n",
      "[800]\ttrain-mlogloss:0.128087\tvalid-mlogloss:0.244051\n",
      "[1000]\ttrain-mlogloss:0.108875\tvalid-mlogloss:0.244306\n",
      "Stopping. Best iteration:\n",
      "[896]\ttrain-mlogloss:0.118623\tvalid-mlogloss:0.243694\n",
      "\n",
      "train log loss 0.10076510877 valid log loss 0.244329955266\n",
      "rev 4.09282602664\n",
      "BETTER\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.011727  0.002051  0.986223\n",
      "1  id24541  0.999162  0.000546  0.000292\n",
      "2  id00134  0.002019  0.997242  0.000739\n",
      "3  id27757  0.899273  0.098155  0.002573\n",
      "4  id04081  0.751804  0.156381  0.091815\n",
      "--------------\n",
      "19.5895835972\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.011687  0.002061  0.986252\n",
      "1  id24541  0.999160  0.000549  0.000291\n",
      "2  id00134  0.002017  0.997241  0.000741\n",
      "3  id27757  0.898895  0.098526  0.002579\n",
      "4  id04081  0.751690  0.156153  0.092157\n",
      "---------------\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.006884  0.002893  0.990223\n",
      "1  id24541  0.998985  0.000798  0.000217\n",
      "2  id00134  0.001564  0.997607  0.000829\n",
      "3  id27757  0.856177  0.140714  0.003110\n",
      "4  id04081  0.746810  0.137129  0.116061\n",
      "---------------\n",
      "local average valid loss 0.255369950956\n",
      "train log loss 0.130490057061\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cv_test(10, True)\n",
    "# 276xx not good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
