{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 18) (8392, 18)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('nn_feat.pkl','rb') as fin:\n",
    "    all_nn_train,all_nn_test = pickle.load(fin)\n",
    "print(all_nn_train.shape,all_nn_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:26<00:00,  1.00s/it]\n",
      "100%|██████████| 26/26 [01:26<00:00,  3.32s/it]\n",
      "100%|██████████| 26/26 [08:31<00:00, 19.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:11<00:00,  2.23it/s]\n",
      "100%|██████████| 26/26 [00:28<00:00,  1.09s/it]\n",
      "100%|██████████| 26/26 [03:44<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 26685) (8392, 26685)\n",
      "Index(['id', 'text', 'author', 'n_.', 'n_...', 'n_,', 'n_:', 'n_;', 'n_-',\n",
      "       'n_?', 'n_!', 'n_'', 'n_\"', 'n_The ', 'n_I ', 'n_It ', 'n_He ', 'n_Me ',\n",
      "       'n_She ', 'n_We ', 'n_They ', 'n_You ', 'n_the', 'n_ a ', 'n_appear',\n",
      "       'n_little', 'n_was ', 'n_one ', 'n_two ', 'n_three ', 'n_ten ', 'n_is ',\n",
      "       'n_are ', 'n_ed', 'n_however', 'n_ to ', 'n_into', 'n_about ', 'n_th',\n",
      "       'n_er', 'n_ex', 'n_an ', 'n_ground', 'n_any', 'n_silence', 'n_wall'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "def extract_features(df, train_flag=False):\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    # Find numbers of different combinations\n",
    "    for c in tqdm(alphabet.upper()):\n",
    "        new_df['n_' + c] = new_df['text'].str.count(c)\n",
    "        new_df['n_' + c + '.'] = new_df['text'].str.count(c + '\\.')\n",
    "        new_df['n_' + c + ','] = new_df['text'].str.count(c + '\\,')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2] = new_df['text'].str.count(c + c2)\n",
    "            new_df['n_' + c + c2 + '.'] = new_df['text'].str.count(c + c2 + '\\.')\n",
    "            new_df['n_' + c + c2 + ','] = new_df['text'].str.count(c + c2 + '\\,')\n",
    "\n",
    "    for c in tqdm(alphabet):\n",
    "        new_df['n_' + c + '.'] = new_df['text'].str.count(c + '\\.')\n",
    "        new_df['n_' + c + ','] = new_df['text'].str.count(c + '\\,')\n",
    "        new_df['n_' + c + '?'] = new_df['text'].str.count(c + '\\?')\n",
    "        new_df['n_' + c + ';'] = new_df['text'].str.count(c + '\\;')\n",
    "        new_df['n_' + c + ':'] = new_df['text'].str.count(c + '\\:')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2 + '.'] = new_df['text'].str.count(c + c2 + '\\.')\n",
    "            new_df['n_' + c + c2 + ','] = new_df['text'].str.count(c + c2 + '\\,')\n",
    "            new_df['n_' + c + c2 + '?'] = new_df['text'].str.count(c + c2 + '\\?')\n",
    "            new_df['n_' + c + c2 + ';'] = new_df['text'].str.count(c + c2 + '\\;')\n",
    "            new_df['n_' + c + c2 + ':'] = new_df['text'].str.count(c + c2 + '\\:')\n",
    "            new_df['n_' + c + ', ' + c2] = new_df['text'].str.count(c + '\\, ' + c2)\n",
    "\n",
    "    # And now starting processing of cleaned text\n",
    "    for c in tqdm(alphabet):\n",
    "        new_df['n_' + c] = new_df['text_cleaned'].str.count(c)\n",
    "        new_df['n_' + c + ' '] = new_df['text_cleaned'].str.count(c + ' ')\n",
    "        new_df['n_' + ' ' + c] = new_df['text_cleaned'].str.count(' ' + c)\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            new_df['n_' + c + c2] = new_df['text_cleaned'].str.count(c + c2)\n",
    "            new_df['n_' + c + c2 + ' '] = new_df['text_cleaned'].str.count(c + c2 + ' ')\n",
    "            new_df['n_' + ' ' + c + c2] = new_df['text_cleaned'].str.count(' ' + c + c2)\n",
    "            new_df['n_' + c + ' ' + c2] = new_df['text_cleaned'].str.count(c + ' ' + c2)\n",
    "\n",
    "            for c3 in alphabet:\n",
    "                new_df['n_' + c + c2 + c3] = new_df['text_cleaned'].str.count(c + c2 + c3)\n",
    "                \n",
    "    if train_flag:\n",
    "        new_df.drop(['text_cleaned','text','author','id'], axis=1, inplace=True)\n",
    "    else:\n",
    "        new_df.drop(['text_cleaned','text','id'], axis=1, inplace=True)\n",
    "        \n",
    "    df.drop(['text_cleaned'],axis=1,inplace=True)\n",
    "    return new_df.values\n",
    "    \n",
    "print('Processing train...')\n",
    "train_hand_features = extract_features(train_df,train_flag=True)\n",
    "print('Processing test...')\n",
    "test_hand_features = extract_features(test_df)\n",
    "print(train_hand_features.shape,test_hand_features.shape)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' DET VERB NOUN ADJ NOUN PUNCT', ' DT VBZ NN JJ NNS .', ' nsubj ROOT nmod amod attr punct')\n",
      "train done 314.5725464820862\n",
      "test done 148.73289585113525\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/models#usage-import\n",
    "# https://spacy.io/usage/models\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# change ne to tag\n",
    "def get_spacy_text(s):\n",
    "    pos,tag,dep = '','',''\n",
    "    for token in spacy_nlp(s):\n",
    "        pos = pos + ' ' + token.pos_\n",
    "        tag = tag + ' ' + token.tag_\n",
    "        dep = dep + ' ' + token.dep_\n",
    "\n",
    "    return pos,tag,dep\n",
    "\n",
    "print(get_spacy_text('this is kaggle spooky games.'))\n",
    "\n",
    "import time\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in train_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "train_df['pos_txt'],train_df['tag_txt'],train_df['dep_txt'] = poss, tags, deps\n",
    "print('train done',time.time() - start_t)\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in test_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "test_df['pos_txt'],test_df['tag_txt'],test_df['dep_txt'] = poss, tags, deps\n",
    "print('test done', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n",
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n"
     ]
    }
   ],
   "source": [
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "c_vec4.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on dep\n",
    "c_vec7 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec7.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_cvec7 = c_vec7.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_cvec7 = c_vec7.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_cvec7.shape,test_cvec7.shape)\n",
    "\n",
    "# tfidf on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "# tfidf on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "tf_vec6.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)\n",
    "\n",
    "# tfidf on dep\n",
    "tf_vec8 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec8.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_tf8 = tf_vec8.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_tf8 = tf_vec8.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_tf8.shape,test_tf8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp feat done\n"
     ]
    }
   ],
   "source": [
    "all_nlp_train = np.hstack([train_cvec3,train_cvec4,train_tf5,train_tf6,train_cvec7, train_tf8]) \n",
    "all_nlp_test = np.hstack([test_cvec3,test_cvec4,test_tf5,test_tf6, test_cvec7, test_tf8]) \n",
    "print('nlp feat done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'author', 'n_.', 'n_...', 'n_,', 'n_:', 'n_;', 'n_-',\n",
      "       'n_?', 'n_!', 'n_'', 'n_\"', 'n_The ', 'n_I ', 'n_It ', 'n_He ', 'n_Me ',\n",
      "       'n_She ', 'n_We ', 'n_They ', 'n_You ', 'n_the', 'n_ a ', 'n_appear',\n",
      "       'n_little', 'n_was ', 'n_one ', 'n_two ', 'n_three ', 'n_ten ', 'n_is ',\n",
      "       'n_are ', 'n_ed', 'n_however', 'n_ to ', 'n_into', 'n_about ', 'n_th',\n",
      "       'n_er', 'n_ex', 'n_an ', 'n_ground', 'n_any', 'n_silence', 'n_wall',\n",
      "       'pos_txt', 'tag_txt', 'dep_txt', 'num_words', 'num_unique_words',\n",
      "       'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper',\n",
      "       'num_words_title', 'mean_word_len', 'unique_r', 'w_p', 'w_p_r',\n",
      "       'stop_r', 'w_p_stop', 'w_p_stop_r', 'num_words_upper_r',\n",
      "       'num_words_title_r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# add features\n",
    "def add_feat(df):\n",
    "    df['unique_r'] = df['num_unique_words'] / df['num_words']\n",
    "    df['w_p'] = df['num_words'] - df['num_punctuations']\n",
    "    df['w_p_r'] = df['w_p'] / df['num_words']\n",
    "    df['stop_r'] = df['num_stopwords'] / df['num_words']\n",
    "    df['w_p_stop'] = df['w_p'] - df['num_stopwords']\n",
    "    df['w_p_stop_r'] = df['w_p_stop'] / df['num_words']\n",
    "    df['num_words_upper_r'] = df['num_words_upper'] / df['num_words']\n",
    "    df['num_words_title_r'] = df['num_words_title'] / df['num_words']\n",
    "\n",
    "add_feat(train_df)\n",
    "add_feat(test_df)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "full_cvec1 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "## add svd3\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec1)\n",
    "train_svd3 = pd.DataFrame(svd_obj.transform(train_cvec))\n",
    "test_svd3 = pd.DataFrame(svd_obj.transform(test_cvec))\n",
    "\n",
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "full_cvec2 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "## add svd4\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec2)\n",
    "train_svd4 = pd.DataFrame(svd_obj.transform(train_cvec2))\n",
    "test_svd4 = pd.DataFrame(svd_obj.transform(test_cvec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 1068) (8392, 1068)\n"
     ]
    }
   ],
   "source": [
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(1,2), analyzer='char',max_df=0.8)\n",
    "full_cvec3 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec3 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec3 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec3.shape,test_cvec3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_svd_train = np.hstack([train_svd,train_svd2,train_svd3,train_svd4,train_cvec3.toarray()])\n",
    "all_svd_test = np.hstack([test_svd,test_svd2,test_svd3,test_svd4,test_cvec3.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 15) (8392, 15)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "        \n",
    "        # hand feature to nb\n",
    "        X_train, X_test = train_hand_features[train_index], train_hand_features[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_hand_features)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train,hand_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test,hand_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "\n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 1848) (8392, 1848)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id','text','tag_txt','pos_txt','dep_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X, all_svd_train, all_nlp_train])\n",
    "test_X = np.hstack([test_X, all_svd_test, all_nlp_test])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n",
    "#f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n",
    "#f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 420\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=200,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    #test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    #weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=best_single_pred[:,0]\n",
    "    submiss['HPL']=best_single_pred[:,1]\n",
    "    submiss['MWS']=best_single_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05514\tvalid-mlogloss:1.05525\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.225724\tvalid-mlogloss:0.263544\n",
      "[400]\ttrain-mlogloss:0.180512\tvalid-mlogloss:0.256156\n",
      "[600]\ttrain-mlogloss:0.150557\tvalid-mlogloss:0.2546\n",
      "[800]\ttrain-mlogloss:0.127049\tvalid-mlogloss:0.255149\n",
      "Stopping. Best iteration:\n",
      "[609]\ttrain-mlogloss:0.149469\tvalid-mlogloss:0.254431\n",
      "\n",
      "train log loss 0.126073148186 valid log loss 0.255262673679\n",
      "rev 3.91753320447\n",
      "[0]\ttrain-mlogloss:1.05502\tvalid-mlogloss:1.05547\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.223454\tvalid-mlogloss:0.271227\n",
      "[400]\ttrain-mlogloss:0.17933\tvalid-mlogloss:0.262546\n",
      "[600]\ttrain-mlogloss:0.14935\tvalid-mlogloss:0.260836\n",
      "[800]\ttrain-mlogloss:0.125979\tvalid-mlogloss:0.26116\n",
      "Stopping. Best iteration:\n",
      "[660]\ttrain-mlogloss:0.141579\tvalid-mlogloss:0.260571\n",
      "\n",
      "train log loss 0.119874467529 valid log loss 0.261384360355\n",
      "rev 3.82578360328\n",
      "[0]\ttrain-mlogloss:1.05508\tvalid-mlogloss:1.05556\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.224261\tvalid-mlogloss:0.265359\n",
      "[400]\ttrain-mlogloss:0.179706\tvalid-mlogloss:0.256812\n",
      "[600]\ttrain-mlogloss:0.150275\tvalid-mlogloss:0.255444\n",
      "[800]\ttrain-mlogloss:0.126662\tvalid-mlogloss:0.255286\n",
      "Stopping. Best iteration:\n",
      "[756]\ttrain-mlogloss:0.131352\tvalid-mlogloss:0.255037\n",
      "\n",
      "train log loss 0.111100862958 valid log loss 0.256335804524\n",
      "rev 3.9011327421\n",
      "[0]\ttrain-mlogloss:1.05501\tvalid-mlogloss:1.05555\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.22367\tvalid-mlogloss:0.270543\n",
      "[400]\ttrain-mlogloss:0.179706\tvalid-mlogloss:0.261378\n",
      "[600]\ttrain-mlogloss:0.149818\tvalid-mlogloss:0.259115\n",
      "[800]\ttrain-mlogloss:0.126407\tvalid-mlogloss:0.258634\n",
      "[1000]\ttrain-mlogloss:0.107371\tvalid-mlogloss:0.259329\n",
      "Stopping. Best iteration:\n",
      "[811]\ttrain-mlogloss:0.125316\tvalid-mlogloss:0.258494\n",
      "\n",
      "train log loss 0.106348716133 valid log loss 0.259395567645\n",
      "rev 3.85511598783\n",
      "[0]\ttrain-mlogloss:1.05514\tvalid-mlogloss:1.05539\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.226597\tvalid-mlogloss:0.260791\n",
      "[400]\ttrain-mlogloss:0.1822\tvalid-mlogloss:0.250089\n",
      "[600]\ttrain-mlogloss:0.152334\tvalid-mlogloss:0.24688\n",
      "[800]\ttrain-mlogloss:0.128414\tvalid-mlogloss:0.245445\n",
      "Stopping. Best iteration:\n",
      "[776]\ttrain-mlogloss:0.130939\tvalid-mlogloss:0.245272\n",
      "\n",
      "train log loss 0.111347265814 valid log loss 0.245735423016\n",
      "rev 4.0694173747\n",
      "BETTER\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.023671  0.003180  0.973150\n",
      "1  id24541  0.998881  0.000699  0.000421\n",
      "2  id00134  0.001768  0.997596  0.000636\n",
      "3  id27757  0.872622  0.125121  0.002257\n",
      "4  id04081  0.733627  0.162124  0.104249\n",
      "--------------\n",
      "19.5689829124\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.023619  0.003194  0.973187\n",
      "1  id24541  0.998876  0.000704  0.000420\n",
      "2  id00134  0.001776  0.997586  0.000638\n",
      "3  id27757  0.872242  0.125485  0.002273\n",
      "4  id04081  0.733949  0.161570  0.104481\n",
      "---------------\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.023619  0.003194  0.973187\n",
      "1  id24541  0.998876  0.000704  0.000420\n",
      "2  id00134  0.001776  0.997586  0.000638\n",
      "3  id27757  0.872242  0.125485  0.002273\n",
      "4  id04081  0.733949  0.161570  0.104481\n",
      "---------------\n",
      "local average valid loss 0.255622765844\n",
      "train log loss 0.131476192904\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05509\tvalid-mlogloss:1.05539\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.227781\tvalid-mlogloss:0.271576\n",
      "[400]\ttrain-mlogloss:0.186057\tvalid-mlogloss:0.262812\n",
      "[600]\ttrain-mlogloss:0.15689\tvalid-mlogloss:0.261084\n",
      "Stopping. Best iteration:\n",
      "[565]\ttrain-mlogloss:0.161752\tvalid-mlogloss:0.260695\n",
      "\n",
      "train log loss 0.137835699792 valid log loss 0.261752301899\n",
      "rev 3.82040575287\n",
      "[0]\ttrain-mlogloss:1.05521\tvalid-mlogloss:1.05519\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.229082\tvalid-mlogloss:0.256533\n",
      "[400]\ttrain-mlogloss:0.186233\tvalid-mlogloss:0.250211\n",
      "[600]\ttrain-mlogloss:0.15788\tvalid-mlogloss:0.249059\n",
      "Stopping. Best iteration:\n",
      "[592]\ttrain-mlogloss:0.158771\tvalid-mlogloss:0.248945\n",
      "\n",
      "train log loss 0.135850226711 valid log loss 0.249636900683\n",
      "rev 4.00581803917\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05489\tvalid-mlogloss:1.05572\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.225881\tvalid-mlogloss:0.287205\n",
      "[400]\ttrain-mlogloss:0.183851\tvalid-mlogloss:0.280954\n",
      "[600]\ttrain-mlogloss:0.155647\tvalid-mlogloss:0.280467\n",
      "Stopping. Best iteration:\n",
      "[526]\ttrain-mlogloss:0.165341\tvalid-mlogloss:0.279888\n",
      "\n",
      "train log loss 0.140796206522 valid log loss 0.28241780524\n",
      "rev 3.54085323746\n",
      "[0]\ttrain-mlogloss:1.05516\tvalid-mlogloss:1.05489\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.229926\tvalid-mlogloss:0.253754\n",
      "[400]\ttrain-mlogloss:0.188535\tvalid-mlogloss:0.24347\n",
      "[600]\ttrain-mlogloss:0.160235\tvalid-mlogloss:0.240655\n",
      "[800]\ttrain-mlogloss:0.137626\tvalid-mlogloss:0.239799\n",
      "Stopping. Best iteration:\n",
      "[770]\ttrain-mlogloss:0.140765\tvalid-mlogloss:0.239588\n",
      "\n",
      "train log loss 0.12135414518 valid log loss 0.239933009857\n",
      "rev 4.1678300147\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05516\tvalid-mlogloss:1.05507\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.230066\tvalid-mlogloss:0.24863\n",
      "[400]\ttrain-mlogloss:0.187759\tvalid-mlogloss:0.238692\n",
      "[600]\ttrain-mlogloss:0.159237\tvalid-mlogloss:0.23735\n",
      "[800]\ttrain-mlogloss:0.136299\tvalid-mlogloss:0.237307\n",
      "[1000]\ttrain-mlogloss:0.117352\tvalid-mlogloss:0.237527\n",
      "Stopping. Best iteration:\n",
      "[895]\ttrain-mlogloss:0.126793\tvalid-mlogloss:0.236803\n",
      "\n",
      "train log loss 0.109529660039 valid log loss 0.237902886388\n",
      "rev 4.20339582753\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05516\tvalid-mlogloss:1.05611\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.225795\tvalid-mlogloss:0.283561\n",
      "[400]\ttrain-mlogloss:0.18426\tvalid-mlogloss:0.275373\n",
      "[600]\ttrain-mlogloss:0.156151\tvalid-mlogloss:0.273443\n",
      "[800]\ttrain-mlogloss:0.133972\tvalid-mlogloss:0.273447\n",
      "Stopping. Best iteration:\n",
      "[617]\ttrain-mlogloss:0.154049\tvalid-mlogloss:0.272829\n",
      "\n",
      "train log loss 0.132339484129 valid log loss 0.27365933467\n",
      "rev 3.65417829143\n",
      "[0]\ttrain-mlogloss:1.05503\tvalid-mlogloss:1.05599\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.225893\tvalid-mlogloss:0.2845\n",
      "[400]\ttrain-mlogloss:0.18444\tvalid-mlogloss:0.27467\n",
      "[600]\ttrain-mlogloss:0.156415\tvalid-mlogloss:0.272224\n",
      "[800]\ttrain-mlogloss:0.134004\tvalid-mlogloss:0.271078\n",
      "[1000]\ttrain-mlogloss:0.115273\tvalid-mlogloss:0.270973\n",
      "Stopping. Best iteration:\n",
      "[972]\ttrain-mlogloss:0.117837\tvalid-mlogloss:0.270756\n",
      "\n",
      "train log loss 0.101841311155 valid log loss 0.272565144019\n",
      "rev 3.66884769364\n",
      "[0]\ttrain-mlogloss:1.055\tvalid-mlogloss:1.05536\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.228924\tvalid-mlogloss:0.256841\n",
      "[400]\ttrain-mlogloss:0.187228\tvalid-mlogloss:0.245844\n",
      "[600]\ttrain-mlogloss:0.158417\tvalid-mlogloss:0.242739\n",
      "[800]\ttrain-mlogloss:0.135637\tvalid-mlogloss:0.242714\n",
      "[1000]\ttrain-mlogloss:0.116503\tvalid-mlogloss:0.243338\n",
      "Stopping. Best iteration:\n",
      "[823]\ttrain-mlogloss:0.13329\tvalid-mlogloss:0.242405\n",
      "\n",
      "train log loss 0.114661811964 valid log loss 0.24330731614\n",
      "rev 4.11002848523\n",
      "[0]\ttrain-mlogloss:1.05509\tvalid-mlogloss:1.05504\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.229646\tvalid-mlogloss:0.25286\n",
      "[400]\ttrain-mlogloss:0.187654\tvalid-mlogloss:0.240741\n",
      "[600]\ttrain-mlogloss:0.159074\tvalid-mlogloss:0.237392\n",
      "[800]\ttrain-mlogloss:0.136259\tvalid-mlogloss:0.236482\n",
      "[1000]\ttrain-mlogloss:0.117433\tvalid-mlogloss:0.235392\n",
      "[1200]\ttrain-mlogloss:0.101931\tvalid-mlogloss:0.235373\n",
      "Stopping. Best iteration:\n",
      "[1168]\ttrain-mlogloss:0.104301\tvalid-mlogloss:0.235046\n",
      "\n",
      "train log loss 0.0906305413885 valid log loss 0.236085464756\n",
      "rev 4.23575420466\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.0551\tvalid-mlogloss:1.05539\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 200 rounds.\n",
      "[200]\ttrain-mlogloss:0.228321\tvalid-mlogloss:0.265789\n",
      "[400]\ttrain-mlogloss:0.186724\tvalid-mlogloss:0.251739\n",
      "[600]\ttrain-mlogloss:0.158654\tvalid-mlogloss:0.247798\n",
      "[800]\ttrain-mlogloss:0.136364\tvalid-mlogloss:0.246763\n",
      "[1000]\ttrain-mlogloss:0.117633\tvalid-mlogloss:0.245991\n",
      "Stopping. Best iteration:\n",
      "[977]\ttrain-mlogloss:0.119691\tvalid-mlogloss:0.245927\n",
      "\n",
      "train log loss 0.103729724753 valid log loss 0.24601739083\n",
      "rev 4.06475329499\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.019930  0.003402  0.976668\n",
      "1  id24541  0.999024  0.000562  0.000414\n",
      "2  id00134  0.001859  0.997393  0.000748\n",
      "3  id27757  0.875121  0.122403  0.002476\n",
      "4  id04081  0.742480  0.159884  0.097635\n",
      "--------------\n",
      "39.4718648417\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.019771  0.003379  0.976850\n",
      "1  id24541  0.999027  0.000565  0.000408\n",
      "2  id00134  0.001847  0.997408  0.000745\n",
      "3  id27757  0.876034  0.121528  0.002437\n",
      "4  id04081  0.742607  0.159570  0.097824\n",
      "---------------\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.019771  0.003379  0.976850\n",
      "1  id24541  0.999027  0.000565  0.000408\n",
      "2  id00134  0.001847  0.997408  0.000745\n",
      "3  id27757  0.876034  0.121528  0.002437\n",
      "4  id04081  0.742607  0.159570  0.097824\n",
      "---------------\n",
      "local average valid loss 0.254327755448\n",
      "train log loss 0.125521644043\n"
     ]
    }
   ],
   "source": [
    "# cv_test(10, True)\n",
    "# 276xx not good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
