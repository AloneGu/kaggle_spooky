{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:29<00:00,  1.12s/it]\n",
      "100%|██████████| 26/26 [01:04<00:00,  2.49s/it]\n",
      "100%|██████████| 26/26 [08:07<00:00, 18.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:11<00:00,  2.34it/s]\n",
      "100%|██████████| 26/26 [00:27<00:00,  1.07s/it]\n",
      "100%|██████████| 26/26 [03:38<00:00,  8.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 26685) (8392, 26685)\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "def extract_features(in_df, train_flag=False):\n",
    "    df = in_df.copy()\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "    \n",
    "    # Find numbers of different combinations\n",
    "    for c in tqdm(alphabet.upper()):\n",
    "        df['n_' + c] = df['text'].str.count(c)\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "        df['n_' + c + '?'] = df['text'].str.count(c + '\\?')\n",
    "        df['n_' + c + ';'] = df['text'].str.count(c + '\\;')\n",
    "        df['n_' + c + ':'] = df['text'].str.count(c + '\\:')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "            df['n_' + c + c2 + '?'] = df['text'].str.count(c + c2 + '\\?')\n",
    "            df['n_' + c + c2 + ';'] = df['text'].str.count(c + c2 + '\\;')\n",
    "            df['n_' + c + c2 + ':'] = df['text'].str.count(c + c2 + '\\:')\n",
    "            df['n_' + c + ', ' + c2] = df['text'].str.count(c + '\\, ' + c2)\n",
    "\n",
    "    # And now starting processing of cleaned text\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c] = df['text_cleaned'].str.count(c)\n",
    "        df['n_' + c + ' '] = df['text_cleaned'].str.count(c + ' ')\n",
    "        df['n_' + ' ' + c] = df['text_cleaned'].str.count(' ' + c)\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text_cleaned'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + ' '] = df['text_cleaned'].str.count(c + c2 + ' ')\n",
    "            df['n_' + ' ' + c + c2] = df['text_cleaned'].str.count(' ' + c + c2)\n",
    "            df['n_' + c + ' ' + c2] = df['text_cleaned'].str.count(c + ' ' + c2)\n",
    "\n",
    "            for c3 in alphabet:\n",
    "                df['n_' + c + c2 + c3] = df['text_cleaned'].str.count(c + c2 + c3)\n",
    "                \n",
    "    if train_flag:\n",
    "        df.drop(['text_cleaned','text','author','id'], axis=1, inplace=True)\n",
    "    else:\n",
    "        df.drop(['text_cleaned','text','id'], axis=1, inplace=True)\n",
    "    return df.values\n",
    "    \n",
    "print('Processing train...')\n",
    "train_hand_features = extract_features(train_df,train_flag=True)\n",
    "print('Processing test...')\n",
    "test_hand_features = extract_features(test_df)\n",
    "print(train_hand_features.shape,test_hand_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Processing test...\n"
     ]
    }
   ],
   "source": [
    "def extract_features_2(df):\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['len'] = df['text'].apply(lambda x: len(x))\n",
    "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "\n",
    "    df.drop(['text_cleaned'], axis=1, inplace=True)\n",
    "\n",
    "print('Processing train...')\n",
    "extract_features_2(train_df)\n",
    "print('Processing test...')\n",
    "extract_features_2(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' DET VERB NOUN ADJ NOUN PUNCT', ' DT VBZ NN JJ NNS .', ' nsubj ROOT nmod amod attr punct')\n",
      "train done 289.98655915260315\n",
      "test done 134.5491213798523\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/models#usage-import\n",
    "# https://spacy.io/usage/models\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# change ne to tag\n",
    "def get_spacy_text(s):\n",
    "    pos,tag,dep = '','',''\n",
    "    for token in spacy_nlp(s):\n",
    "        pos = pos + ' ' + token.pos_\n",
    "        tag = tag + ' ' + token.tag_\n",
    "        dep = dep + ' ' + token.dep_\n",
    "\n",
    "    return pos,tag,dep\n",
    "\n",
    "print(get_spacy_text('this is kaggle spooky games.'))\n",
    "\n",
    "import time\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in train_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "train_df['pos_txt'],train_df['tag_txt'],train_df['dep_txt'] = poss, tags, deps\n",
    "print('train done',time.time() - start_t)\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in test_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "test_df['pos_txt'],test_df['tag_txt'],test_df['dep_txt'] = poss, tags, deps\n",
    "print('test done', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n",
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n"
     ]
    }
   ],
   "source": [
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "c_vec4.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on dep\n",
    "c_vec7 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec7.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_cvec7 = c_vec7.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_cvec7 = c_vec7.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_cvec7.shape,test_cvec7.shape)\n",
    "\n",
    "# tfidf on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "# tfidf on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "tf_vec6.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)\n",
    "\n",
    "# tfidf on dep\n",
    "tf_vec8 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec8.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_tf8 = tf_vec8.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_tf8 = tf_vec8.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_tf8.shape,test_tf8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp feat done\n"
     ]
    }
   ],
   "source": [
    "all_nlp_train = np.hstack([train_cvec3,train_cvec4,train_tf5,train_tf6,train_cvec7, train_tf8]) \n",
    "all_nlp_test = np.hstack([test_cvec3,test_cvec4,test_tf5,test_tf6, test_cvec7, test_tf8]) \n",
    "print('nlp feat done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "full_cvec1 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "## add svd3\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_cvec1)\n",
    "train_svd3 = pd.DataFrame(svd_obj.transform(train_cvec))\n",
    "test_svd3 = pd.DataFrame(svd_obj.transform(test_cvec))\n",
    "\n",
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "full_cvec2 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "## add svd4\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_cvec2)\n",
    "train_svd4 = pd.DataFrame(svd_obj.transform(train_cvec2))\n",
    "test_svd4 = pd.DataFrame(svd_obj.transform(test_cvec2))\n",
    "\n",
    "all_svd_train = np.hstack([train_svd,train_svd2,train_svd3,train_svd4])\n",
    "all_svd_test = np.hstack([test_svd,test_svd2,test_svd3,test_svd4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 15) (8392, 15)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "        \n",
    "        # hand feature to nb\n",
    "        X_train, X_test = train_hand_features[train_index], train_hand_features[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_hand_features)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train,hand_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test,hand_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "\n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.06339, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0855 - acc: 0.4022 - val_loss: 1.0634 - val_acc: 0.4110\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.06339 to 0.84664, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9634 - acc: 0.5223 - val_loss: 0.8466 - val_acc: 0.6720\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.84664 to 0.60759, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6816 - acc: 0.7548 - val_loss: 0.6076 - val_acc: 0.7703\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.60759 to 0.48770, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4627 - acc: 0.8370 - val_loss: 0.4877 - val_acc: 0.8149\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.48770 to 0.44639, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3279 - acc: 0.8878 - val_loss: 0.4464 - val_acc: 0.8341\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2460 - acc: 0.9219 - val_loss: 0.4502 - val_acc: 0.8360\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1940 - acc: 0.9396 - val_loss: 0.4571 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1555 - acc: 0.9507 - val_loss: 0.4804 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1276 - acc: 0.9624 - val_loss: 0.4954 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1069 - acc: 0.9679 - val_loss: 0.5270 - val_acc: 0.8366\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04945, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0813 - acc: 0.4128 - val_loss: 1.0495 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04945 to 0.76797, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9213 - acc: 0.5922 - val_loss: 0.7680 - val_acc: 0.7147\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76797 to 0.51774, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5798 - acc: 0.7943 - val_loss: 0.5177 - val_acc: 0.8054\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51774 to 0.43819, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3703 - acc: 0.8762 - val_loss: 0.4382 - val_acc: 0.8251\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43819 to 0.41412, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2662 - acc: 0.9127 - val_loss: 0.4141 - val_acc: 0.8366\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2030 - acc: 0.9347 - val_loss: 0.4155 - val_acc: 0.8437\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1618 - acc: 0.9482 - val_loss: 0.4301 - val_acc: 0.8417\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1298 - acc: 0.9605 - val_loss: 0.4502 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1060 - acc: 0.9680 - val_loss: 0.4749 - val_acc: 0.8385\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0913 - acc: 0.9742 - val_loss: 0.4995 - val_acc: 0.8341\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03886, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0788 - acc: 0.4207 - val_loss: 1.0389 - val_acc: 0.4825\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03886 to 0.78767, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9203 - acc: 0.5855 - val_loss: 0.7877 - val_acc: 0.6886\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.78767 to 0.53528, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6020 - acc: 0.7850 - val_loss: 0.5353 - val_acc: 0.7932\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53528 to 0.44582, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3820 - acc: 0.8693 - val_loss: 0.4458 - val_acc: 0.8207\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44582 to 0.41891, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2736 - acc: 0.9071 - val_loss: 0.4189 - val_acc: 0.8392\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2073 - acc: 0.9334 - val_loss: 0.4199 - val_acc: 0.8385\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1609 - acc: 0.9491 - val_loss: 0.4303 - val_acc: 0.8366\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1302 - acc: 0.9593 - val_loss: 0.4488 - val_acc: 0.8379\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1052 - acc: 0.9691 - val_loss: 0.4758 - val_acc: 0.8373\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0846 - acc: 0.9762 - val_loss: 0.5076 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04135, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0816 - acc: 0.4117 - val_loss: 1.0414 - val_acc: 0.4761\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04135 to 0.75280, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9067 - acc: 0.5924 - val_loss: 0.7528 - val_acc: 0.7211\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75280 to 0.50267, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5801 - acc: 0.7920 - val_loss: 0.5027 - val_acc: 0.8105\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50267 to 0.42459, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3739 - acc: 0.8709 - val_loss: 0.4246 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42459 to 0.40036, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2718 - acc: 0.9088 - val_loss: 0.4004 - val_acc: 0.8437\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40036 to 0.39322, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2086 - acc: 0.9315 - val_loss: 0.3932 - val_acc: 0.8443\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1651 - acc: 0.9482 - val_loss: 0.3992 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1327 - acc: 0.9590 - val_loss: 0.4090 - val_acc: 0.8551\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1072 - acc: 0.9699 - val_loss: 0.4277 - val_acc: 0.8488\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0904 - acc: 0.9727 - val_loss: 0.4458 - val_acc: 0.8462\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03789, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0799 - acc: 0.4170 - val_loss: 1.0379 - val_acc: 0.4888\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03789 to 0.71593, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8924 - acc: 0.6089 - val_loss: 0.7159 - val_acc: 0.7441\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71593 to 0.48439, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5551 - acc: 0.8077 - val_loss: 0.4844 - val_acc: 0.8207\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48439 to 0.41277, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3624 - acc: 0.8756 - val_loss: 0.4128 - val_acc: 0.8481\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41277 to 0.39316, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2651 - acc: 0.9104 - val_loss: 0.3932 - val_acc: 0.8558\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39316 to 0.39186, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2035 - acc: 0.9326 - val_loss: 0.3919 - val_acc: 0.8539\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1560 - acc: 0.9498 - val_loss: 0.4042 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1277 - acc: 0.9616 - val_loss: 0.4348 - val_acc: 0.8443\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1031 - acc: 0.9689 - val_loss: 0.4565 - val_acc: 0.8405\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0882 - acc: 0.9743 - val_loss: 0.4868 - val_acc: 0.8405\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04611, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0806 - acc: 0.4159 - val_loss: 1.0461 - val_acc: 0.4659\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04611 to 0.81652, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9385 - acc: 0.5630 - val_loss: 0.8165 - val_acc: 0.6624\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.81652 to 0.56766, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6502 - acc: 0.7563 - val_loss: 0.5677 - val_acc: 0.7779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.56766 to 0.45039, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4181 - acc: 0.8571 - val_loss: 0.4504 - val_acc: 0.8277\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.45039 to 0.41223, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2906 - acc: 0.9032 - val_loss: 0.4122 - val_acc: 0.8373\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41223 to 0.40640, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2175 - acc: 0.9281 - val_loss: 0.4064 - val_acc: 0.8392\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1655 - acc: 0.9515 - val_loss: 0.4195 - val_acc: 0.8411\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1296 - acc: 0.9615 - val_loss: 0.4366 - val_acc: 0.8392\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1073 - acc: 0.9663 - val_loss: 0.4627 - val_acc: 0.8354\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0890 - acc: 0.9734 - val_loss: 0.4807 - val_acc: 0.8322\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04153, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0805 - acc: 0.4141 - val_loss: 1.0415 - val_acc: 0.4850\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04153 to 0.80178, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9352 - acc: 0.5699 - val_loss: 0.8018 - val_acc: 0.6694\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80178 to 0.52856, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6126 - acc: 0.7798 - val_loss: 0.5286 - val_acc: 0.7945\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52856 to 0.43187, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3820 - acc: 0.8689 - val_loss: 0.4319 - val_acc: 0.8405\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43187 to 0.39535, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2700 - acc: 0.9100 - val_loss: 0.3954 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2009 - acc: 0.9354 - val_loss: 0.3965 - val_acc: 0.8507\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1565 - acc: 0.9503 - val_loss: 0.4134 - val_acc: 0.8443\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1252 - acc: 0.9599 - val_loss: 0.4250 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1022 - acc: 0.9705 - val_loss: 0.4482 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0846 - acc: 0.9754 - val_loss: 0.4795 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04462, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0813 - acc: 0.4139 - val_loss: 1.0446 - val_acc: 0.4697\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04462 to 0.75674, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9119 - acc: 0.5948 - val_loss: 0.7567 - val_acc: 0.7262\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75674 to 0.50379, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5761 - acc: 0.7938 - val_loss: 0.5038 - val_acc: 0.8156\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50379 to 0.41877, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3700 - acc: 0.8743 - val_loss: 0.4188 - val_acc: 0.8379\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41877 to 0.39084, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2604 - acc: 0.9128 - val_loss: 0.3908 - val_acc: 0.8526\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.1995 - acc: 0.9357 - val_loss: 0.3947 - val_acc: 0.8519\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1565 - acc: 0.9515 - val_loss: 0.4078 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1271 - acc: 0.9612 - val_loss: 0.4241 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1025 - acc: 0.9692 - val_loss: 0.4516 - val_acc: 0.8513\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0869 - acc: 0.9738 - val_loss: 0.4723 - val_acc: 0.8507\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05778, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0830 - acc: 0.4111 - val_loss: 1.0578 - val_acc: 0.4391\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05778 to 0.74347, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9088 - acc: 0.5968 - val_loss: 0.7435 - val_acc: 0.7511\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.74347 to 0.50790, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5599 - acc: 0.8073 - val_loss: 0.5079 - val_acc: 0.8117\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50790 to 0.42924, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3677 - acc: 0.8732 - val_loss: 0.4292 - val_acc: 0.8392\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42924 to 0.40677, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2672 - acc: 0.9137 - val_loss: 0.4068 - val_acc: 0.8449\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40677 to 0.40599, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2043 - acc: 0.9347 - val_loss: 0.4060 - val_acc: 0.8481\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1625 - acc: 0.9483 - val_loss: 0.4207 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1322 - acc: 0.9598 - val_loss: 0.4419 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1083 - acc: 0.9681 - val_loss: 0.4612 - val_acc: 0.8475\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0902 - acc: 0.9744 - val_loss: 0.4937 - val_acc: 0.8398\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04677, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0818 - acc: 0.4092 - val_loss: 1.0468 - val_acc: 0.4665\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04677 to 0.69676, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8784 - acc: 0.6301 - val_loss: 0.6968 - val_acc: 0.7690\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.69676 to 0.49186, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5297 - acc: 0.8209 - val_loss: 0.4919 - val_acc: 0.8239\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49186 to 0.43058, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3507 - acc: 0.8802 - val_loss: 0.4306 - val_acc: 0.8373\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43058 to 0.41276, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2553 - acc: 0.9154 - val_loss: 0.4128 - val_acc: 0.8398\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.1987 - acc: 0.9349 - val_loss: 0.4137 - val_acc: 0.8424\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1572 - acc: 0.9506 - val_loss: 0.4256 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1250 - acc: 0.9614 - val_loss: 0.4521 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1035 - acc: 0.9684 - val_loss: 0.4745 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0852 - acc: 0.9757 - val_loss: 0.5059 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04564, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0813 - acc: 0.4127 - val_loss: 1.0456 - val_acc: 0.4531\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04564 to 0.71830, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8883 - acc: 0.6161 - val_loss: 0.7183 - val_acc: 0.7103\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71830 to 0.48898, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5496 - acc: 0.8082 - val_loss: 0.4890 - val_acc: 0.8149\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48898 to 0.41699, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3603 - acc: 0.8783 - val_loss: 0.4170 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41699 to 0.39186, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2593 - acc: 0.9142 - val_loss: 0.3919 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2016 - acc: 0.9341 - val_loss: 0.3937 - val_acc: 0.8468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1556 - acc: 0.9517 - val_loss: 0.4072 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1277 - acc: 0.9625 - val_loss: 0.4254 - val_acc: 0.8449\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1033 - acc: 0.9679 - val_loss: 0.4512 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0871 - acc: 0.9755 - val_loss: 0.4887 - val_acc: 0.8424\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04813, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0824 - acc: 0.4105 - val_loss: 1.0481 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04813 to 0.72585, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8985 - acc: 0.6051 - val_loss: 0.7258 - val_acc: 0.7364\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72585 to 0.49543, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5621 - acc: 0.7992 - val_loss: 0.4954 - val_acc: 0.8188\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49543 to 0.42109, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3718 - acc: 0.8724 - val_loss: 0.4211 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42109 to 0.39656, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2699 - acc: 0.9112 - val_loss: 0.3966 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39656 to 0.39502, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2043 - acc: 0.9321 - val_loss: 0.3950 - val_acc: 0.8526\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1592 - acc: 0.9478 - val_loss: 0.4043 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1322 - acc: 0.9581 - val_loss: 0.4203 - val_acc: 0.8526\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1059 - acc: 0.9679 - val_loss: 0.4449 - val_acc: 0.8443\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0869 - acc: 0.9751 - val_loss: 0.4687 - val_acc: 0.8437\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03746, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0797 - acc: 0.4188 - val_loss: 1.0375 - val_acc: 0.4761\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03746 to 0.76170, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9035 - acc: 0.5895 - val_loss: 0.7617 - val_acc: 0.6969\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76170 to 0.52602, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5903 - acc: 0.7889 - val_loss: 0.5260 - val_acc: 0.8022\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52602 to 0.43724, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3849 - acc: 0.8692 - val_loss: 0.4372 - val_acc: 0.8334\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43724 to 0.40372, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2753 - acc: 0.9072 - val_loss: 0.4037 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40372 to 0.39948, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2087 - acc: 0.9327 - val_loss: 0.3995 - val_acc: 0.8507\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1626 - acc: 0.9485 - val_loss: 0.4067 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1318 - acc: 0.9594 - val_loss: 0.4282 - val_acc: 0.8481\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1046 - acc: 0.9701 - val_loss: 0.4521 - val_acc: 0.8443\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0872 - acc: 0.9750 - val_loss: 0.4707 - val_acc: 0.8437\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03866, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0798 - acc: 0.4186 - val_loss: 1.0387 - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03866 to 0.86583, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9435 - acc: 0.5448 - val_loss: 0.8658 - val_acc: 0.5718\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.86583 to 0.75756, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7754 - acc: 0.6252 - val_loss: 0.7576 - val_acc: 0.6209\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.75756 to 0.72297, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6730 - acc: 0.6521 - val_loss: 0.7230 - val_acc: 0.6369\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72297 to 0.71789, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6048 - acc: 0.6959 - val_loss: 0.7179 - val_acc: 0.6509\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.71789 to 0.71316, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5282 - acc: 0.7689 - val_loss: 0.7132 - val_acc: 0.6707\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.4405 - acc: 0.8277 - val_loss: 0.7181 - val_acc: 0.6937\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss improved from 0.71316 to 0.69288, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3534 - acc: 0.8733 - val_loss: 0.6929 - val_acc: 0.7250\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.69288 to 0.67208, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2742 - acc: 0.9100 - val_loss: 0.6721 - val_acc: 0.7415\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.2157 - acc: 0.9303 - val_loss: 0.6800 - val_acc: 0.7511\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03465, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0791 - acc: 0.4204 - val_loss: 1.0347 - val_acc: 0.4882\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03465 to 0.76227, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9116 - acc: 0.5915 - val_loss: 0.7623 - val_acc: 0.7103\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76227 to 0.51808, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5877 - acc: 0.7918 - val_loss: 0.5181 - val_acc: 0.7958\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51808 to 0.42888, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3761 - acc: 0.8722 - val_loss: 0.4289 - val_acc: 0.8437\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42888 to 0.40496, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2678 - acc: 0.9092 - val_loss: 0.4050 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40496 to 0.40157, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2068 - acc: 0.9319 - val_loss: 0.4016 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1622 - acc: 0.9498 - val_loss: 0.4096 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1305 - acc: 0.9594 - val_loss: 0.4326 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1089 - acc: 0.9664 - val_loss: 0.4516 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0915 - acc: 0.9718 - val_loss: 0.4792 - val_acc: 0.8392\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.80975, saving model to /tmp/nn_model.h5\n",
      " - 42s - loss: 1.0312 - acc: 0.4702 - val_loss: 0.8098 - val_acc: 0.6662\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.80975 to 0.49086, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.5627 - acc: 0.7726 - val_loss: 0.4909 - val_acc: 0.8054\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.49086 to 0.43985, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.3129 - acc: 0.8851 - val_loss: 0.4399 - val_acc: 0.8271\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 40s - loss: 0.2018 - acc: 0.9296 - val_loss: 0.4679 - val_acc: 0.8398\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 39s - loss: 0.1430 - acc: 0.9496 - val_loss: 0.4951 - val_acc: 0.8334\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 39s - loss: 0.1059 - acc: 0.9640 - val_loss: 0.5373 - val_acc: 0.8411\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.79648, saving model to /tmp/nn_model.h5\n",
      " - 43s - loss: 1.0022 - acc: 0.4812 - val_loss: 0.7965 - val_acc: 0.6790\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.79648 to 0.49361, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 0.5794 - acc: 0.7660 - val_loss: 0.4936 - val_acc: 0.8073\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.49361 to 0.45105, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.3137 - acc: 0.8824 - val_loss: 0.4511 - val_acc: 0.8207\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 41s - loss: 0.2062 - acc: 0.9256 - val_loss: 0.5292 - val_acc: 0.8124\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 40s - loss: 0.1416 - acc: 0.9504 - val_loss: 0.5431 - val_acc: 0.8156\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 41s - loss: 0.1033 - acc: 0.9650 - val_loss: 0.6277 - val_acc: 0.8200\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.77758, saving model to /tmp/nn_model.h5\n",
      " - 44s - loss: 1.0088 - acc: 0.4835 - val_loss: 0.7776 - val_acc: 0.6765\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.77758 to 0.46273, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.5540 - acc: 0.7826 - val_loss: 0.4627 - val_acc: 0.8143\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.46273 to 0.44165, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 0.3007 - acc: 0.8906 - val_loss: 0.4416 - val_acc: 0.8264\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 41s - loss: 0.1979 - acc: 0.9288 - val_loss: 0.4904 - val_acc: 0.8245\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 41s - loss: 0.1381 - acc: 0.9522 - val_loss: 0.5391 - val_acc: 0.8194\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 40s - loss: 0.1006 - acc: 0.9651 - val_loss: 0.6051 - val_acc: 0.8111\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.77718, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 1.0009 - acc: 0.4948 - val_loss: 0.7772 - val_acc: 0.6624\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.77718 to 0.50149, saving model to /tmp/nn_model.h5\n",
      " - 38s - loss: 0.5469 - acc: 0.7811 - val_loss: 0.5015 - val_acc: 0.7996\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.50149 to 0.46363, saving model to /tmp/nn_model.h5\n",
      " - 39s - loss: 0.3008 - acc: 0.8887 - val_loss: 0.4636 - val_acc: 0.8181\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 38s - loss: 0.1937 - acc: 0.9305 - val_loss: 0.4935 - val_acc: 0.8239\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 38s - loss: 0.1361 - acc: 0.9558 - val_loss: 0.5116 - val_acc: 0.8290\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 39s - loss: 0.1026 - acc: 0.9664 - val_loss: 0.6040 - val_acc: 0.8251\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.81076, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 1.0005 - acc: 0.4841 - val_loss: 0.8108 - val_acc: 0.6503\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.81076 to 0.50715, saving model to /tmp/nn_model.h5\n",
      " - 38s - loss: 0.6035 - acc: 0.7475 - val_loss: 0.5071 - val_acc: 0.7926\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.50715 to 0.44042, saving model to /tmp/nn_model.h5\n",
      " - 38s - loss: 0.3261 - acc: 0.8758 - val_loss: 0.4404 - val_acc: 0.8220\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 39s - loss: 0.2122 - acc: 0.9248 - val_loss: 0.4701 - val_acc: 0.8309\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 39s - loss: 0.1492 - acc: 0.9481 - val_loss: 0.5249 - val_acc: 0.8271\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 39s - loss: 0.1067 - acc: 0.9618 - val_loss: 0.5908 - val_acc: 0.8341\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=128, epochs=6, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07740, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0881 - acc: 0.3928 - val_loss: 1.0774 - val_acc: 0.4035\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07740 to 0.99502, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0476 - acc: 0.4467 - val_loss: 0.9950 - val_acc: 0.5471\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99502 to 0.79465, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8821 - acc: 0.6530 - val_loss: 0.7946 - val_acc: 0.7004\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79465 to 0.64252, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6575 - acc: 0.7758 - val_loss: 0.6425 - val_acc: 0.7531\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64252 to 0.55364, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5029 - acc: 0.8329 - val_loss: 0.5536 - val_acc: 0.7855\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55364 to 0.50103, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3982 - acc: 0.8708 - val_loss: 0.5010 - val_acc: 0.8049\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50103 to 0.46828, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3237 - acc: 0.8994 - val_loss: 0.4683 - val_acc: 0.8125\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46828 to 0.44593, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2686 - acc: 0.9151 - val_loss: 0.4459 - val_acc: 0.8202\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44593 to 0.43421, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2261 - acc: 0.9332 - val_loss: 0.4342 - val_acc: 0.8219\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43421 to 0.42620, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1921 - acc: 0.9438 - val_loss: 0.4262 - val_acc: 0.8295\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1634 - acc: 0.9531 - val_loss: 0.4275 - val_acc: 0.8287\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1419 - acc: 0.9606 - val_loss: 0.4279 - val_acc: 0.8293\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1236 - acc: 0.9672 - val_loss: 0.4292 - val_acc: 0.8323\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1091 - acc: 0.9703 - val_loss: 0.4344 - val_acc: 0.8325\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0953 - acc: 0.9755 - val_loss: 0.4400 - val_acc: 0.8312\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0841 - acc: 0.9775 - val_loss: 0.4518 - val_acc: 0.8293\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0760 - acc: 0.9808 - val_loss: 0.4614 - val_acc: 0.8283\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0660 - acc: 0.9826 - val_loss: 0.4722 - val_acc: 0.8302\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0590 - acc: 0.9859 - val_loss: 0.4845 - val_acc: 0.8285\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0529 - acc: 0.9865 - val_loss: 0.4939 - val_acc: 0.8274\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07493, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0866 - acc: 0.4001 - val_loss: 1.0749 - val_acc: 0.4033\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07493 to 0.98332, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0402 - acc: 0.4471 - val_loss: 0.9833 - val_acc: 0.5144\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98332 to 0.81529, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8829 - acc: 0.6241 - val_loss: 0.8153 - val_acc: 0.6546\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81529 to 0.67515, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6999 - acc: 0.7526 - val_loss: 0.6751 - val_acc: 0.7427\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67515 to 0.57708, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5460 - acc: 0.8242 - val_loss: 0.5771 - val_acc: 0.7844\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57708 to 0.51981, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4367 - acc: 0.8597 - val_loss: 0.5198 - val_acc: 0.8029\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51981 to 0.48392, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3585 - acc: 0.8875 - val_loss: 0.4839 - val_acc: 0.8106\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48392 to 0.46104, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2995 - acc: 0.9061 - val_loss: 0.4610 - val_acc: 0.8163\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46104 to 0.44594, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2522 - acc: 0.9241 - val_loss: 0.4459 - val_acc: 0.8223\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44594 to 0.43792, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2162 - acc: 0.9347 - val_loss: 0.4379 - val_acc: 0.8240\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43792 to 0.43372, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1859 - acc: 0.9449 - val_loss: 0.4337 - val_acc: 0.8266\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss improved from 0.43372 to 0.43099, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1611 - acc: 0.9518 - val_loss: 0.4310 - val_acc: 0.8289\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss improved from 0.43099 to 0.42946, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1399 - acc: 0.9621 - val_loss: 0.4295 - val_acc: 0.8289\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1220 - acc: 0.9672 - val_loss: 0.4356 - val_acc: 0.8306\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1071 - acc: 0.9713 - val_loss: 0.4444 - val_acc: 0.8300\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0926 - acc: 0.9756 - val_loss: 0.4464 - val_acc: 0.8336\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0830 - acc: 0.9792 - val_loss: 0.4542 - val_acc: 0.8336\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0734 - acc: 0.9823 - val_loss: 0.4673 - val_acc: 0.8325\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0650 - acc: 0.9850 - val_loss: 0.4732 - val_acc: 0.8344\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0579 - acc: 0.9868 - val_loss: 0.4862 - val_acc: 0.8332\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07425, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0856 - acc: 0.4037 - val_loss: 1.0743 - val_acc: 0.4103\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07425 to 0.98784, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0425 - acc: 0.4521 - val_loss: 0.9878 - val_acc: 0.5310\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98784 to 0.80706, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8844 - acc: 0.6084 - val_loss: 0.8071 - val_acc: 0.6835\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80706 to 0.64693, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6780 - acc: 0.7650 - val_loss: 0.6469 - val_acc: 0.7672\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64693 to 0.55435, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5152 - acc: 0.8356 - val_loss: 0.5543 - val_acc: 0.7936\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55435 to 0.50162, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4087 - acc: 0.8659 - val_loss: 0.5016 - val_acc: 0.8083\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50162 to 0.46917, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3347 - acc: 0.8942 - val_loss: 0.4692 - val_acc: 0.8134\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46917 to 0.45005, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2804 - acc: 0.9121 - val_loss: 0.4501 - val_acc: 0.8183\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45005 to 0.43652, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2315 - acc: 0.9293 - val_loss: 0.4365 - val_acc: 0.8227\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43652 to 0.43350, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1986 - acc: 0.9423 - val_loss: 0.4335 - val_acc: 0.8291\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43350 to 0.43094, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1711 - acc: 0.9509 - val_loss: 0.4309 - val_acc: 0.8300\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss improved from 0.43094 to 0.43068, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1479 - acc: 0.9588 - val_loss: 0.4307 - val_acc: 0.8342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1275 - acc: 0.9668 - val_loss: 0.4349 - val_acc: 0.8332\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1118 - acc: 0.9710 - val_loss: 0.4410 - val_acc: 0.8319\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0985 - acc: 0.9754 - val_loss: 0.4524 - val_acc: 0.8302\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0865 - acc: 0.9774 - val_loss: 0.4611 - val_acc: 0.8298\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0761 - acc: 0.9819 - val_loss: 0.4722 - val_acc: 0.8283\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0675 - acc: 0.9848 - val_loss: 0.4832 - val_acc: 0.8276\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0599 - acc: 0.9870 - val_loss: 0.4984 - val_acc: 0.8268\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0523 - acc: 0.9881 - val_loss: 0.5065 - val_acc: 0.8253\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07476, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0865 - acc: 0.3999 - val_loss: 1.0748 - val_acc: 0.4088\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07476 to 0.97068, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0373 - acc: 0.4576 - val_loss: 0.9707 - val_acc: 0.5382\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.97068 to 0.79814, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8698 - acc: 0.6224 - val_loss: 0.7981 - val_acc: 0.6535\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79814 to 0.64859, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6722 - acc: 0.7664 - val_loss: 0.6486 - val_acc: 0.7574\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64859 to 0.55794, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5131 - acc: 0.8342 - val_loss: 0.5579 - val_acc: 0.7883\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55794 to 0.50516, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4036 - acc: 0.8685 - val_loss: 0.5052 - val_acc: 0.8010\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50516 to 0.47306, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3288 - acc: 0.8931 - val_loss: 0.4731 - val_acc: 0.8087\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47306 to 0.45564, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2692 - acc: 0.9153 - val_loss: 0.4556 - val_acc: 0.8159\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45564 to 0.44165, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2265 - acc: 0.9342 - val_loss: 0.4417 - val_acc: 0.8185\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44165 to 0.43735, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1929 - acc: 0.9439 - val_loss: 0.4374 - val_acc: 0.8223\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43735 to 0.43442, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1627 - acc: 0.9553 - val_loss: 0.4344 - val_acc: 0.8204\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1418 - acc: 0.9620 - val_loss: 0.4352 - val_acc: 0.8257\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1220 - acc: 0.9667 - val_loss: 0.4426 - val_acc: 0.8259\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1076 - acc: 0.9725 - val_loss: 0.4474 - val_acc: 0.8268\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0926 - acc: 0.9778 - val_loss: 0.4565 - val_acc: 0.8268\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0839 - acc: 0.9783 - val_loss: 0.4657 - val_acc: 0.8272\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0724 - acc: 0.9820 - val_loss: 0.4735 - val_acc: 0.8259\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0646 - acc: 0.9850 - val_loss: 0.4873 - val_acc: 0.8249\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0563 - acc: 0.9871 - val_loss: 0.5020 - val_acc: 0.8229\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0511 - acc: 0.9883 - val_loss: 0.5111 - val_acc: 0.8217\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07611, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0849 - acc: 0.4055 - val_loss: 1.0761 - val_acc: 0.4006\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07611 to 0.98495, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0388 - acc: 0.4413 - val_loss: 0.9849 - val_acc: 0.5113\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98495 to 0.81667, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8786 - acc: 0.6377 - val_loss: 0.8167 - val_acc: 0.6670\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81667 to 0.67416, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6853 - acc: 0.7629 - val_loss: 0.6742 - val_acc: 0.7389\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67416 to 0.58255, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5343 - acc: 0.8271 - val_loss: 0.5825 - val_acc: 0.7740\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.58255 to 0.52642, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4286 - acc: 0.8593 - val_loss: 0.5264 - val_acc: 0.7930\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52642 to 0.49223, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3527 - acc: 0.8854 - val_loss: 0.4922 - val_acc: 0.8032\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.49223 to 0.47547, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2957 - acc: 0.9076 - val_loss: 0.4755 - val_acc: 0.8089\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.47547 to 0.45682, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2492 - acc: 0.9226 - val_loss: 0.4568 - val_acc: 0.8130\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.45682 to 0.45215, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2138 - acc: 0.9347 - val_loss: 0.4521 - val_acc: 0.8172\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.45215 to 0.44467, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1863 - acc: 0.9455 - val_loss: 0.4447 - val_acc: 0.8198\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9566 - val_loss: 0.4506 - val_acc: 0.8198\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1405 - acc: 0.9617 - val_loss: 0.4490 - val_acc: 0.8211\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1217 - acc: 0.9660 - val_loss: 0.4508 - val_acc: 0.8226\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1061 - acc: 0.9717 - val_loss: 0.4656 - val_acc: 0.8204\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0971 - acc: 0.9745 - val_loss: 0.4646 - val_acc: 0.8217\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0834 - acc: 0.9789 - val_loss: 0.4739 - val_acc: 0.8219\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0737 - acc: 0.9823 - val_loss: 0.4848 - val_acc: 0.8196\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0658 - acc: 0.9842 - val_loss: 0.4936 - val_acc: 0.8217\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0591 - acc: 0.9860 - val_loss: 0.5090 - val_acc: 0.8194\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07681, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0869 - acc: 0.3992 - val_loss: 1.0768 - val_acc: 0.4024\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07681 to 0.98249, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0427 - acc: 0.4497 - val_loss: 0.9825 - val_acc: 0.5422\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98249 to 0.79451, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8739 - acc: 0.6357 - val_loss: 0.7945 - val_acc: 0.6797\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79451 to 0.64505, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6698 - acc: 0.7748 - val_loss: 0.6450 - val_acc: 0.7544\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64505 to 0.55852, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5194 - acc: 0.8273 - val_loss: 0.5585 - val_acc: 0.7840\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55852 to 0.50757, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4129 - acc: 0.8659 - val_loss: 0.5076 - val_acc: 0.8004\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: val_loss improved from 0.50757 to 0.47509, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3385 - acc: 0.8933 - val_loss: 0.4751 - val_acc: 0.8070\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47509 to 0.45422, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2809 - acc: 0.9128 - val_loss: 0.4542 - val_acc: 0.8161\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45422 to 0.43939, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2359 - acc: 0.9304 - val_loss: 0.4394 - val_acc: 0.8221\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43939 to 0.43597, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2011 - acc: 0.9412 - val_loss: 0.4360 - val_acc: 0.8266\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43597 to 0.43302, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1731 - acc: 0.9507 - val_loss: 0.4330 - val_acc: 0.8293\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1497 - acc: 0.9578 - val_loss: 0.4386 - val_acc: 0.8280\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1331 - acc: 0.9641 - val_loss: 0.4372 - val_acc: 0.8317\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1140 - acc: 0.9718 - val_loss: 0.4453 - val_acc: 0.8295\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1011 - acc: 0.9730 - val_loss: 0.4544 - val_acc: 0.8289\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0900 - acc: 0.9761 - val_loss: 0.4697 - val_acc: 0.8249\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0800 - acc: 0.9799 - val_loss: 0.4721 - val_acc: 0.8270\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0710 - acc: 0.9817 - val_loss: 0.4870 - val_acc: 0.8242\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0616 - acc: 0.9853 - val_loss: 0.5035 - val_acc: 0.8234\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0571 - acc: 0.9858 - val_loss: 0.5229 - val_acc: 0.8215\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07773, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0866 - acc: 0.4003 - val_loss: 1.0777 - val_acc: 0.4022\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07773 to 0.99710, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0487 - acc: 0.4337 - val_loss: 0.9971 - val_acc: 0.5101\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99710 to 0.81047, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8894 - acc: 0.6338 - val_loss: 0.8105 - val_acc: 0.6970\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81047 to 0.65323, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6741 - acc: 0.7764 - val_loss: 0.6532 - val_acc: 0.7597\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65323 to 0.56132, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5117 - acc: 0.8323 - val_loss: 0.5613 - val_acc: 0.7891\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56132 to 0.50860, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4073 - acc: 0.8659 - val_loss: 0.5086 - val_acc: 0.8034\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50860 to 0.47603, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3335 - acc: 0.8922 - val_loss: 0.4760 - val_acc: 0.8106\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47603 to 0.45826, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2777 - acc: 0.9120 - val_loss: 0.4583 - val_acc: 0.8155\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45826 to 0.44510, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2337 - acc: 0.9302 - val_loss: 0.4451 - val_acc: 0.8187\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44510 to 0.44071, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1996 - acc: 0.9422 - val_loss: 0.4407 - val_acc: 0.8206\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44071 to 0.43533, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1703 - acc: 0.9515 - val_loss: 0.4353 - val_acc: 0.8268\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1487 - acc: 0.9566 - val_loss: 0.4378 - val_acc: 0.8270\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1284 - acc: 0.9651 - val_loss: 0.4399 - val_acc: 0.8268\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1122 - acc: 0.9690 - val_loss: 0.4457 - val_acc: 0.8255\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0981 - acc: 0.9749 - val_loss: 0.4532 - val_acc: 0.8249\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0853 - acc: 0.9782 - val_loss: 0.4615 - val_acc: 0.8251\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0775 - acc: 0.9795 - val_loss: 0.4691 - val_acc: 0.8251\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0661 - acc: 0.9851 - val_loss: 0.4810 - val_acc: 0.8229\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0601 - acc: 0.9871 - val_loss: 0.4913 - val_acc: 0.8215\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0545 - acc: 0.9871 - val_loss: 0.5038 - val_acc: 0.8219\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07672, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0868 - acc: 0.4004 - val_loss: 1.0767 - val_acc: 0.4035\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07672 to 0.98535, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0421 - acc: 0.4481 - val_loss: 0.9854 - val_acc: 0.5382\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98535 to 0.80486, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8764 - acc: 0.6283 - val_loss: 0.8049 - val_acc: 0.6610\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80486 to 0.65840, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6760 - acc: 0.7710 - val_loss: 0.6584 - val_acc: 0.7499\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65840 to 0.56442, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5194 - acc: 0.8373 - val_loss: 0.5644 - val_acc: 0.7829\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56442 to 0.50726, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4101 - acc: 0.8685 - val_loss: 0.5073 - val_acc: 0.8057\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50726 to 0.47213, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3338 - acc: 0.8976 - val_loss: 0.4721 - val_acc: 0.8149\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47213 to 0.45281, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2778 - acc: 0.9120 - val_loss: 0.4528 - val_acc: 0.8206\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45281 to 0.44027, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2332 - acc: 0.9303 - val_loss: 0.4403 - val_acc: 0.8263\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44027 to 0.43497, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1985 - acc: 0.9415 - val_loss: 0.4350 - val_acc: 0.8278\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43497 to 0.43176, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1714 - acc: 0.9525 - val_loss: 0.4318 - val_acc: 0.8306\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1461 - acc: 0.9609 - val_loss: 0.4325 - val_acc: 0.8323\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1282 - acc: 0.9652 - val_loss: 0.4355 - val_acc: 0.8325\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1104 - acc: 0.9701 - val_loss: 0.4385 - val_acc: 0.8342\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0968 - acc: 0.9754 - val_loss: 0.4492 - val_acc: 0.8334\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0856 - acc: 0.9793 - val_loss: 0.4576 - val_acc: 0.8334\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0754 - acc: 0.9821 - val_loss: 0.4678 - val_acc: 0.8325\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0668 - acc: 0.9842 - val_loss: 0.4763 - val_acc: 0.8310\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0589 - acc: 0.9857 - val_loss: 0.4879 - val_acc: 0.8319\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0543 - acc: 0.9876 - val_loss: 0.5014 - val_acc: 0.8304\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07374, saving model to /tmp/nn_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1.0853 - acc: 0.4052 - val_loss: 1.0737 - val_acc: 0.4126\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07374 to 0.99581, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0464 - acc: 0.4422 - val_loss: 0.9958 - val_acc: 0.5103\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99581 to 0.81888, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9004 - acc: 0.6024 - val_loss: 0.8189 - val_acc: 0.6927\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81888 to 0.66223, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6955 - acc: 0.7593 - val_loss: 0.6622 - val_acc: 0.7680\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66223 to 0.56755, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5343 - acc: 0.8252 - val_loss: 0.5675 - val_acc: 0.7883\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56755 to 0.51337, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4251 - acc: 0.8627 - val_loss: 0.5134 - val_acc: 0.7978\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51337 to 0.48147, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3447 - acc: 0.8891 - val_loss: 0.4815 - val_acc: 0.8102\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48147 to 0.46389, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2874 - acc: 0.9109 - val_loss: 0.4639 - val_acc: 0.8174\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46389 to 0.44722, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2414 - acc: 0.9269 - val_loss: 0.4472 - val_acc: 0.8221\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44722 to 0.44622, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2059 - acc: 0.9382 - val_loss: 0.4462 - val_acc: 0.8227\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44622 to 0.43746, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1768 - acc: 0.9495 - val_loss: 0.4375 - val_acc: 0.8249\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1524 - acc: 0.9588 - val_loss: 0.4454 - val_acc: 0.8242\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1346 - acc: 0.9644 - val_loss: 0.4415 - val_acc: 0.8257\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1172 - acc: 0.9686 - val_loss: 0.4497 - val_acc: 0.8236\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1032 - acc: 0.9739 - val_loss: 0.4574 - val_acc: 0.8197\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0915 - acc: 0.9772 - val_loss: 0.4567 - val_acc: 0.8242\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0795 - acc: 0.9815 - val_loss: 0.4684 - val_acc: 0.8225\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0708 - acc: 0.9831 - val_loss: 0.4717 - val_acc: 0.8257\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0621 - acc: 0.9855 - val_loss: 0.4891 - val_acc: 0.8221\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0556 - acc: 0.9876 - val_loss: 0.4956 - val_acc: 0.8229\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07622, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0855 - acc: 0.4028 - val_loss: 1.0762 - val_acc: 0.4045\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07622 to 0.99094, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0452 - acc: 0.4415 - val_loss: 0.9909 - val_acc: 0.5049\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99094 to 0.80776, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8876 - acc: 0.6221 - val_loss: 0.8078 - val_acc: 0.6643\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80776 to 0.65503, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6837 - acc: 0.7661 - val_loss: 0.6550 - val_acc: 0.7449\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65503 to 0.56508, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5288 - acc: 0.8269 - val_loss: 0.5651 - val_acc: 0.7766\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56508 to 0.50921, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4208 - acc: 0.8637 - val_loss: 0.5092 - val_acc: 0.8023\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50921 to 0.47327, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3443 - acc: 0.8908 - val_loss: 0.4733 - val_acc: 0.8102\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47327 to 0.45056, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2865 - acc: 0.9131 - val_loss: 0.4506 - val_acc: 0.8179\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45056 to 0.43474, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2409 - acc: 0.9251 - val_loss: 0.4347 - val_acc: 0.8240\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43474 to 0.42774, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2035 - acc: 0.9377 - val_loss: 0.4277 - val_acc: 0.8247\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42774 to 0.42285, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1741 - acc: 0.9481 - val_loss: 0.4229 - val_acc: 0.8279\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1511 - acc: 0.9564 - val_loss: 0.4248 - val_acc: 0.8272\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1309 - acc: 0.9632 - val_loss: 0.4253 - val_acc: 0.8309\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1148 - acc: 0.9677 - val_loss: 0.4295 - val_acc: 0.8338\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0980 - acc: 0.9745 - val_loss: 0.4390 - val_acc: 0.8317\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0893 - acc: 0.9765 - val_loss: 0.4419 - val_acc: 0.8336\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0789 - acc: 0.9807 - val_loss: 0.4477 - val_acc: 0.8317\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0694 - acc: 0.9839 - val_loss: 0.4594 - val_acc: 0.8309\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0616 - acc: 0.9839 - val_loss: 0.4704 - val_acc: 0.8321\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0546 - acc: 0.9866 - val_loss: 0.4853 - val_acc: 0.8300\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07691, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0866 - acc: 0.4011 - val_loss: 1.0769 - val_acc: 0.4046\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07691 to 0.98180, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0437 - acc: 0.4487 - val_loss: 0.9818 - val_acc: 0.5127\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98180 to 0.78757, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8730 - acc: 0.6374 - val_loss: 0.7876 - val_acc: 0.7014\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.78757 to 0.63538, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6655 - acc: 0.7741 - val_loss: 0.6354 - val_acc: 0.7617\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.63538 to 0.54509, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5135 - acc: 0.8326 - val_loss: 0.5451 - val_acc: 0.7889\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54509 to 0.49119, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4076 - acc: 0.8684 - val_loss: 0.4912 - val_acc: 0.8085\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49119 to 0.46211, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3330 - acc: 0.8941 - val_loss: 0.4621 - val_acc: 0.8151\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46211 to 0.44295, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2770 - acc: 0.9157 - val_loss: 0.4430 - val_acc: 0.8208\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44295 to 0.42997, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2309 - acc: 0.9308 - val_loss: 0.4300 - val_acc: 0.8272\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42997 to 0.42470, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1990 - acc: 0.9419 - val_loss: 0.4247 - val_acc: 0.8295\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42470 to 0.42296, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1708 - acc: 0.9518 - val_loss: 0.4230 - val_acc: 0.8334\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1470 - acc: 0.9608 - val_loss: 0.4262 - val_acc: 0.8336\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1300 - acc: 0.9667 - val_loss: 0.4271 - val_acc: 0.8327\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1122 - acc: 0.9704 - val_loss: 0.4338 - val_acc: 0.8323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0995 - acc: 0.9755 - val_loss: 0.4390 - val_acc: 0.8300\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0880 - acc: 0.9794 - val_loss: 0.4486 - val_acc: 0.8310\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0786 - acc: 0.9814 - val_loss: 0.4582 - val_acc: 0.8308\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0684 - acc: 0.9849 - val_loss: 0.4652 - val_acc: 0.8319\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0611 - acc: 0.9867 - val_loss: 0.4800 - val_acc: 0.8278\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0540 - acc: 0.9870 - val_loss: 0.4971 - val_acc: 0.8278\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07565, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0850 - acc: 0.4062 - val_loss: 1.0757 - val_acc: 0.4046\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07565 to 0.99053, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0436 - acc: 0.4433 - val_loss: 0.9905 - val_acc: 0.5395\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99053 to 0.81389, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8870 - acc: 0.6254 - val_loss: 0.8139 - val_acc: 0.6748\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81389 to 0.66050, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6824 - acc: 0.7613 - val_loss: 0.6605 - val_acc: 0.7510\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66050 to 0.56595, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5197 - acc: 0.8304 - val_loss: 0.5660 - val_acc: 0.7791\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56595 to 0.51838, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4119 - acc: 0.8669 - val_loss: 0.5184 - val_acc: 0.7923\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51838 to 0.48171, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3357 - acc: 0.8916 - val_loss: 0.4817 - val_acc: 0.8068\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48171 to 0.46292, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2778 - acc: 0.9138 - val_loss: 0.4629 - val_acc: 0.8117\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46292 to 0.44615, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2332 - acc: 0.9310 - val_loss: 0.4462 - val_acc: 0.8187\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44615 to 0.43885, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2002 - acc: 0.9407 - val_loss: 0.4389 - val_acc: 0.8204\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43885 to 0.43612, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1701 - acc: 0.9507 - val_loss: 0.4361 - val_acc: 0.8225\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1481 - acc: 0.9599 - val_loss: 0.4386 - val_acc: 0.8246\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1278 - acc: 0.9671 - val_loss: 0.4373 - val_acc: 0.8276\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1118 - acc: 0.9711 - val_loss: 0.4440 - val_acc: 0.8266\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0960 - acc: 0.9766 - val_loss: 0.4547 - val_acc: 0.8236\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0840 - acc: 0.9808 - val_loss: 0.4661 - val_acc: 0.8225\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0749 - acc: 0.9824 - val_loss: 0.4753 - val_acc: 0.8236\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0668 - acc: 0.9859 - val_loss: 0.4932 - val_acc: 0.8217\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0592 - acc: 0.9856 - val_loss: 0.5101 - val_acc: 0.8161\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0531 - acc: 0.9873 - val_loss: 0.5152 - val_acc: 0.8200\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07653, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0861 - acc: 0.4013 - val_loss: 1.0765 - val_acc: 0.4067\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07653 to 0.98696, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0439 - acc: 0.4476 - val_loss: 0.9870 - val_acc: 0.5222\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98696 to 0.79859, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8770 - acc: 0.6333 - val_loss: 0.7986 - val_acc: 0.6806\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79859 to 0.64346, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6701 - acc: 0.7725 - val_loss: 0.6435 - val_acc: 0.7595\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64346 to 0.54967, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5106 - acc: 0.8337 - val_loss: 0.5497 - val_acc: 0.7951\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54967 to 0.49771, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4043 - acc: 0.8688 - val_loss: 0.4977 - val_acc: 0.8083\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49771 to 0.46508, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3291 - acc: 0.8962 - val_loss: 0.4651 - val_acc: 0.8149\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46508 to 0.44465, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2768 - acc: 0.9120 - val_loss: 0.4446 - val_acc: 0.8212\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44465 to 0.43075, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2311 - acc: 0.9284 - val_loss: 0.4308 - val_acc: 0.8242\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43075 to 0.42493, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1991 - acc: 0.9405 - val_loss: 0.4249 - val_acc: 0.8289\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42493 to 0.42275, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1704 - acc: 0.9512 - val_loss: 0.4228 - val_acc: 0.8276\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1465 - acc: 0.9581 - val_loss: 0.4250 - val_acc: 0.8293\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1276 - acc: 0.9657 - val_loss: 0.4263 - val_acc: 0.8287\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1119 - acc: 0.9700 - val_loss: 0.4315 - val_acc: 0.8306\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0979 - acc: 0.9743 - val_loss: 0.4406 - val_acc: 0.8306\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0873 - acc: 0.9764 - val_loss: 0.4450 - val_acc: 0.8310\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0767 - acc: 0.9807 - val_loss: 0.4555 - val_acc: 0.8300\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0678 - acc: 0.9843 - val_loss: 0.4648 - val_acc: 0.8306\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0597 - acc: 0.9858 - val_loss: 0.4758 - val_acc: 0.8293\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0535 - acc: 0.9869 - val_loss: 0.4876 - val_acc: 0.8272\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07632, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0867 - acc: 0.3990 - val_loss: 1.0763 - val_acc: 0.4020\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07632 to 0.99437, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0453 - acc: 0.4422 - val_loss: 0.9944 - val_acc: 0.5305\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99437 to 0.82590, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8942 - acc: 0.6084 - val_loss: 0.8259 - val_acc: 0.6627\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.82590 to 0.67364, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6955 - acc: 0.7599 - val_loss: 0.6736 - val_acc: 0.7493\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67364 to 0.57825, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5370 - acc: 0.8277 - val_loss: 0.5782 - val_acc: 0.7793\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57825 to 0.52292, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4313 - acc: 0.8583 - val_loss: 0.5229 - val_acc: 0.7974\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52292 to 0.49030, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3524 - acc: 0.8901 - val_loss: 0.4903 - val_acc: 0.8051\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.49030 to 0.47007, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2949 - acc: 0.9079 - val_loss: 0.4701 - val_acc: 0.8117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.47007 to 0.44915, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2509 - acc: 0.9226 - val_loss: 0.4491 - val_acc: 0.8215\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44915 to 0.44592, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2152 - acc: 0.9373 - val_loss: 0.4459 - val_acc: 0.8234\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44592 to 0.44066, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1853 - acc: 0.9450 - val_loss: 0.4407 - val_acc: 0.8270\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1601 - acc: 0.9556 - val_loss: 0.4418 - val_acc: 0.8289\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss improved from 0.44066 to 0.43354, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1388 - acc: 0.9617 - val_loss: 0.4335 - val_acc: 0.8315\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1206 - acc: 0.9675 - val_loss: 0.4426 - val_acc: 0.8291\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1074 - acc: 0.9715 - val_loss: 0.4472 - val_acc: 0.8300\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0941 - acc: 0.9752 - val_loss: 0.4532 - val_acc: 0.8310\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0842 - acc: 0.9780 - val_loss: 0.4605 - val_acc: 0.8289\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0743 - acc: 0.9819 - val_loss: 0.4713 - val_acc: 0.8285\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0652 - acc: 0.9844 - val_loss: 0.4852 - val_acc: 0.8285\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0589 - acc: 0.9850 - val_loss: 0.4909 - val_acc: 0.8268\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07477, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0859 - acc: 0.4006 - val_loss: 1.0748 - val_acc: 0.4074\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07477 to 0.98655, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0422 - acc: 0.4490 - val_loss: 0.9866 - val_acc: 0.5360\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98655 to 0.81356, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8815 - acc: 0.6303 - val_loss: 0.8136 - val_acc: 0.6470\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81356 to 0.66549, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6817 - acc: 0.7655 - val_loss: 0.6655 - val_acc: 0.7404\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66549 to 0.57362, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5232 - acc: 0.8327 - val_loss: 0.5736 - val_acc: 0.7751\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57362 to 0.52048, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4171 - acc: 0.8630 - val_loss: 0.5205 - val_acc: 0.7953\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52048 to 0.48742, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3395 - acc: 0.8919 - val_loss: 0.4874 - val_acc: 0.8051\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48742 to 0.46890, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2817 - acc: 0.9127 - val_loss: 0.4689 - val_acc: 0.8121\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46890 to 0.45290, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2389 - acc: 0.9272 - val_loss: 0.4529 - val_acc: 0.8168\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.45290 to 0.44496, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2007 - acc: 0.9406 - val_loss: 0.4450 - val_acc: 0.8200\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44496 to 0.44057, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1742 - acc: 0.9511 - val_loss: 0.4406 - val_acc: 0.8243\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1496 - acc: 0.9582 - val_loss: 0.4477 - val_acc: 0.8230\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1302 - acc: 0.9639 - val_loss: 0.4438 - val_acc: 0.8268\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1125 - acc: 0.9703 - val_loss: 0.4508 - val_acc: 0.8232\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0994 - acc: 0.9746 - val_loss: 0.4586 - val_acc: 0.8253\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0880 - acc: 0.9767 - val_loss: 0.4673 - val_acc: 0.8272\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0801 - acc: 0.9796 - val_loss: 0.4769 - val_acc: 0.8240\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0696 - acc: 0.9829 - val_loss: 0.4811 - val_acc: 0.8257\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0628 - acc: 0.9848 - val_loss: 0.4889 - val_acc: 0.8268\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0551 - acc: 0.9870 - val_loss: 0.4998 - val_acc: 0.8245\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.3,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(4)\n",
    "nn_train3,nn_test3,nn_train4,nn_test4 = get_nn_feats(5)\n",
    "nn_train5,nn_test5,nn_train6,nn_test6 = get_nn_feats(6)\n",
    "\n",
    "\n",
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                          cnn_train1, cnn_train2,cnn_train3, cnn_train4,cnn_train5, cnn_train6,\n",
    "                          nn_train1,nn_train2,nn_train3,nn_train4,nn_train5,nn_train6\n",
    "                         ])\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                         cnn_test1, cnn_test2,cnn_test3, cnn_test4,cnn_test5, cnn_test6,\n",
    "                         nn_test1,nn_test2,nn_test3,nn_test4,nn_test5,nn_test6\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 798) (8392, 798)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id', 'text','tag_txt','pos_txt','dep_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X, all_svd_train, all_nlp_train])\n",
    "test_X = np.hstack([test_X, all_svd_test, all_nlp_test])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n",
    "#f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n",
    "#f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = np.round(best_single_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05485\tvalid-mlogloss:1.05599\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.221488\tvalid-mlogloss:0.280795\n",
      "[400]\ttrain-mlogloss:0.177581\tvalid-mlogloss:0.270376\n",
      "Stopping. Best iteration:\n",
      "[533]\ttrain-mlogloss:0.156503\tvalid-mlogloss:0.268771\n",
      "\n",
      "train log loss 0.149523098456 valid log loss 0.269121766835\n",
      "rev 3.71579011152\n",
      "[0]\ttrain-mlogloss:1.05491\tvalid-mlogloss:1.05563\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.222293\tvalid-mlogloss:0.272717\n",
      "[400]\ttrain-mlogloss:0.178213\tvalid-mlogloss:0.264969\n",
      "Stopping. Best iteration:\n",
      "[537]\ttrain-mlogloss:0.157412\tvalid-mlogloss:0.264233\n",
      "\n",
      "train log loss 0.150395820849 valid log loss 0.264380298594\n",
      "rev 3.78243010284\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05544\tvalid-mlogloss:1.055\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.229453\tvalid-mlogloss:0.24723\n",
      "[400]\ttrain-mlogloss:0.186174\tvalid-mlogloss:0.236292\n",
      "[600]\ttrain-mlogloss:0.155849\tvalid-mlogloss:0.232105\n",
      "[800]\ttrain-mlogloss:0.131737\tvalid-mlogloss:0.230729\n",
      "[1000]\ttrain-mlogloss:0.112012\tvalid-mlogloss:0.230189\n",
      "Stopping. Best iteration:\n",
      "[1009]\ttrain-mlogloss:0.111199\tvalid-mlogloss:0.230052\n",
      "\n",
      "train log loss 0.106929169381 valid log loss 0.230582596399\n",
      "rev 4.33684074869\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05528\tvalid-mlogloss:1.05558\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.224533\tvalid-mlogloss:0.263495\n",
      "[400]\ttrain-mlogloss:0.180169\tvalid-mlogloss:0.254555\n",
      "[600]\ttrain-mlogloss:0.15124\tvalid-mlogloss:0.25307\n",
      "Stopping. Best iteration:\n",
      "[570]\ttrain-mlogloss:0.155068\tvalid-mlogloss:0.252998\n",
      "\n",
      "train log loss 0.148624468551 valid log loss 0.253178153296\n",
      "rev 3.94978787459\n",
      "[0]\ttrain-mlogloss:1.05499\tvalid-mlogloss:1.05528\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.22383\tvalid-mlogloss:0.269813\n",
      "[400]\ttrain-mlogloss:0.180045\tvalid-mlogloss:0.261519\n",
      "[600]\ttrain-mlogloss:0.150609\tvalid-mlogloss:0.259905\n",
      "Stopping. Best iteration:\n",
      "[585]\ttrain-mlogloss:0.15261\tvalid-mlogloss:0.259879\n",
      "\n",
      "train log loss 0.146144836143 valid log loss 0.260076230619\n",
      "rev 3.8450265048\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0124  0.0020  0.9855\n",
      "1  id24541  0.9989  0.0005  0.0007\n",
      "2  id00134  0.0021  0.9972  0.0007\n",
      "3  id27757  0.8844  0.1120  0.0035\n",
      "4  id04081  0.7542  0.1365  0.1093\n",
      "--------------\n",
      "19.6298753424\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0126  0.0020  0.9854\n",
      "1  id24541  0.9989  0.0005  0.0006\n",
      "2  id00134  0.0021  0.9972  0.0007\n",
      "3  id27757  0.8844  0.1121  0.0035\n",
      "4  id04081  0.7525  0.1370  0.1104\n",
      "---------------\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0154  0.0015  0.9831\n",
      "1  id24541  0.9992  0.0003  0.0005\n",
      "2  id00134  0.0012  0.9985  0.0003\n",
      "3  id27757  0.8804  0.1178  0.0018\n",
      "4  id04081  0.6953  0.1496  0.1551\n",
      "---------------\n",
      "local average valid loss 0.255467809149\n",
      "train log loss 0.153396360837\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05503\tvalid-mlogloss:1.05568\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.227454\tvalid-mlogloss:0.269699\n",
      "[400]\ttrain-mlogloss:0.186528\tvalid-mlogloss:0.257652\n",
      "[600]\ttrain-mlogloss:0.157903\tvalid-mlogloss:0.255313\n",
      "Stopping. Best iteration:\n",
      "[699]\ttrain-mlogloss:0.146044\tvalid-mlogloss:0.25433\n",
      "\n",
      "train log loss 0.140450477448 valid log loss 0.254583709453\n",
      "rev 3.92798110354\n",
      "[0]\ttrain-mlogloss:1.05501\tvalid-mlogloss:1.05633\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.22469\tvalid-mlogloss:0.294069\n",
      "[400]\ttrain-mlogloss:0.182786\tvalid-mlogloss:0.284528\n",
      "[600]\ttrain-mlogloss:0.154454\tvalid-mlogloss:0.28286\n",
      "Stopping. Best iteration:\n",
      "[559]\ttrain-mlogloss:0.159669\tvalid-mlogloss:0.282648\n",
      "\n",
      "train log loss 0.153284262756 valid log loss 0.282816827867\n",
      "rev 3.53585749314\n",
      "[0]\ttrain-mlogloss:1.05505\tvalid-mlogloss:1.0557\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.227385\tvalid-mlogloss:0.268914\n",
      "[400]\ttrain-mlogloss:0.185361\tvalid-mlogloss:0.261353\n",
      "Stopping. Best iteration:\n",
      "[531]\ttrain-mlogloss:0.165496\tvalid-mlogloss:0.259807\n",
      "\n",
      "train log loss 0.158933986506 valid log loss 0.259953627371\n",
      "rev 3.84683995416\n",
      "[0]\ttrain-mlogloss:1.05499\tvalid-mlogloss:1.05549\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.226929\tvalid-mlogloss:0.27443\n",
      "[400]\ttrain-mlogloss:0.185674\tvalid-mlogloss:0.264506\n",
      "Stopping. Best iteration:\n",
      "[547]\ttrain-mlogloss:0.164414\tvalid-mlogloss:0.26246\n",
      "\n",
      "train log loss 0.158081993331 valid log loss 0.262806255489\n",
      "rev 3.80508446475\n",
      "[0]\ttrain-mlogloss:1.05537\tvalid-mlogloss:1.05453\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.231084\tvalid-mlogloss:0.238417\n",
      "[400]\ttrain-mlogloss:0.189388\tvalid-mlogloss:0.226202\n",
      "Stopping. Best iteration:\n",
      "[506]\ttrain-mlogloss:0.173215\tvalid-mlogloss:0.223974\n",
      "\n",
      "train log loss 0.16637693699 valid log loss 0.22403171827\n",
      "rev 4.46365366352\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05531\tvalid-mlogloss:1.05545\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.229333\tvalid-mlogloss:0.251673\n",
      "[400]\ttrain-mlogloss:0.18809\tvalid-mlogloss:0.237702\n",
      "[600]\ttrain-mlogloss:0.159651\tvalid-mlogloss:0.232843\n",
      "[800]\ttrain-mlogloss:0.136842\tvalid-mlogloss:0.231082\n",
      "Stopping. Best iteration:\n",
      "[923]\ttrain-mlogloss:0.124845\tvalid-mlogloss:0.229552\n",
      "\n",
      "train log loss 0.120311951633 valid log loss 0.229649963481\n",
      "rev 4.35445312005\n",
      "[0]\ttrain-mlogloss:1.05517\tvalid-mlogloss:1.05542\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.228271\tvalid-mlogloss:0.263131\n",
      "[400]\ttrain-mlogloss:0.186017\tvalid-mlogloss:0.255415\n",
      "[600]\ttrain-mlogloss:0.157739\tvalid-mlogloss:0.252459\n",
      "Stopping. Best iteration:\n",
      "[641]\ttrain-mlogloss:0.152714\tvalid-mlogloss:0.252312\n",
      "\n",
      "train log loss 0.147155015191 valid log loss 0.25248176579\n",
      "rev 3.96068205905\n",
      "[0]\ttrain-mlogloss:1.05515\tvalid-mlogloss:1.05592\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.228401\tvalid-mlogloss:0.267242\n",
      "[400]\ttrain-mlogloss:0.187036\tvalid-mlogloss:0.254955\n",
      "[600]\ttrain-mlogloss:0.158724\tvalid-mlogloss:0.252224\n",
      "Stopping. Best iteration:\n",
      "[600]\ttrain-mlogloss:0.158724\tvalid-mlogloss:0.252224\n",
      "\n",
      "train log loss 0.152577276842 valid log loss 0.252467039181\n",
      "rev 3.96091308887\n",
      "[0]\ttrain-mlogloss:1.05536\tvalid-mlogloss:1.05493\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.228845\tvalid-mlogloss:0.254686\n",
      "[400]\ttrain-mlogloss:0.187223\tvalid-mlogloss:0.245736\n",
      "Stopping. Best iteration:\n",
      "[463]\ttrain-mlogloss:0.177639\tvalid-mlogloss:0.244885\n",
      "\n",
      "train log loss 0.170340156398 valid log loss 0.24516539719\n",
      "rev 4.07887904028\n",
      "[0]\ttrain-mlogloss:1.05508\tvalid-mlogloss:1.05572\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.226219\tvalid-mlogloss:0.280125\n",
      "[400]\ttrain-mlogloss:0.184706\tvalid-mlogloss:0.269316\n",
      "[600]\ttrain-mlogloss:0.156417\tvalid-mlogloss:0.266466\n",
      "[800]\ttrain-mlogloss:0.134142\tvalid-mlogloss:0.264961\n",
      "Stopping. Best iteration:\n",
      "[803]\ttrain-mlogloss:0.133831\tvalid-mlogloss:0.264877\n",
      "\n",
      "train log loss 0.128669186606 valid log loss 0.26511748206\n",
      "rev 3.77191270915\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0108  0.0018  0.9874\n",
      "1  id24541  0.9987  0.0005  0.0007\n",
      "2  id00134  0.0021  0.9973  0.0007\n",
      "3  id27757  0.8666  0.1299  0.0036\n",
      "4  id04081  0.7413  0.1555  0.1032\n",
      "--------------\n",
      "39.7062566965\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0109  0.0018  0.9873\n",
      "1  id24541  0.9987  0.0005  0.0007\n",
      "2  id00134  0.0021  0.9973  0.0007\n",
      "3  id27757  0.8653  0.1311  0.0036\n",
      "4  id04081  0.7412  0.1554  0.1034\n",
      "---------------\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0118  0.0016  0.9866\n",
      "1  id24541  0.9984  0.0007  0.0009\n",
      "2  id00134  0.0030  0.9962  0.0009\n",
      "3  id27757  0.8575  0.1383  0.0042\n",
      "4  id04081  0.7342  0.1498  0.1159\n",
      "---------------\n",
      "local average valid loss 0.252907378615\n",
      "train log loss 0.154882779909\n"
     ]
    }
   ],
   "source": [
    "cv_test(10, True)\n",
    "# 276xx not good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
