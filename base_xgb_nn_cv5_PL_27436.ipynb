{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Processing test...\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(lambda x: clean_text(x))\n",
    "test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "def extract_features(df):\n",
    "    df['len'] = df['text'].apply(lambda x: len(x))\n",
    "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "\n",
    "    df.drop(['text_cleaned'], axis=1, inplace=True)\n",
    "    \n",
    "print('Processing train...')\n",
    "extract_features(train_df)\n",
    "print('Processing test...')\n",
    "extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tag_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = [a[1] for a in nltk.pos_tag(words)]\n",
    "        res.append(' '.join(tag_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "def ne_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = nltk.pos_tag(words)\n",
    "        ne_tree = nltk.ne_chunk(tag_res)\n",
    "        list_res = nltk.tree2conlltags(ne_tree)\n",
    "        ne_res = [a[2] for a in list_res]\n",
    "        res.append(' '.join(ne_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "train_df['ne_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['ne_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "\n",
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False)\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False)\n",
    "c_vec4.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec6.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 12) (8392, 12)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "\n",
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04715, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0821 - acc: 0.4062 - val_loss: 1.0471 - val_acc: 0.4422\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04715 to 0.72417, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9004 - acc: 0.5970 - val_loss: 0.7242 - val_acc: 0.7313\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72417 to 0.49925, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5651 - acc: 0.7989 - val_loss: 0.4993 - val_acc: 0.8092\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49925 to 0.42555, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3816 - acc: 0.8716 - val_loss: 0.4255 - val_acc: 0.8373\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42555 to 0.40495, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2828 - acc: 0.9132 - val_loss: 0.4049 - val_acc: 0.8526\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2226 - acc: 0.9350 - val_loss: 0.4074 - val_acc: 0.8532\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1754 - acc: 0.9506 - val_loss: 0.4145 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1435 - acc: 0.9630 - val_loss: 0.4361 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1179 - acc: 0.9707 - val_loss: 0.4645 - val_acc: 0.8475\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0989 - acc: 0.9753 - val_loss: 0.4812 - val_acc: 0.8437\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04948, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0813 - acc: 0.4127 - val_loss: 1.0495 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04948 to 0.76628, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9205 - acc: 0.5919 - val_loss: 0.7663 - val_acc: 0.7173\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76628 to 0.51590, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5766 - acc: 0.7970 - val_loss: 0.5159 - val_acc: 0.8060\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51590 to 0.43448, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3675 - acc: 0.8767 - val_loss: 0.4345 - val_acc: 0.8258\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43448 to 0.41325, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2644 - acc: 0.9118 - val_loss: 0.4133 - val_acc: 0.8379\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2010 - acc: 0.9348 - val_loss: 0.4139 - val_acc: 0.8437\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1597 - acc: 0.9491 - val_loss: 0.4320 - val_acc: 0.8360\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1274 - acc: 0.9606 - val_loss: 0.4495 - val_acc: 0.8417\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1040 - acc: 0.9689 - val_loss: 0.4812 - val_acc: 0.8334\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0894 - acc: 0.9741 - val_loss: 0.5082 - val_acc: 0.8302\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03622, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0782 - acc: 0.4224 - val_loss: 1.0362 - val_acc: 0.4850\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03622 to 0.80292, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9213 - acc: 0.5846 - val_loss: 0.8029 - val_acc: 0.6899\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80292 to 0.54383, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6151 - acc: 0.7784 - val_loss: 0.5438 - val_acc: 0.7945\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.54383 to 0.45128, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3910 - acc: 0.8673 - val_loss: 0.4513 - val_acc: 0.8200\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.45128 to 0.42242, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2794 - acc: 0.9061 - val_loss: 0.4224 - val_acc: 0.8379\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.42242 to 0.41819, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2118 - acc: 0.9320 - val_loss: 0.4182 - val_acc: 0.8392\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1642 - acc: 0.9484 - val_loss: 0.4318 - val_acc: 0.8360\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1338 - acc: 0.9586 - val_loss: 0.4486 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1085 - acc: 0.9682 - val_loss: 0.4751 - val_acc: 0.8354\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0879 - acc: 0.9749 - val_loss: 0.5103 - val_acc: 0.8354\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04172, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0816 - acc: 0.4120 - val_loss: 1.0417 - val_acc: 0.4767\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04172 to 0.72961, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8978 - acc: 0.5999 - val_loss: 0.7296 - val_acc: 0.7486\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72961 to 0.49134, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5604 - acc: 0.7999 - val_loss: 0.4913 - val_acc: 0.8130\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49134 to 0.42177, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3635 - acc: 0.8746 - val_loss: 0.4218 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42177 to 0.39867, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2664 - acc: 0.9113 - val_loss: 0.3987 - val_acc: 0.8443\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39867 to 0.39185, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2052 - acc: 0.9332 - val_loss: 0.3918 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1627 - acc: 0.9487 - val_loss: 0.3983 - val_acc: 0.8519\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1309 - acc: 0.9604 - val_loss: 0.4084 - val_acc: 0.8519\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1059 - acc: 0.9699 - val_loss: 0.4329 - val_acc: 0.8468\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0890 - acc: 0.9736 - val_loss: 0.4465 - val_acc: 0.8468\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03770, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0799 - acc: 0.4170 - val_loss: 1.0377 - val_acc: 0.4869\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03770 to 0.72796, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8978 - acc: 0.6028 - val_loss: 0.7280 - val_acc: 0.7326\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72796 to 0.49181, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5679 - acc: 0.8027 - val_loss: 0.4918 - val_acc: 0.8207\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49181 to 0.41519, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3700 - acc: 0.8726 - val_loss: 0.4152 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41519 to 0.39518, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2709 - acc: 0.9083 - val_loss: 0.3952 - val_acc: 0.8551\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39518 to 0.39202, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2082 - acc: 0.9303 - val_loss: 0.3920 - val_acc: 0.8590\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1599 - acc: 0.9481 - val_loss: 0.4051 - val_acc: 0.8507\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1304 - acc: 0.9610 - val_loss: 0.4306 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1055 - acc: 0.9683 - val_loss: 0.4539 - val_acc: 0.8385\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0901 - acc: 0.9739 - val_loss: 0.4831 - val_acc: 0.8392\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04420, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0802 - acc: 0.4172 - val_loss: 1.0442 - val_acc: 0.4671\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04420 to 0.81636, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9375 - acc: 0.5602 - val_loss: 0.8164 - val_acc: 0.6586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.81636 to 0.55755, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6430 - acc: 0.7610 - val_loss: 0.5575 - val_acc: 0.7811\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.55755 to 0.44500, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4092 - acc: 0.8610 - val_loss: 0.4450 - val_acc: 0.8328\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44500 to 0.41222, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2841 - acc: 0.9052 - val_loss: 0.4122 - val_acc: 0.8392\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41222 to 0.40705, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2119 - acc: 0.9304 - val_loss: 0.4071 - val_acc: 0.8424\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9534 - val_loss: 0.4209 - val_acc: 0.8398\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1260 - acc: 0.9630 - val_loss: 0.4384 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1039 - acc: 0.9675 - val_loss: 0.4659 - val_acc: 0.8366\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0861 - acc: 0.9744 - val_loss: 0.4862 - val_acc: 0.8334\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04171, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0806 - acc: 0.4141 - val_loss: 1.0417 - val_acc: 0.4844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04171 to 0.79584, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9326 - acc: 0.5743 - val_loss: 0.7958 - val_acc: 0.6752\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.79584 to 0.52461, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6067 - acc: 0.7823 - val_loss: 0.5246 - val_acc: 0.7951\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52461 to 0.42479, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3789 - acc: 0.8702 - val_loss: 0.4248 - val_acc: 0.8385\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42479 to 0.39388, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2681 - acc: 0.9105 - val_loss: 0.3939 - val_acc: 0.8507\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.1998 - acc: 0.9357 - val_loss: 0.3958 - val_acc: 0.8513\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1559 - acc: 0.9503 - val_loss: 0.4106 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1246 - acc: 0.9595 - val_loss: 0.4242 - val_acc: 0.8411\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1018 - acc: 0.9706 - val_loss: 0.4482 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0842 - acc: 0.9756 - val_loss: 0.4806 - val_acc: 0.8373\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03757, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0802 - acc: 0.4166 - val_loss: 1.0376 - val_acc: 0.4818\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03757 to 0.78445, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9208 - acc: 0.5817 - val_loss: 0.7845 - val_acc: 0.7020\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.78445 to 0.52387, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6038 - acc: 0.7822 - val_loss: 0.5239 - val_acc: 0.8073\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52387 to 0.42802, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3847 - acc: 0.8701 - val_loss: 0.4280 - val_acc: 0.8430\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42802 to 0.39402, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2677 - acc: 0.9113 - val_loss: 0.3940 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2019 - acc: 0.9343 - val_loss: 0.3968 - val_acc: 0.8539\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1557 - acc: 0.9510 - val_loss: 0.4099 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1249 - acc: 0.9614 - val_loss: 0.4266 - val_acc: 0.8519\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1001 - acc: 0.9716 - val_loss: 0.4561 - val_acc: 0.8500\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0838 - acc: 0.9753 - val_loss: 0.4782 - val_acc: 0.8468\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05459, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0827 - acc: 0.4108 - val_loss: 1.0546 - val_acc: 0.4505\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05459 to 0.72243, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8944 - acc: 0.6154 - val_loss: 0.7224 - val_acc: 0.7505\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72243 to 0.50886, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5509 - acc: 0.8097 - val_loss: 0.5089 - val_acc: 0.8092\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50886 to 0.43152, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3661 - acc: 0.8743 - val_loss: 0.4315 - val_acc: 0.8385\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43152 to 0.40825, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2651 - acc: 0.9131 - val_loss: 0.4083 - val_acc: 0.8443\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2027 - acc: 0.9350 - val_loss: 0.4094 - val_acc: 0.8456\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1612 - acc: 0.9486 - val_loss: 0.4246 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1309 - acc: 0.9603 - val_loss: 0.4475 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1072 - acc: 0.9692 - val_loss: 0.4671 - val_acc: 0.8456\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0891 - acc: 0.9740 - val_loss: 0.5002 - val_acc: 0.8392\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04266, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0816 - acc: 0.4110 - val_loss: 1.0427 - val_acc: 0.4710\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04266 to 0.75670, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9067 - acc: 0.5891 - val_loss: 0.7567 - val_acc: 0.7154\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75670 to 0.52977, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5976 - acc: 0.7813 - val_loss: 0.5298 - val_acc: 0.8009\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52977 to 0.44770, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3886 - acc: 0.8637 - val_loss: 0.4477 - val_acc: 0.8290\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44770 to 0.42250, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2793 - acc: 0.9073 - val_loss: 0.4225 - val_acc: 0.8424\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.42250 to 0.41543, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2156 - acc: 0.9288 - val_loss: 0.4154 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1701 - acc: 0.9452 - val_loss: 0.4227 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1357 - acc: 0.9577 - val_loss: 0.4401 - val_acc: 0.8424\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1132 - acc: 0.9654 - val_loss: 0.4621 - val_acc: 0.8385\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0930 - acc: 0.9725 - val_loss: 0.4907 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04857, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0820 - acc: 0.4114 - val_loss: 1.0486 - val_acc: 0.4576\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04857 to 0.71169, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8886 - acc: 0.6194 - val_loss: 0.7117 - val_acc: 0.7352\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71169 to 0.48433, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5386 - acc: 0.8118 - val_loss: 0.4843 - val_acc: 0.8175\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48433 to 0.41395, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3544 - acc: 0.8811 - val_loss: 0.4139 - val_acc: 0.8424\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41395 to 0.39370, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2558 - acc: 0.9168 - val_loss: 0.3937 - val_acc: 0.8558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39370 to 0.39268, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1994 - acc: 0.9356 - val_loss: 0.3927 - val_acc: 0.8532\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1543 - acc: 0.9527 - val_loss: 0.4082 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1267 - acc: 0.9626 - val_loss: 0.4267 - val_acc: 0.8456\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1025 - acc: 0.9677 - val_loss: 0.4533 - val_acc: 0.8437\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0867 - acc: 0.9747 - val_loss: 0.4824 - val_acc: 0.8392\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04827, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0820 - acc: 0.4113 - val_loss: 1.0483 - val_acc: 0.4697\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04827 to 0.74694, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9118 - acc: 0.5869 - val_loss: 0.7469 - val_acc: 0.7256\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.74694 to 0.50797, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5868 - acc: 0.7875 - val_loss: 0.5080 - val_acc: 0.8137\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50797 to 0.42735, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3820 - acc: 0.8693 - val_loss: 0.4273 - val_acc: 0.8328\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42735 to 0.40060, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2743 - acc: 0.9092 - val_loss: 0.4006 - val_acc: 0.8449\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40060 to 0.39716, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2072 - acc: 0.9314 - val_loss: 0.3972 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1609 - acc: 0.9474 - val_loss: 0.4042 - val_acc: 0.8507\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1329 - acc: 0.9576 - val_loss: 0.4201 - val_acc: 0.8519\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1062 - acc: 0.9671 - val_loss: 0.4417 - val_acc: 0.8481\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0870 - acc: 0.9740 - val_loss: 0.4673 - val_acc: 0.8468\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03709, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0796 - acc: 0.4189 - val_loss: 1.0371 - val_acc: 0.4773\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03709 to 0.76197, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9019 - acc: 0.5914 - val_loss: 0.7620 - val_acc: 0.6713\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76197 to 0.52337, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5906 - acc: 0.7879 - val_loss: 0.5234 - val_acc: 0.8028\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52337 to 0.43693, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3831 - acc: 0.8702 - val_loss: 0.4369 - val_acc: 0.8302\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43693 to 0.40455, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2742 - acc: 0.9066 - val_loss: 0.4046 - val_acc: 0.8443\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40455 to 0.40075, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2082 - acc: 0.9327 - val_loss: 0.4008 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1623 - acc: 0.9484 - val_loss: 0.4084 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1314 - acc: 0.9596 - val_loss: 0.4294 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1045 - acc: 0.9701 - val_loss: 0.4549 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0869 - acc: 0.9748 - val_loss: 0.4725 - val_acc: 0.8449\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03940, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0798 - acc: 0.4185 - val_loss: 1.0394 - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03940 to 0.86126, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9457 - acc: 0.5432 - val_loss: 0.8613 - val_acc: 0.5820\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.86126 to 0.75719, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7768 - acc: 0.6244 - val_loss: 0.7572 - val_acc: 0.6216\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.75719 to 0.72070, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6725 - acc: 0.6552 - val_loss: 0.7207 - val_acc: 0.6458\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72070 to 0.70753, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5979 - acc: 0.7062 - val_loss: 0.7075 - val_acc: 0.6675\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.70753 to 0.68377, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5100 - acc: 0.7875 - val_loss: 0.6838 - val_acc: 0.6911\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss improved from 0.68377 to 0.64206, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4010 - acc: 0.8529 - val_loss: 0.6421 - val_acc: 0.7422\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss improved from 0.64206 to 0.59129, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3030 - acc: 0.8967 - val_loss: 0.5913 - val_acc: 0.7741\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.59129 to 0.57708, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2295 - acc: 0.9277 - val_loss: 0.5771 - val_acc: 0.7862\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1751 - acc: 0.9462 - val_loss: 0.5845 - val_acc: 0.7977\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03164, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0786 - acc: 0.4214 - val_loss: 1.0316 - val_acc: 0.4876\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03164 to 0.75305, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9039 - acc: 0.5954 - val_loss: 0.7531 - val_acc: 0.7103\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75305 to 0.51581, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5807 - acc: 0.7919 - val_loss: 0.5158 - val_acc: 0.8034\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51581 to 0.42824, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3728 - acc: 0.8717 - val_loss: 0.4282 - val_acc: 0.8398\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42824 to 0.40524, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2651 - acc: 0.9101 - val_loss: 0.4052 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40524 to 0.40285, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2041 - acc: 0.9312 - val_loss: 0.4029 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1595 - acc: 0.9503 - val_loss: 0.4113 - val_acc: 0.8468\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1283 - acc: 0.9607 - val_loss: 0.4317 - val_acc: 0.8424\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1071 - acc: 0.9669 - val_loss: 0.4548 - val_acc: 0.8417\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0899 - acc: 0.9728 - val_loss: 0.4831 - val_acc: 0.8392\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.98398, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0640 - acc: 0.4398 - val_loss: 0.9840 - val_acc: 0.5839\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.98398 to 0.59433, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7665 - acc: 0.6805 - val_loss: 0.5943 - val_acc: 0.7639\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.59433 to 0.46062, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.4240 - acc: 0.8356 - val_loss: 0.4606 - val_acc: 0.8162\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.46062 to 0.44431, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2646 - acc: 0.9030 - val_loss: 0.4443 - val_acc: 0.8334\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1825 - acc: 0.9357 - val_loss: 0.4712 - val_acc: 0.8315\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1331 - acc: 0.9528 - val_loss: 0.5013 - val_acc: 0.8334\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93597, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0514 - acc: 0.4505 - val_loss: 0.9360 - val_acc: 0.5909\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93597 to 0.56386, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7218 - acc: 0.6903 - val_loss: 0.5639 - val_acc: 0.7735\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.56386 to 0.45979, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3940 - acc: 0.8472 - val_loss: 0.4598 - val_acc: 0.8149\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 20s - loss: 0.2522 - acc: 0.9083 - val_loss: 0.4645 - val_acc: 0.8264\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1707 - acc: 0.9413 - val_loss: 0.5079 - val_acc: 0.8258\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1259 - acc: 0.9567 - val_loss: 0.5719 - val_acc: 0.8283\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.94786, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0593 - acc: 0.4362 - val_loss: 0.9479 - val_acc: 0.5239\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.94786 to 0.74618, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.8144 - acc: 0.5934 - val_loss: 0.7462 - val_acc: 0.6241\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.74618 to 0.59388, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.5969 - acc: 0.7302 - val_loss: 0.5939 - val_acc: 0.7511\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.59388 to 0.50033, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3752 - acc: 0.8510 - val_loss: 0.5003 - val_acc: 0.8047\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.2341 - acc: 0.9120 - val_loss: 0.5081 - val_acc: 0.8188\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1599 - acc: 0.9432 - val_loss: 0.5555 - val_acc: 0.8181\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93017, saving model to /tmp/nn_model.h5\n",
      " - 24s - loss: 1.0536 - acc: 0.4493 - val_loss: 0.9302 - val_acc: 0.6011\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93017 to 0.54995, saving model to /tmp/nn_model.h5\n",
      " - 21s - loss: 0.7144 - acc: 0.7056 - val_loss: 0.5499 - val_acc: 0.7824\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.54995 to 0.46841, saving model to /tmp/nn_model.h5\n",
      " - 21s - loss: 0.3845 - acc: 0.8541 - val_loss: 0.4684 - val_acc: 0.8105\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.46841 to 0.45528, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2480 - acc: 0.9100 - val_loss: 0.4553 - val_acc: 0.8302\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1743 - acc: 0.9408 - val_loss: 0.4899 - val_acc: 0.8283\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1270 - acc: 0.9549 - val_loss: 0.5275 - val_acc: 0.8290\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93198, saving model to /tmp/nn_model.h5\n",
      " - 24s - loss: 1.0524 - acc: 0.4439 - val_loss: 0.9320 - val_acc: 0.5897\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93198 to 0.59970, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7621 - acc: 0.6609 - val_loss: 0.5997 - val_acc: 0.7524\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.59970 to 0.45217, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.4274 - acc: 0.8324 - val_loss: 0.4522 - val_acc: 0.8149\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.45217 to 0.44267, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2696 - acc: 0.9018 - val_loss: 0.4427 - val_acc: 0.8315\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1883 - acc: 0.9331 - val_loss: 0.4733 - val_acc: 0.8283\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1389 - acc: 0.9496 - val_loss: 0.5281 - val_acc: 0.8290\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=256, epochs=6, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07522, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0864 - acc: 0.4000 - val_loss: 1.0752 - val_acc: 0.4051\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07522 to 0.96389, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0346 - acc: 0.4719 - val_loss: 0.9639 - val_acc: 0.5839\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.96389 to 0.73410, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8294 - acc: 0.6930 - val_loss: 0.7341 - val_acc: 0.7104\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.73410 to 0.59017, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5976 - acc: 0.7941 - val_loss: 0.5902 - val_acc: 0.7700\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.59017 to 0.51383, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4547 - acc: 0.8459 - val_loss: 0.5138 - val_acc: 0.8010\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.51383 to 0.47323, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3570 - acc: 0.8831 - val_loss: 0.4732 - val_acc: 0.8143\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47323 to 0.44560, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2926 - acc: 0.9058 - val_loss: 0.4456 - val_acc: 0.8226\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44560 to 0.43332, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2412 - acc: 0.9222 - val_loss: 0.4333 - val_acc: 0.8258\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43332 to 0.42737, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2014 - acc: 0.9407 - val_loss: 0.4274 - val_acc: 0.8262\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42737 to 0.42531, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1727 - acc: 0.9492 - val_loss: 0.4253 - val_acc: 0.8298\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1480 - acc: 0.9584 - val_loss: 0.4281 - val_acc: 0.8322\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1280 - acc: 0.9642 - val_loss: 0.4364 - val_acc: 0.8311\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1106 - acc: 0.9705 - val_loss: 0.4460 - val_acc: 0.8300\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0966 - acc: 0.9748 - val_loss: 0.4458 - val_acc: 0.8330\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0849 - acc: 0.9778 - val_loss: 0.4569 - val_acc: 0.8330\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0745 - acc: 0.9809 - val_loss: 0.4675 - val_acc: 0.8311\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0639 - acc: 0.9849 - val_loss: 0.4703 - val_acc: 0.8351\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0574 - acc: 0.9858 - val_loss: 0.4864 - val_acc: 0.8313\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0515 - acc: 0.9875 - val_loss: 0.4990 - val_acc: 0.8305\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0461 - acc: 0.9893 - val_loss: 0.5089 - val_acc: 0.8311\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07121, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0840 - acc: 0.4044 - val_loss: 1.0712 - val_acc: 0.4053\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07121 to 0.93727, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0165 - acc: 0.4770 - val_loss: 0.9373 - val_acc: 0.5642\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.93727 to 0.75840, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8271 - acc: 0.6624 - val_loss: 0.7584 - val_acc: 0.6945\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.75840 to 0.61711, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6362 - acc: 0.7781 - val_loss: 0.6171 - val_acc: 0.7698\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61711 to 0.53429, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4889 - acc: 0.8366 - val_loss: 0.5343 - val_acc: 0.7933\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.53429 to 0.48712, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3916 - acc: 0.8716 - val_loss: 0.4871 - val_acc: 0.8107\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48712 to 0.45957, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3231 - acc: 0.8965 - val_loss: 0.4596 - val_acc: 0.8179\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45957 to 0.44232, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2694 - acc: 0.9170 - val_loss: 0.4423 - val_acc: 0.8214\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44232 to 0.43294, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2270 - acc: 0.9304 - val_loss: 0.4329 - val_acc: 0.8232\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43294 to 0.42701, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1934 - acc: 0.9433 - val_loss: 0.4270 - val_acc: 0.8281\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1659 - acc: 0.9519 - val_loss: 0.4291 - val_acc: 0.8294\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1427 - acc: 0.9594 - val_loss: 0.4302 - val_acc: 0.8305\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1240 - acc: 0.9673 - val_loss: 0.4348 - val_acc: 0.8334\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1101 - acc: 0.9700 - val_loss: 0.4414 - val_acc: 0.8315\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0953 - acc: 0.9751 - val_loss: 0.4457 - val_acc: 0.8324\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0854 - acc: 0.9772 - val_loss: 0.4553 - val_acc: 0.8336\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0744 - acc: 0.9811 - val_loss: 0.4597 - val_acc: 0.8332\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0656 - acc: 0.9835 - val_loss: 0.4716 - val_acc: 0.8326\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0584 - acc: 0.9850 - val_loss: 0.4841 - val_acc: 0.8315\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0525 - acc: 0.9864 - val_loss: 0.4973 - val_acc: 0.8313\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07190, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0851 - acc: 0.4020 - val_loss: 1.0719 - val_acc: 0.4065\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07190 to 0.95506, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0275 - acc: 0.4640 - val_loss: 0.9551 - val_acc: 0.5483\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95506 to 0.75747, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8392 - acc: 0.6693 - val_loss: 0.7575 - val_acc: 0.7087\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.75747 to 0.61532, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6320 - acc: 0.7892 - val_loss: 0.6153 - val_acc: 0.7628\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61532 to 0.53531, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4874 - acc: 0.8414 - val_loss: 0.5353 - val_acc: 0.7910\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.53531 to 0.48767, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3909 - acc: 0.8714 - val_loss: 0.4877 - val_acc: 0.8082\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48767 to 0.45899, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3224 - acc: 0.8934 - val_loss: 0.4590 - val_acc: 0.8152\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45899 to 0.44073, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2678 - acc: 0.9126 - val_loss: 0.4407 - val_acc: 0.8211\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44073 to 0.42813, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2257 - acc: 0.9289 - val_loss: 0.4281 - val_acc: 0.8262\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1940 - acc: 0.9430 - val_loss: 0.4290 - val_acc: 0.8286\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42813 to 0.42506, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1662 - acc: 0.9506 - val_loss: 0.4251 - val_acc: 0.8326\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1450 - acc: 0.9594 - val_loss: 0.4311 - val_acc: 0.8320\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1272 - acc: 0.9650 - val_loss: 0.4373 - val_acc: 0.8309\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1123 - acc: 0.9699 - val_loss: 0.4423 - val_acc: 0.8313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0977 - acc: 0.9750 - val_loss: 0.4508 - val_acc: 0.8317\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0860 - acc: 0.9779 - val_loss: 0.4598 - val_acc: 0.8319\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0766 - acc: 0.9809 - val_loss: 0.4701 - val_acc: 0.8320\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0684 - acc: 0.9835 - val_loss: 0.4816 - val_acc: 0.8315\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0592 - acc: 0.9862 - val_loss: 0.4968 - val_acc: 0.8309\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0540 - acc: 0.9874 - val_loss: 0.5147 - val_acc: 0.8252\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07213, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0846 - acc: 0.4037 - val_loss: 1.0721 - val_acc: 0.4084\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07213 to 0.94340, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0235 - acc: 0.4676 - val_loss: 0.9434 - val_acc: 0.5688\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94340 to 0.74387, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8221 - acc: 0.6659 - val_loss: 0.7439 - val_acc: 0.7142\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74387 to 0.60195, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6160 - acc: 0.7885 - val_loss: 0.6019 - val_acc: 0.7675\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60195 to 0.52213, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4656 - acc: 0.8451 - val_loss: 0.5221 - val_acc: 0.7999\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52213 to 0.47670, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3668 - acc: 0.8815 - val_loss: 0.4767 - val_acc: 0.8145\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47670 to 0.44851, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2985 - acc: 0.9051 - val_loss: 0.4485 - val_acc: 0.8209\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44851 to 0.43502, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2483 - acc: 0.9194 - val_loss: 0.4350 - val_acc: 0.8245\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43502 to 0.42824, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2076 - acc: 0.9377 - val_loss: 0.4282 - val_acc: 0.8266\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42824 to 0.42587, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1764 - acc: 0.9484 - val_loss: 0.4259 - val_acc: 0.8301\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1507 - acc: 0.9562 - val_loss: 0.4262 - val_acc: 0.8311\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1298 - acc: 0.9648 - val_loss: 0.4317 - val_acc: 0.8307\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1140 - acc: 0.9682 - val_loss: 0.4338 - val_acc: 0.8336\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0993 - acc: 0.9736 - val_loss: 0.4458 - val_acc: 0.8315\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0868 - acc: 0.9779 - val_loss: 0.4519 - val_acc: 0.8301\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0760 - acc: 0.9823 - val_loss: 0.4661 - val_acc: 0.8298\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0681 - acc: 0.9820 - val_loss: 0.4753 - val_acc: 0.8311\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0599 - acc: 0.9846 - val_loss: 0.4916 - val_acc: 0.8290\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0526 - acc: 0.9872 - val_loss: 0.4969 - val_acc: 0.8319\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0472 - acc: 0.9881 - val_loss: 0.5183 - val_acc: 0.8286\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.06895, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0847 - acc: 0.4005 - val_loss: 1.0689 - val_acc: 0.4129\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.06895 to 0.93654, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0130 - acc: 0.4805 - val_loss: 0.9365 - val_acc: 0.5678\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.93654 to 0.76232, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8247 - acc: 0.6635 - val_loss: 0.7623 - val_acc: 0.6908\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.76232 to 0.62283, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6366 - acc: 0.7834 - val_loss: 0.6228 - val_acc: 0.7674\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.62283 to 0.54100, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4951 - acc: 0.8384 - val_loss: 0.5410 - val_acc: 0.7946\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54100 to 0.49381, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3951 - acc: 0.8704 - val_loss: 0.4938 - val_acc: 0.8073\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49381 to 0.46661, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3233 - acc: 0.8954 - val_loss: 0.4666 - val_acc: 0.8152\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46661 to 0.44692, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2719 - acc: 0.9137 - val_loss: 0.4469 - val_acc: 0.8230\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44692 to 0.43881, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2289 - acc: 0.9294 - val_loss: 0.4388 - val_acc: 0.8264\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43881 to 0.43141, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1947 - acc: 0.9435 - val_loss: 0.4314 - val_acc: 0.8284\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1656 - acc: 0.9515 - val_loss: 0.4319 - val_acc: 0.8296\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1469 - acc: 0.9574 - val_loss: 0.4369 - val_acc: 0.8301\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1294 - acc: 0.9646 - val_loss: 0.4361 - val_acc: 0.8324\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1113 - acc: 0.9694 - val_loss: 0.4445 - val_acc: 0.8332\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0975 - acc: 0.9744 - val_loss: 0.4537 - val_acc: 0.8334\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0866 - acc: 0.9771 - val_loss: 0.4654 - val_acc: 0.8326\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0760 - acc: 0.9811 - val_loss: 0.4721 - val_acc: 0.8356\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0675 - acc: 0.9844 - val_loss: 0.4890 - val_acc: 0.8337\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0606 - acc: 0.9860 - val_loss: 0.4983 - val_acc: 0.8337\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0540 - acc: 0.9869 - val_loss: 0.5153 - val_acc: 0.8311\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07381, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0850 - acc: 0.4027 - val_loss: 1.0738 - val_acc: 0.4034\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07381 to 0.94632, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0238 - acc: 0.4697 - val_loss: 0.9463 - val_acc: 0.5623\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94632 to 0.74058, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8244 - acc: 0.6752 - val_loss: 0.7406 - val_acc: 0.7129\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74058 to 0.59707, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6035 - acc: 0.7955 - val_loss: 0.5971 - val_acc: 0.7681\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.59707 to 0.52126, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4595 - acc: 0.8463 - val_loss: 0.5213 - val_acc: 0.7986\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52126 to 0.47874, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3632 - acc: 0.8781 - val_loss: 0.4787 - val_acc: 0.8120\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47874 to 0.44960, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2963 - acc: 0.9055 - val_loss: 0.4496 - val_acc: 0.8196\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44960 to 0.43170, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2482 - acc: 0.9222 - val_loss: 0.4317 - val_acc: 0.8250\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: val_loss improved from 0.43170 to 0.42656, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2086 - acc: 0.9353 - val_loss: 0.4266 - val_acc: 0.8269\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42656 to 0.42349, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1761 - acc: 0.9471 - val_loss: 0.4235 - val_acc: 0.8288\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1504 - acc: 0.9580 - val_loss: 0.4235 - val_acc: 0.8311\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1313 - acc: 0.9635 - val_loss: 0.4274 - val_acc: 0.8326\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1132 - acc: 0.9696 - val_loss: 0.4312 - val_acc: 0.8322\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0991 - acc: 0.9739 - val_loss: 0.4471 - val_acc: 0.8330\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0876 - acc: 0.9775 - val_loss: 0.4519 - val_acc: 0.8334\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0773 - acc: 0.9801 - val_loss: 0.4636 - val_acc: 0.8319\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0677 - acc: 0.9840 - val_loss: 0.4679 - val_acc: 0.8317\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0607 - acc: 0.9844 - val_loss: 0.4841 - val_acc: 0.8311\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0541 - acc: 0.9869 - val_loss: 0.4947 - val_acc: 0.8313\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0479 - acc: 0.9890 - val_loss: 0.5072 - val_acc: 0.8294\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07274, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0852 - acc: 0.4015 - val_loss: 1.0727 - val_acc: 0.4065\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07274 to 0.94217, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0216 - acc: 0.4732 - val_loss: 0.9422 - val_acc: 0.5563\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94217 to 0.74834, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8271 - acc: 0.6618 - val_loss: 0.7483 - val_acc: 0.7036\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74834 to 0.60519, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6199 - acc: 0.7933 - val_loss: 0.6052 - val_acc: 0.7689\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60519 to 0.52771, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4718 - acc: 0.8426 - val_loss: 0.5277 - val_acc: 0.7942\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52771 to 0.48227, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3756 - acc: 0.8765 - val_loss: 0.4823 - val_acc: 0.8073\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48227 to 0.45640, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3086 - acc: 0.9004 - val_loss: 0.4564 - val_acc: 0.8148\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45640 to 0.44067, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2557 - acc: 0.9184 - val_loss: 0.4407 - val_acc: 0.8171\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44067 to 0.43517, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2170 - acc: 0.9336 - val_loss: 0.4352 - val_acc: 0.8201\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43517 to 0.43084, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1824 - acc: 0.9485 - val_loss: 0.4308 - val_acc: 0.8226\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43084 to 0.42953, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1584 - acc: 0.9539 - val_loss: 0.4295 - val_acc: 0.8252\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1364 - acc: 0.9623 - val_loss: 0.4380 - val_acc: 0.8247\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1204 - acc: 0.9667 - val_loss: 0.4393 - val_acc: 0.8271\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1043 - acc: 0.9739 - val_loss: 0.4450 - val_acc: 0.8273\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0921 - acc: 0.9771 - val_loss: 0.4560 - val_acc: 0.8256\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0812 - acc: 0.9802 - val_loss: 0.4621 - val_acc: 0.8288\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0717 - acc: 0.9824 - val_loss: 0.4765 - val_acc: 0.8283\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0636 - acc: 0.9844 - val_loss: 0.4867 - val_acc: 0.8283\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0573 - acc: 0.9865 - val_loss: 0.4987 - val_acc: 0.8315\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0502 - acc: 0.9892 - val_loss: 0.5160 - val_acc: 0.8281\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07480, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0855 - acc: 0.4024 - val_loss: 1.0748 - val_acc: 0.4019\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07480 to 0.95448, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0287 - acc: 0.4611 - val_loss: 0.9545 - val_acc: 0.5589\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95448 to 0.74625, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8334 - acc: 0.6639 - val_loss: 0.7462 - val_acc: 0.7142\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74625 to 0.60248, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6193 - acc: 0.7951 - val_loss: 0.6025 - val_acc: 0.7700\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60248 to 0.52631, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4732 - acc: 0.8433 - val_loss: 0.5263 - val_acc: 0.7935\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52631 to 0.48389, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3804 - acc: 0.8717 - val_loss: 0.4839 - val_acc: 0.8093\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48389 to 0.45478, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3117 - acc: 0.8986 - val_loss: 0.4548 - val_acc: 0.8196\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45478 to 0.43726, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2585 - acc: 0.9195 - val_loss: 0.4373 - val_acc: 0.8250\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.43726 to 0.42888, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2206 - acc: 0.9307 - val_loss: 0.4289 - val_acc: 0.8275\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.42888 to 0.42622, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1848 - acc: 0.9450 - val_loss: 0.4262 - val_acc: 0.8294\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42622 to 0.42524, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1593 - acc: 0.9548 - val_loss: 0.4252 - val_acc: 0.8328\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1369 - acc: 0.9631 - val_loss: 0.4276 - val_acc: 0.8349\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1192 - acc: 0.9684 - val_loss: 0.4308 - val_acc: 0.8328\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1036 - acc: 0.9722 - val_loss: 0.4351 - val_acc: 0.8332\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0915 - acc: 0.9761 - val_loss: 0.4435 - val_acc: 0.8303\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0794 - acc: 0.9806 - val_loss: 0.4553 - val_acc: 0.8330\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0720 - acc: 0.9811 - val_loss: 0.4607 - val_acc: 0.8334\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0635 - acc: 0.9841 - val_loss: 0.4704 - val_acc: 0.8322\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0550 - acc: 0.9872 - val_loss: 0.4872 - val_acc: 0.8317\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0501 - acc: 0.9882 - val_loss: 0.5039 - val_acc: 0.8300\n",
      "------------------\n",
      "Train on 12334 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07179, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0850 - acc: 0.4017 - val_loss: 1.0718 - val_acc: 0.4082\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07179 to 0.94014, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0197 - acc: 0.4728 - val_loss: 0.9401 - val_acc: 0.5652\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.94014 to 0.74393, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8241 - acc: 0.6684 - val_loss: 0.7439 - val_acc: 0.7169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.74393 to 0.60095, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6174 - acc: 0.7931 - val_loss: 0.6009 - val_acc: 0.7719\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.60095 to 0.52290, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4745 - acc: 0.8430 - val_loss: 0.5229 - val_acc: 0.7978\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52290 to 0.47427, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3779 - acc: 0.8758 - val_loss: 0.4743 - val_acc: 0.8129\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.47427 to 0.44530, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3074 - acc: 0.9013 - val_loss: 0.4453 - val_acc: 0.8196\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.44530 to 0.42883, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2554 - acc: 0.9200 - val_loss: 0.4288 - val_acc: 0.8245\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.42883 to 0.41950, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2150 - acc: 0.9351 - val_loss: 0.4195 - val_acc: 0.8284\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.41950 to 0.41892, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1820 - acc: 0.9465 - val_loss: 0.4189 - val_acc: 0.8309\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.41892 to 0.41628, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1548 - acc: 0.9550 - val_loss: 0.4163 - val_acc: 0.8324\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1342 - acc: 0.9625 - val_loss: 0.4194 - val_acc: 0.8351\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1168 - acc: 0.9685 - val_loss: 0.4263 - val_acc: 0.8364\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1029 - acc: 0.9721 - val_loss: 0.4316 - val_acc: 0.8375\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0894 - acc: 0.9766 - val_loss: 0.4403 - val_acc: 0.8379\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0784 - acc: 0.9790 - val_loss: 0.4488 - val_acc: 0.8370\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0686 - acc: 0.9828 - val_loss: 0.4593 - val_acc: 0.8364\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0607 - acc: 0.9850 - val_loss: 0.4709 - val_acc: 0.8353\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0547 - acc: 0.9870 - val_loss: 0.4830 - val_acc: 0.8349\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0476 - acc: 0.9895 - val_loss: 0.4939 - val_acc: 0.8339\n",
      "------------------\n",
      "Train on 12335 samples, validate on 5287 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07167, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0846 - acc: 0.4031 - val_loss: 1.0717 - val_acc: 0.4070\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07167 to 0.95496, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0268 - acc: 0.4609 - val_loss: 0.9550 - val_acc: 0.5578\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.95496 to 0.76902, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8465 - acc: 0.6456 - val_loss: 0.7690 - val_acc: 0.6919\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.76902 to 0.61568, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6454 - acc: 0.7798 - val_loss: 0.6157 - val_acc: 0.7657\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.61568 to 0.52939, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4905 - acc: 0.8399 - val_loss: 0.5294 - val_acc: 0.7989\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.52939 to 0.48192, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3862 - acc: 0.8717 - val_loss: 0.4819 - val_acc: 0.8118\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.48192 to 0.45225, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3152 - acc: 0.8977 - val_loss: 0.4523 - val_acc: 0.8182\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.45225 to 0.44061, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2622 - acc: 0.9154 - val_loss: 0.4406 - val_acc: 0.8226\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44061 to 0.43122, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2202 - acc: 0.9323 - val_loss: 0.4312 - val_acc: 0.8264\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43122 to 0.43108, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1873 - acc: 0.9462 - val_loss: 0.4311 - val_acc: 0.8292\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1601 - acc: 0.9543 - val_loss: 0.4332 - val_acc: 0.8300\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1389 - acc: 0.9609 - val_loss: 0.4365 - val_acc: 0.8328\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1228 - acc: 0.9664 - val_loss: 0.4406 - val_acc: 0.8341\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1073 - acc: 0.9714 - val_loss: 0.4463 - val_acc: 0.8351\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0938 - acc: 0.9745 - val_loss: 0.4580 - val_acc: 0.8337\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0825 - acc: 0.9791 - val_loss: 0.4636 - val_acc: 0.8356\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0740 - acc: 0.9807 - val_loss: 0.4752 - val_acc: 0.8330\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0651 - acc: 0.9835 - val_loss: 0.4833 - val_acc: 0.8330\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0576 - acc: 0.9855 - val_loss: 0.5019 - val_acc: 0.8301\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0517 - acc: 0.9872 - val_loss: 0.5090 - val_acc: 0.8311\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 10\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.3,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(1)\n",
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                          cnn_train1, cnn_train2,cnn_train3, cnn_train4,cnn_train5, cnn_train6,\n",
    "                          nn_train1,nn_train2\n",
    "                         ])\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                         cnn_test1, cnn_test2,cnn_test3, cnn_test4,cnn_test5, cnn_test6,\n",
    "                         nn_test1,nn_test2 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 307) (8392, 307)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id', 'text','tag_txt','ne_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2,train_cvec3,train_cvec4,train_tf5,train_tf6])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2,test_cvec3,test_cvec4,test_tf5,test_tf6])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n",
    "f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = np.round(best_single_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05495\tvalid-mlogloss:1.05589\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.22917\tvalid-mlogloss:0.283362\n",
      "[400]\ttrain-mlogloss:0.188912\tvalid-mlogloss:0.275492\n",
      "[600]\ttrain-mlogloss:0.159888\tvalid-mlogloss:0.274038\n",
      "Stopping. Best iteration:\n",
      "[582]\ttrain-mlogloss:0.162114\tvalid-mlogloss:0.273804\n",
      "\n",
      "train log loss 0.155785202408 valid log loss 0.274052922462\n",
      "rev 3.64893025411\n",
      "[0]\ttrain-mlogloss:1.05511\tvalid-mlogloss:1.05574\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.230411\tvalid-mlogloss:0.279515\n",
      "[400]\ttrain-mlogloss:0.189621\tvalid-mlogloss:0.273599\n",
      "Stopping. Best iteration:\n",
      "[466]\ttrain-mlogloss:0.179648\tvalid-mlogloss:0.273092\n",
      "\n",
      "train log loss 0.172694835367 valid log loss 0.273272968444\n",
      "rev 3.65934474124\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05548\tvalid-mlogloss:1.05497\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.238174\tvalid-mlogloss:0.249359\n",
      "[400]\ttrain-mlogloss:0.197235\tvalid-mlogloss:0.2396\n",
      "[600]\ttrain-mlogloss:0.167741\tvalid-mlogloss:0.237097\n",
      "Stopping. Best iteration:\n",
      "[680]\ttrain-mlogloss:0.157927\tvalid-mlogloss:0.236586\n",
      "\n",
      "train log loss 0.152131046655 valid log loss 0.236807728203\n",
      "rev 4.22283515656\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.05518\tvalid-mlogloss:1.05561\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.233405\tvalid-mlogloss:0.267178\n",
      "[400]\ttrain-mlogloss:0.192164\tvalid-mlogloss:0.25931\n",
      "[600]\ttrain-mlogloss:0.163783\tvalid-mlogloss:0.257689\n",
      "Stopping. Best iteration:\n",
      "[557]\ttrain-mlogloss:0.169489\tvalid-mlogloss:0.25761\n",
      "\n",
      "train log loss 0.162889313897 valid log loss 0.257734417397\n",
      "rev 3.8799629871\n",
      "[0]\ttrain-mlogloss:1.05515\tvalid-mlogloss:1.05553\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.232826\tvalid-mlogloss:0.272237\n",
      "[400]\ttrain-mlogloss:0.192198\tvalid-mlogloss:0.264543\n",
      "Stopping. Best iteration:\n",
      "[501]\ttrain-mlogloss:0.176554\tvalid-mlogloss:0.263232\n",
      "\n",
      "train log loss 0.169764098607 valid log loss 0.263276203693\n",
      "rev 3.79829238637\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0108  0.0023  0.9869\n",
      "1  id24541  0.9990  0.0006  0.0004\n",
      "2  id00134  0.0024  0.9969  0.0007\n",
      "3  id27757  0.8513  0.1451  0.0036\n",
      "4  id04081  0.7748  0.1333  0.0919\n",
      "--------------\n",
      "19.2093655254\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0109  0.0023  0.9868\n",
      "1  id24541  0.9990  0.0006  0.0004\n",
      "2  id00134  0.0024  0.9969  0.0007\n",
      "3  id27757  0.8503  0.1461  0.0036\n",
      "4  id04081  0.7736  0.1336  0.0928\n",
      "---------------\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0121  0.0020  0.9860\n",
      "1  id24541  0.9989  0.0008  0.0003\n",
      "2  id00134  0.0024  0.9967  0.0009\n",
      "3  id27757  0.7872  0.2084  0.0044\n",
      "4  id04081  0.7389  0.1326  0.1285\n",
      "---------------\n",
      "local average valid loss 0.26102884804\n",
      "train log loss 0.174869389817\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)\n",
    "# single 27745\n",
    "# mean 27577\n",
    "# weight 27582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.05513\tvalid-mlogloss:1.05578\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.235728\tvalid-mlogloss:0.274042\n",
      "[400]\ttrain-mlogloss:0.197466\tvalid-mlogloss:0.262217\n",
      "[600]\ttrain-mlogloss:0.170176\tvalid-mlogloss:0.259819\n",
      "Stopping. Best iteration:\n",
      "[709]\ttrain-mlogloss:0.157758\tvalid-mlogloss:0.258998\n",
      "\n",
      "train log loss 0.152363540947 valid log loss 0.259178305769\n",
      "rev 3.85834762301\n",
      "[0]\ttrain-mlogloss:1.05514\tvalid-mlogloss:1.05637\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.232729\tvalid-mlogloss:0.296806\n",
      "[400]\ttrain-mlogloss:0.194331\tvalid-mlogloss:0.289282\n",
      "Stopping. Best iteration:\n",
      "[457]\ttrain-mlogloss:0.186021\tvalid-mlogloss:0.28836\n",
      "\n",
      "train log loss 0.179298678934 valid log loss 0.28850220957\n",
      "rev 3.46617795923\n",
      "[0]\ttrain-mlogloss:1.05507\tvalid-mlogloss:1.05578\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.235098\tvalid-mlogloss:0.277992\n",
      "[400]\ttrain-mlogloss:0.19652\tvalid-mlogloss:0.272626\n",
      "Stopping. Best iteration:\n",
      "[435]\ttrain-mlogloss:0.191264\tvalid-mlogloss:0.272243\n",
      "\n",
      "train log loss 0.184246266176 valid log loss 0.272592289131\n",
      "rev 3.66848234478\n",
      "[0]\ttrain-mlogloss:1.05533\tvalid-mlogloss:1.05564\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.234744\tvalid-mlogloss:0.279131\n",
      "[400]\ttrain-mlogloss:0.196543\tvalid-mlogloss:0.270169\n",
      "[600]\ttrain-mlogloss:0.16988\tvalid-mlogloss:0.268135\n",
      "[800]\ttrain-mlogloss:0.148406\tvalid-mlogloss:0.267915\n",
      "Stopping. Best iteration:\n",
      "[755]\ttrain-mlogloss:0.153117\tvalid-mlogloss:0.267491\n",
      "\n",
      "train log loss 0.147880074 valid log loss 0.267848983254\n",
      "rev 3.73344706354\n",
      "[0]\ttrain-mlogloss:1.05546\tvalid-mlogloss:1.05478\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.238863\tvalid-mlogloss:0.24079\n",
      "[400]\ttrain-mlogloss:0.200159\tvalid-mlogloss:0.23143\n",
      "[600]\ttrain-mlogloss:0.17261\tvalid-mlogloss:0.230401\n",
      "Stopping. Best iteration:\n",
      "[674]\ttrain-mlogloss:0.164122\tvalid-mlogloss:0.230061\n",
      "\n",
      "train log loss 0.158613504759 valid log loss 0.230412290555\n",
      "rev 4.34004626051\n",
      "BETTER\n",
      "[0]\ttrain-mlogloss:1.0553\tvalid-mlogloss:1.05536\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.237628\tvalid-mlogloss:0.258179\n",
      "[400]\ttrain-mlogloss:0.199846\tvalid-mlogloss:0.247807\n",
      "[600]\ttrain-mlogloss:0.17255\tvalid-mlogloss:0.244168\n",
      "[800]\ttrain-mlogloss:0.15047\tvalid-mlogloss:0.243113\n",
      "Stopping. Best iteration:\n",
      "[816]\ttrain-mlogloss:0.148787\tvalid-mlogloss:0.242728\n",
      "\n",
      "train log loss 0.14383393616 valid log loss 0.242879332272\n",
      "rev 4.11727087129\n",
      "[0]\ttrain-mlogloss:1.05534\tvalid-mlogloss:1.05551\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.236171\tvalid-mlogloss:0.267833\n",
      "[400]\ttrain-mlogloss:0.197531\tvalid-mlogloss:0.261302\n",
      "[600]\ttrain-mlogloss:0.170781\tvalid-mlogloss:0.2597\n",
      "Stopping. Best iteration:\n",
      "[554]\ttrain-mlogloss:0.176561\tvalid-mlogloss:0.259641\n",
      "\n",
      "train log loss 0.170288282799 valid log loss 0.259699088954\n",
      "rev 3.85061035073\n",
      "[0]\ttrain-mlogloss:1.05522\tvalid-mlogloss:1.05582\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.236123\tvalid-mlogloss:0.269753\n",
      "[400]\ttrain-mlogloss:0.197882\tvalid-mlogloss:0.259661\n",
      "[600]\ttrain-mlogloss:0.170684\tvalid-mlogloss:0.256391\n",
      "Stopping. Best iteration:\n",
      "[593]\ttrain-mlogloss:0.171434\tvalid-mlogloss:0.256246\n",
      "\n",
      "train log loss 0.165741503428 valid log loss 0.256750561739\n",
      "rev 3.89483081645\n",
      "[0]\ttrain-mlogloss:1.05532\tvalid-mlogloss:1.05484\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.23713\tvalid-mlogloss:0.259529\n",
      "[400]\ttrain-mlogloss:0.198381\tvalid-mlogloss:0.251527\n",
      "Stopping. Best iteration:\n",
      "[536]\ttrain-mlogloss:0.179268\tvalid-mlogloss:0.249543\n",
      "\n",
      "train log loss 0.173035664364 valid log loss 0.249855521755\n",
      "rev 4.00231298863\n",
      "[0]\ttrain-mlogloss:1.05513\tvalid-mlogloss:1.05573\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.234852\tvalid-mlogloss:0.279964\n",
      "[400]\ttrain-mlogloss:0.196472\tvalid-mlogloss:0.269867\n",
      "Stopping. Best iteration:\n",
      "[488]\ttrain-mlogloss:0.183462\tvalid-mlogloss:0.268325\n",
      "\n",
      "train log loss 0.176839260263 valid log loss 0.26857140339\n",
      "rev 3.72340460443\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0087  0.0022  0.9891\n",
      "1  id24541  0.9988  0.0007  0.0004\n",
      "2  id00134  0.0025  0.9967  0.0008\n",
      "3  id27757  0.8521  0.1444  0.0035\n",
      "4  id04081  0.7896  0.1223  0.0881\n",
      "--------------\n",
      "38.6549308826\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0087  0.0022  0.9891\n",
      "1  id24541  0.9988  0.0007  0.0004\n",
      "2  id00134  0.0025  0.9967  0.0008\n",
      "3  id27757  0.8522  0.1443  0.0035\n",
      "4  id04081  0.7891  0.1223  0.0886\n",
      "---------------\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0069  0.0019  0.9912\n",
      "1  id24541  0.9987  0.0008  0.0005\n",
      "2  id00134  0.0022  0.9968  0.0010\n",
      "3  id27757  0.8759  0.1205  0.0037\n",
      "4  id04081  0.7823  0.1185  0.0992\n",
      "---------------\n",
      "local average valid loss 0.259628998639\n",
      "train log loss 0.170158857629\n"
     ]
    }
   ],
   "source": [
    "cv_test(10, True)\n",
    "# mean 27436\n",
    "# weight 27437"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
