{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Processing test...\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "train_df['text_cleaned'] = train_df['text'].apply(lambda x: clean_text(x))\n",
    "test_df['text_cleaned'] = test_df['text'].apply(lambda x: clean_text(x))\n",
    "\n",
    "def extract_features(df):\n",
    "    df['len'] = df['text'].apply(lambda x: len(x))\n",
    "    df['n_words'] = df['text'].apply(lambda x: len(x.split(' ')))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "\n",
    "    df.drop(['text_cleaned'], axis=1, inplace=True)\n",
    "    \n",
    "print('Processing train...')\n",
    "extract_features(train_df)\n",
    "print('Processing test...')\n",
    "extract_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tag_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = [a[1] for a in nltk.pos_tag(words)]\n",
    "        res.append(' '.join(tag_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "def ne_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = nltk.pos_tag(words)\n",
    "        ne_tree = nltk.ne_chunk(tag_res)\n",
    "        list_res = nltk.tree2conlltags(ne_tree)\n",
    "        ne_res = [a[2] for a in list_res]\n",
    "        res.append(' '.join(ne_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "train_df['ne_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['ne_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "\n",
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False)\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False)\n",
    "c_vec4.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec6.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 12) (8392, 12)\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "\n",
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04947, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0819 - acc: 0.4045 - val_loss: 1.0495 - val_acc: 0.4359\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04947 to 0.84575, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9451 - acc: 0.5468 - val_loss: 0.8458 - val_acc: 0.6094\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.84575 to 0.73946, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7682 - acc: 0.6417 - val_loss: 0.7395 - val_acc: 0.6592\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.73946 to 0.68408, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6512 - acc: 0.7125 - val_loss: 0.6841 - val_acc: 0.7013\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.68408 to 0.65980, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5477 - acc: 0.7724 - val_loss: 0.6598 - val_acc: 0.7160\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.4472 - acc: 0.8304 - val_loss: 0.6671 - val_acc: 0.7205\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.3606 - acc: 0.8726 - val_loss: 0.6644 - val_acc: 0.7345\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.2951 - acc: 0.9037 - val_loss: 0.6685 - val_acc: 0.7498\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.2393 - acc: 0.9276 - val_loss: 0.6781 - val_acc: 0.7594\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1981 - acc: 0.9442 - val_loss: 0.7004 - val_acc: 0.7696\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04871, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0810 - acc: 0.4129 - val_loss: 1.0487 - val_acc: 0.4684\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04871 to 0.77511, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9231 - acc: 0.5919 - val_loss: 0.7751 - val_acc: 0.7141\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77511 to 0.52088, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5816 - acc: 0.7956 - val_loss: 0.5209 - val_acc: 0.8041\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52088 to 0.43866, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3710 - acc: 0.8744 - val_loss: 0.4387 - val_acc: 0.8245\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43866 to 0.41527, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2671 - acc: 0.9113 - val_loss: 0.4153 - val_acc: 0.8379\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2035 - acc: 0.9335 - val_loss: 0.4153 - val_acc: 0.8411\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1617 - acc: 0.9490 - val_loss: 0.4312 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1293 - acc: 0.9605 - val_loss: 0.4503 - val_acc: 0.8417\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1056 - acc: 0.9681 - val_loss: 0.4760 - val_acc: 0.8373\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0909 - acc: 0.9736 - val_loss: 0.5009 - val_acc: 0.8322\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03800, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0786 - acc: 0.4208 - val_loss: 1.0380 - val_acc: 0.4856\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03800 to 0.78909, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9190 - acc: 0.5858 - val_loss: 0.7891 - val_acc: 0.6873\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.78909 to 0.53515, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6025 - acc: 0.7854 - val_loss: 0.5351 - val_acc: 0.7913\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53515 to 0.44707, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3822 - acc: 0.8694 - val_loss: 0.4471 - val_acc: 0.8213\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44707 to 0.41916, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2739 - acc: 0.9072 - val_loss: 0.4192 - val_acc: 0.8398\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2073 - acc: 0.9325 - val_loss: 0.4193 - val_acc: 0.8373\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1611 - acc: 0.9497 - val_loss: 0.4303 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1305 - acc: 0.9590 - val_loss: 0.4466 - val_acc: 0.8360\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1055 - acc: 0.9689 - val_loss: 0.4749 - val_acc: 0.8366\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0849 - acc: 0.9758 - val_loss: 0.5066 - val_acc: 0.8385\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04201, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0817 - acc: 0.4119 - val_loss: 1.0420 - val_acc: 0.4754\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04201 to 0.77097, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9159 - acc: 0.5795 - val_loss: 0.7710 - val_acc: 0.7052\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77097 to 0.51289, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6005 - acc: 0.7826 - val_loss: 0.5129 - val_acc: 0.8073\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51289 to 0.42939, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3846 - acc: 0.8656 - val_loss: 0.4294 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42939 to 0.40420, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2785 - acc: 0.9066 - val_loss: 0.4042 - val_acc: 0.8411\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40420 to 0.39426, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2130 - acc: 0.9303 - val_loss: 0.3943 - val_acc: 0.8437\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1686 - acc: 0.9473 - val_loss: 0.3993 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1355 - acc: 0.9589 - val_loss: 0.4078 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1095 - acc: 0.9689 - val_loss: 0.4307 - val_acc: 0.8507\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0922 - acc: 0.9728 - val_loss: 0.4421 - val_acc: 0.8488\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03782, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0799 - acc: 0.4170 - val_loss: 1.0378 - val_acc: 0.4869\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03782 to 0.71686, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8921 - acc: 0.6076 - val_loss: 0.7169 - val_acc: 0.7377\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71686 to 0.48691, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5574 - acc: 0.8072 - val_loss: 0.4869 - val_acc: 0.8194\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48691 to 0.41355, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3646 - acc: 0.8745 - val_loss: 0.4135 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41355 to 0.39329, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2668 - acc: 0.9100 - val_loss: 0.3933 - val_acc: 0.8545\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39329 to 0.39217, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2050 - acc: 0.9318 - val_loss: 0.3922 - val_acc: 0.8571\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1573 - acc: 0.9497 - val_loss: 0.4063 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1284 - acc: 0.9611 - val_loss: 0.4332 - val_acc: 0.8462\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1037 - acc: 0.9687 - val_loss: 0.4554 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0886 - acc: 0.9742 - val_loss: 0.4857 - val_acc: 0.8398\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04589, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0804 - acc: 0.4164 - val_loss: 1.0459 - val_acc: 0.4678\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04589 to 0.80696, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9342 - acc: 0.5634 - val_loss: 0.8070 - val_acc: 0.6618\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80696 to 0.55736, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6399 - acc: 0.7596 - val_loss: 0.5574 - val_acc: 0.7843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.55736 to 0.44606, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4101 - acc: 0.8606 - val_loss: 0.4461 - val_acc: 0.8309\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44606 to 0.41209, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2847 - acc: 0.9055 - val_loss: 0.4121 - val_acc: 0.8385\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41209 to 0.40841, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2128 - acc: 0.9305 - val_loss: 0.4084 - val_acc: 0.8392\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1613 - acc: 0.9530 - val_loss: 0.4217 - val_acc: 0.8379\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1265 - acc: 0.9621 - val_loss: 0.4393 - val_acc: 0.8347\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1046 - acc: 0.9672 - val_loss: 0.4674 - val_acc: 0.8373\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0867 - acc: 0.9745 - val_loss: 0.4872 - val_acc: 0.8322\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03485, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0796 - acc: 0.4187 - val_loss: 1.0349 - val_acc: 0.4927\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03485 to 0.79773, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9296 - acc: 0.5760 - val_loss: 0.7977 - val_acc: 0.6822\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.79773 to 0.52647, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6092 - acc: 0.7812 - val_loss: 0.5265 - val_acc: 0.7977\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52647 to 0.42601, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3797 - acc: 0.8694 - val_loss: 0.4260 - val_acc: 0.8360\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42601 to 0.39465, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2683 - acc: 0.9114 - val_loss: 0.3947 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2000 - acc: 0.9360 - val_loss: 0.3968 - val_acc: 0.8526\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1563 - acc: 0.9506 - val_loss: 0.4104 - val_acc: 0.8449\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1249 - acc: 0.9603 - val_loss: 0.4257 - val_acc: 0.8449\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1017 - acc: 0.9701 - val_loss: 0.4496 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0841 - acc: 0.9755 - val_loss: 0.4841 - val_acc: 0.8354\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03812, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0805 - acc: 0.4158 - val_loss: 1.0381 - val_acc: 0.4780\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03812 to 0.77789, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9169 - acc: 0.5892 - val_loss: 0.7779 - val_acc: 0.6962\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77789 to 0.51781, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5970 - acc: 0.7864 - val_loss: 0.5178 - val_acc: 0.8066\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51781 to 0.42303, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3812 - acc: 0.8704 - val_loss: 0.4230 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42303 to 0.39294, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2664 - acc: 0.9108 - val_loss: 0.3929 - val_acc: 0.8519\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2040 - acc: 0.9344 - val_loss: 0.3952 - val_acc: 0.8519\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1596 - acc: 0.9505 - val_loss: 0.4061 - val_acc: 0.8526\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1294 - acc: 0.9606 - val_loss: 0.4223 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1037 - acc: 0.9697 - val_loss: 0.4505 - val_acc: 0.8500\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0867 - acc: 0.9738 - val_loss: 0.4727 - val_acc: 0.8488\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05830, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0831 - acc: 0.4108 - val_loss: 1.0583 - val_acc: 0.4371\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05830 to 0.74955, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9113 - acc: 0.5897 - val_loss: 0.7496 - val_acc: 0.7518\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.74955 to 0.51574, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5695 - acc: 0.8051 - val_loss: 0.5157 - val_acc: 0.8092\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51574 to 0.43279, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3741 - acc: 0.8719 - val_loss: 0.4328 - val_acc: 0.8385\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43279 to 0.40803, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2700 - acc: 0.9125 - val_loss: 0.4080 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2052 - acc: 0.9339 - val_loss: 0.4090 - val_acc: 0.8462\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1624 - acc: 0.9483 - val_loss: 0.4226 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1318 - acc: 0.9597 - val_loss: 0.4445 - val_acc: 0.8500\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1078 - acc: 0.9686 - val_loss: 0.4636 - val_acc: 0.8456\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0895 - acc: 0.9742 - val_loss: 0.4958 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04460, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0817 - acc: 0.4112 - val_loss: 1.0446 - val_acc: 0.4716\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04460 to 0.75218, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9073 - acc: 0.5942 - val_loss: 0.7522 - val_acc: 0.7275\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75218 to 0.51317, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5788 - acc: 0.7943 - val_loss: 0.5132 - val_acc: 0.8130\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51317 to 0.43950, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3751 - acc: 0.8704 - val_loss: 0.4395 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43950 to 0.41535, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2704 - acc: 0.9119 - val_loss: 0.4153 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41535 to 0.41104, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2091 - acc: 0.9306 - val_loss: 0.4110 - val_acc: 0.8481\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1652 - acc: 0.9463 - val_loss: 0.4202 - val_acc: 0.8443\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1316 - acc: 0.9591 - val_loss: 0.4392 - val_acc: 0.8379\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1099 - acc: 0.9659 - val_loss: 0.4611 - val_acc: 0.8385\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0900 - acc: 0.9735 - val_loss: 0.4905 - val_acc: 0.8328\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04607, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0813 - acc: 0.4126 - val_loss: 1.0461 - val_acc: 0.4544\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04607 to 0.71333, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8874 - acc: 0.6188 - val_loss: 0.7133 - val_acc: 0.7256\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71333 to 0.48488, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5432 - acc: 0.8112 - val_loss: 0.4849 - val_acc: 0.8156\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48488 to 0.41583, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3561 - acc: 0.8800 - val_loss: 0.4158 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41583 to 0.39198, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2564 - acc: 0.9157 - val_loss: 0.3920 - val_acc: 0.8481\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.1994 - acc: 0.9348 - val_loss: 0.3947 - val_acc: 0.8494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1541 - acc: 0.9523 - val_loss: 0.4092 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1268 - acc: 0.9624 - val_loss: 0.4291 - val_acc: 0.8443\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1028 - acc: 0.9677 - val_loss: 0.4540 - val_acc: 0.8430\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0868 - acc: 0.9752 - val_loss: 0.4802 - val_acc: 0.8405\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04819, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0824 - acc: 0.4104 - val_loss: 1.0482 - val_acc: 0.4697\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04819 to 0.72243, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8958 - acc: 0.6068 - val_loss: 0.7224 - val_acc: 0.7384\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72243 to 0.49492, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5613 - acc: 0.8007 - val_loss: 0.4949 - val_acc: 0.8156\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49492 to 0.42133, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3707 - acc: 0.8748 - val_loss: 0.4213 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42133 to 0.39681, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2685 - acc: 0.9127 - val_loss: 0.3968 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39681 to 0.39401, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2034 - acc: 0.9328 - val_loss: 0.3940 - val_acc: 0.8519\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1580 - acc: 0.9486 - val_loss: 0.4021 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1310 - acc: 0.9586 - val_loss: 0.4185 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1048 - acc: 0.9679 - val_loss: 0.4407 - val_acc: 0.8475\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0859 - acc: 0.9746 - val_loss: 0.4666 - val_acc: 0.8443\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03725, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0797 - acc: 0.4187 - val_loss: 1.0372 - val_acc: 0.4780\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03725 to 0.75933, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9012 - acc: 0.5908 - val_loss: 0.7593 - val_acc: 0.6937\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75933 to 0.52116, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5862 - acc: 0.7906 - val_loss: 0.5212 - val_acc: 0.8034\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52116 to 0.43347, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3794 - acc: 0.8721 - val_loss: 0.4335 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43347 to 0.40338, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2714 - acc: 0.9076 - val_loss: 0.4034 - val_acc: 0.8468\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40338 to 0.39980, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2061 - acc: 0.9335 - val_loss: 0.3998 - val_acc: 0.8507\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1608 - acc: 0.9485 - val_loss: 0.4075 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1304 - acc: 0.9598 - val_loss: 0.4289 - val_acc: 0.8456\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1035 - acc: 0.9701 - val_loss: 0.4537 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0863 - acc: 0.9750 - val_loss: 0.4717 - val_acc: 0.8430\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04006, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0802 - acc: 0.4184 - val_loss: 1.0401 - val_acc: 0.4818\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04006 to 0.85700, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9430 - acc: 0.5454 - val_loss: 0.8570 - val_acc: 0.5820\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.85700 to 0.75639, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7728 - acc: 0.6259 - val_loss: 0.7564 - val_acc: 0.6209\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.75639 to 0.72298, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6712 - acc: 0.6522 - val_loss: 0.7230 - val_acc: 0.6369\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72298 to 0.71672, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6017 - acc: 0.6975 - val_loss: 0.7167 - val_acc: 0.6509\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.71672 to 0.71199, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5236 - acc: 0.7709 - val_loss: 0.7120 - val_acc: 0.6707\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.4339 - acc: 0.8317 - val_loss: 0.7137 - val_acc: 0.6975\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss improved from 0.71199 to 0.68515, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3458 - acc: 0.8749 - val_loss: 0.6852 - val_acc: 0.7301\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.68515 to 0.67079, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2751 - acc: 0.9073 - val_loss: 0.6708 - val_acc: 0.7428\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.2173 - acc: 0.9276 - val_loss: 0.6741 - val_acc: 0.7569\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03558, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0791 - acc: 0.4201 - val_loss: 1.0356 - val_acc: 0.4876\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03558 to 0.75225, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9068 - acc: 0.5977 - val_loss: 0.7522 - val_acc: 0.7198\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75225 to 0.51073, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5747 - acc: 0.7971 - val_loss: 0.5107 - val_acc: 0.8009\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51073 to 0.42800, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3709 - acc: 0.8745 - val_loss: 0.4280 - val_acc: 0.8437\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42800 to 0.40363, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2656 - acc: 0.9109 - val_loss: 0.4036 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40363 to 0.40065, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2053 - acc: 0.9323 - val_loss: 0.4007 - val_acc: 0.8456\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1614 - acc: 0.9503 - val_loss: 0.4086 - val_acc: 0.8481\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1301 - acc: 0.9595 - val_loss: 0.4318 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1085 - acc: 0.9663 - val_loss: 0.4499 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0910 - acc: 0.9723 - val_loss: 0.4767 - val_acc: 0.8398\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.98555, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0649 - acc: 0.4362 - val_loss: 0.9855 - val_acc: 0.5705\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.98555 to 0.59517, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7694 - acc: 0.6785 - val_loss: 0.5952 - val_acc: 0.7607\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.59517 to 0.46196, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.4244 - acc: 0.8373 - val_loss: 0.4620 - val_acc: 0.8117\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.46196 to 0.44204, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.2639 - acc: 0.9035 - val_loss: 0.4420 - val_acc: 0.8341\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 19s - loss: 0.1820 - acc: 0.9354 - val_loss: 0.4667 - val_acc: 0.8315\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1335 - acc: 0.9520 - val_loss: 0.5004 - val_acc: 0.8373\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93462, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0512 - acc: 0.4520 - val_loss: 0.9346 - val_acc: 0.5935\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93462 to 0.56442, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7209 - acc: 0.6903 - val_loss: 0.5644 - val_acc: 0.7671\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.56442 to 0.46009, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3944 - acc: 0.8466 - val_loss: 0.4601 - val_acc: 0.8156\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 21s - loss: 0.2526 - acc: 0.9066 - val_loss: 0.4669 - val_acc: 0.8245\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1711 - acc: 0.9408 - val_loss: 0.5078 - val_acc: 0.8251\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 21s - loss: 0.1255 - acc: 0.9581 - val_loss: 0.5768 - val_acc: 0.8258\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.94695, saving model to /tmp/nn_model.h5\n",
      " - 22s - loss: 1.0587 - acc: 0.4363 - val_loss: 0.9470 - val_acc: 0.5303\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.94695 to 0.74135, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.8113 - acc: 0.6036 - val_loss: 0.7414 - val_acc: 0.6299\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.74135 to 0.63935, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.6016 - acc: 0.7300 - val_loss: 0.6394 - val_acc: 0.7167\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.63935 to 0.55147, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.4081 - acc: 0.8353 - val_loss: 0.5515 - val_acc: 0.7747\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss improved from 0.55147 to 0.52523, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2611 - acc: 0.9022 - val_loss: 0.5252 - val_acc: 0.8079\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1758 - acc: 0.9359 - val_loss: 0.5734 - val_acc: 0.8124\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93058, saving model to /tmp/nn_model.h5\n",
      " - 23s - loss: 1.0537 - acc: 0.4496 - val_loss: 0.9306 - val_acc: 0.5973\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93058 to 0.55164, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7141 - acc: 0.7036 - val_loss: 0.5516 - val_acc: 0.7786\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.55164 to 0.47262, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.3838 - acc: 0.8544 - val_loss: 0.4726 - val_acc: 0.8079\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.47262 to 0.45695, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2482 - acc: 0.9097 - val_loss: 0.4569 - val_acc: 0.8302\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1743 - acc: 0.9392 - val_loss: 0.4867 - val_acc: 0.8315\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1270 - acc: 0.9552 - val_loss: 0.5279 - val_acc: 0.8264\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93191, saving model to /tmp/nn_model.h5\n",
      " - 25s - loss: 1.0525 - acc: 0.4436 - val_loss: 0.9319 - val_acc: 0.5846\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93191 to 0.60628, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.7612 - acc: 0.6609 - val_loss: 0.6063 - val_acc: 0.7537\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.60628 to 0.45249, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.4281 - acc: 0.8313 - val_loss: 0.4525 - val_acc: 0.8188\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.45249 to 0.44203, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.2703 - acc: 0.9012 - val_loss: 0.4420 - val_acc: 0.8328\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 19s - loss: 0.1880 - acc: 0.9320 - val_loss: 0.4724 - val_acc: 0.8271\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.1388 - acc: 0.9507 - val_loss: 0.5271 - val_acc: 0.8296\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=256, epochs=6, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 301) (8392, 301)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id', 'text','tag_txt','ne_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2,train_cvec3,train_cvec4,train_tf5,train_tf6])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2,test_cvec3,test_cvec4,test_tf5,test_tf6])\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,\n",
    "                       help_train_feat3,lstm_train1, lstm_train2, cnn_train1, cnn_train2,\n",
    "                       cnn_train3, cnn_train4,cnn_train5, cnn_train6])\n",
    "f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,\n",
    "                      lstm_test1, lstm_test2, cnn_test1, cnn_test2,\n",
    "                      cnn_test3, cnn_test4,cnn_test5, cnn_test6\n",
    "                     ])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07662\tvalid-mlogloss:1.07721\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.270416\tvalid-mlogloss:0.307589\n",
      "[400]\ttrain-mlogloss:0.230787\tvalid-mlogloss:0.284336\n",
      "[600]\ttrain-mlogloss:0.208354\tvalid-mlogloss:0.277979\n",
      "[800]\ttrain-mlogloss:0.190546\tvalid-mlogloss:0.275276\n",
      "Stopping. Best iteration:\n",
      "[908]\ttrain-mlogloss:0.181972\tvalid-mlogloss:0.274109\n",
      "\n",
      "train log loss 0.178203339361 valid log loss 0.274234460869\n",
      "rev 3.64651472624\n",
      "[0]\ttrain-mlogloss:1.07671\tvalid-mlogloss:1.07697\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.272211\tvalid-mlogloss:0.300272\n",
      "[400]\ttrain-mlogloss:0.231836\tvalid-mlogloss:0.279137\n",
      "[600]\ttrain-mlogloss:0.208746\tvalid-mlogloss:0.274635\n",
      "[800]\ttrain-mlogloss:0.19098\tvalid-mlogloss:0.272894\n",
      "Stopping. Best iteration:\n",
      "[848]\ttrain-mlogloss:0.187141\tvalid-mlogloss:0.272786\n",
      "\n",
      "train log loss 0.183418303513 valid log loss 0.272820952055\n",
      "rev 3.66540763261\n",
      "[0]\ttrain-mlogloss:1.07689\tvalid-mlogloss:1.07669\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.279457\tvalid-mlogloss:0.276348\n",
      "[400]\ttrain-mlogloss:0.239313\tvalid-mlogloss:0.251537\n",
      "[600]\ttrain-mlogloss:0.216637\tvalid-mlogloss:0.244939\n",
      "[800]\ttrain-mlogloss:0.198223\tvalid-mlogloss:0.241747\n",
      "[1000]\ttrain-mlogloss:0.182614\tvalid-mlogloss:0.240233\n",
      "[1200]\ttrain-mlogloss:0.16878\tvalid-mlogloss:0.239728\n",
      "Stopping. Best iteration:\n",
      "[1156]\ttrain-mlogloss:0.171542\tvalid-mlogloss:0.239648\n",
      "\n",
      "train log loss 0.168367930143 valid log loss 0.239677367939\n",
      "rev 4.17227545762\n",
      "[0]\ttrain-mlogloss:1.07681\tvalid-mlogloss:1.07699\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.275047\tvalid-mlogloss:0.291325\n",
      "[400]\ttrain-mlogloss:0.234727\tvalid-mlogloss:0.268302\n",
      "[600]\ttrain-mlogloss:0.211245\tvalid-mlogloss:0.262794\n",
      "[800]\ttrain-mlogloss:0.193473\tvalid-mlogloss:0.260239\n",
      "[1000]\ttrain-mlogloss:0.178418\tvalid-mlogloss:0.258449\n",
      "Stopping. Best iteration:\n",
      "[1110]\ttrain-mlogloss:0.17076\tvalid-mlogloss:0.25808\n",
      "\n",
      "train log loss 0.167598062316 valid log loss 0.258135766443\n",
      "rev 3.87393042731\n",
      "[0]\ttrain-mlogloss:1.07676\tvalid-mlogloss:1.07692\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.273482\tvalid-mlogloss:0.295486\n",
      "[400]\ttrain-mlogloss:0.233373\tvalid-mlogloss:0.273818\n",
      "[600]\ttrain-mlogloss:0.210255\tvalid-mlogloss:0.268298\n",
      "[800]\ttrain-mlogloss:0.192395\tvalid-mlogloss:0.266401\n",
      "[1000]\ttrain-mlogloss:0.177227\tvalid-mlogloss:0.265576\n",
      "[1200]\ttrain-mlogloss:0.163773\tvalid-mlogloss:0.264999\n",
      "Stopping. Best iteration:\n",
      "[1206]\ttrain-mlogloss:0.163401\tvalid-mlogloss:0.264947\n",
      "\n",
      "train log loss 0.160271395352 valid log loss 0.265238769809\n",
      "rev 3.77018789795\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0168  0.0035  0.9797\n",
      "1  id24541  0.9990  0.0006  0.0004\n",
      "2  id00134  0.0032  0.9957  0.0011\n",
      "3  id27757  0.8512  0.1448  0.0041\n",
      "4  id04081  0.8022  0.1140  0.0838\n",
      "--------------\n",
      "19.1283161417\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0171  0.0035  0.9794\n",
      "1  id24541  0.9990  0.0006  0.0004\n",
      "2  id00134  0.0032  0.9957  0.0011\n",
      "3  id27757  0.8503  0.1456  0.0041\n",
      "4  id04081  0.8017  0.1139  0.0845\n",
      "---------------\n",
      "local average valid loss 0.262021463423\n",
      "train log loss 0.183129634187\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
