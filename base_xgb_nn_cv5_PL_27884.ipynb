{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n",
      "<class 'numpy.ndarray'>\n",
      "(19579, 32) (8392, 32)\n",
      "(19579, 32) (8392, 32)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def pos_tag_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = [a[1] for a in nltk.pos_tag(words)]\n",
    "        res.append(' '.join(tag_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "def ne_text(s):\n",
    "    sents = nltk.sent_tokenize(s)\n",
    "    res = []\n",
    "    for sent in sents:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tag_res = nltk.pos_tag(words)\n",
    "        ne_tree = nltk.ne_chunk(tag_res)\n",
    "        list_res = nltk.tree2conlltags(ne_tree)\n",
    "        ne_res = [a[2] for a in list_res]\n",
    "        res.append(' '.join(ne_res))\n",
    "    return '. '.join(res)\n",
    "\n",
    "train_df['tag_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "train_df['ne_txt'] = train_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['tag_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "test_df['ne_txt'] = test_df[\"text\"].apply(pos_tag_text)\n",
    "\n",
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False)\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False)\n",
    "c_vec4.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "print(type(train_cvec4))\n",
    "\n",
    "# cnt on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "\n",
    "# cnt on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False)\n",
    "tf_vec6.fit(train_df['ne_txt'].values.tolist() + test_df['ne_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['ne_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['ne_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "\n",
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "    \n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.06785, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0861 - acc: 0.4025 - val_loss: 1.0678 - val_acc: 0.4040\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.06785 to 0.83021, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9755 - acc: 0.4977 - val_loss: 0.8302 - val_acc: 0.6311\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.83021 to 0.59028, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6743 - acc: 0.7301 - val_loss: 0.5903 - val_acc: 0.7875\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.59028 to 0.47890, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4581 - acc: 0.8466 - val_loss: 0.4789 - val_acc: 0.8200\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.47890 to 0.43603, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3411 - acc: 0.8889 - val_loss: 0.4360 - val_acc: 0.8385\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.43603 to 0.42680, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2614 - acc: 0.9178 - val_loss: 0.4268 - val_acc: 0.8424\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.2138 - acc: 0.9323 - val_loss: 0.4311 - val_acc: 0.8411\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1757 - acc: 0.9464 - val_loss: 0.4520 - val_acc: 0.8437\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1491 - acc: 0.9554 - val_loss: 0.4970 - val_acc: 0.8411\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1251 - acc: 0.9623 - val_loss: 0.5137 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04966, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0813 - acc: 0.4127 - val_loss: 1.0497 - val_acc: 0.4690\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04966 to 0.76558, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9208 - acc: 0.5916 - val_loss: 0.7656 - val_acc: 0.7141\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76558 to 0.51528, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5757 - acc: 0.7966 - val_loss: 0.5153 - val_acc: 0.8047\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51528 to 0.43481, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3673 - acc: 0.8773 - val_loss: 0.4348 - val_acc: 0.8290\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43481 to 0.41331, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2641 - acc: 0.9120 - val_loss: 0.4133 - val_acc: 0.8366\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2003 - acc: 0.9359 - val_loss: 0.4141 - val_acc: 0.8424\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1590 - acc: 0.9496 - val_loss: 0.4353 - val_acc: 0.8385\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1268 - acc: 0.9608 - val_loss: 0.4502 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1034 - acc: 0.9689 - val_loss: 0.4777 - val_acc: 0.8341\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0887 - acc: 0.9747 - val_loss: 0.5024 - val_acc: 0.8309\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03892, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0788 - acc: 0.4206 - val_loss: 1.0389 - val_acc: 0.4825\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03892 to 0.78998, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9207 - acc: 0.5859 - val_loss: 0.7900 - val_acc: 0.6905\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.78998 to 0.53370, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6026 - acc: 0.7843 - val_loss: 0.5337 - val_acc: 0.7900\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53370 to 0.44713, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3813 - acc: 0.8693 - val_loss: 0.4471 - val_acc: 0.8264\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44713 to 0.41876, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2728 - acc: 0.9073 - val_loss: 0.4188 - val_acc: 0.8392\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2066 - acc: 0.9332 - val_loss: 0.4192 - val_acc: 0.8379\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1601 - acc: 0.9506 - val_loss: 0.4281 - val_acc: 0.8366\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1297 - acc: 0.9589 - val_loss: 0.4497 - val_acc: 0.8411\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1050 - acc: 0.9689 - val_loss: 0.4767 - val_acc: 0.8379\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0844 - acc: 0.9766 - val_loss: 0.5086 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04199, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0817 - acc: 0.4117 - val_loss: 1.0420 - val_acc: 0.4748\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04199 to 0.77317, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9166 - acc: 0.5786 - val_loss: 0.7732 - val_acc: 0.7077\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77317 to 0.51341, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6019 - acc: 0.7819 - val_loss: 0.5134 - val_acc: 0.8086\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51341 to 0.42869, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3835 - acc: 0.8680 - val_loss: 0.4287 - val_acc: 0.8334\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42869 to 0.40330, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2775 - acc: 0.9070 - val_loss: 0.4033 - val_acc: 0.8405\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40330 to 0.39504, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2122 - acc: 0.9307 - val_loss: 0.3950 - val_acc: 0.8443\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1674 - acc: 0.9469 - val_loss: 0.4001 - val_acc: 0.8507\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1345 - acc: 0.9589 - val_loss: 0.4100 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1086 - acc: 0.9695 - val_loss: 0.4342 - val_acc: 0.8513\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0915 - acc: 0.9730 - val_loss: 0.4456 - val_acc: 0.8468\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03779, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0799 - acc: 0.4172 - val_loss: 1.0378 - val_acc: 0.4895\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03779 to 0.70763, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8882 - acc: 0.6123 - val_loss: 0.7076 - val_acc: 0.7441\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.70763 to 0.48541, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5489 - acc: 0.8097 - val_loss: 0.4854 - val_acc: 0.8162\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48541 to 0.41256, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3598 - acc: 0.8770 - val_loss: 0.4126 - val_acc: 0.8468\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41256 to 0.39377, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2642 - acc: 0.9108 - val_loss: 0.3938 - val_acc: 0.8532\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39377 to 0.39259, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2034 - acc: 0.9329 - val_loss: 0.3926 - val_acc: 0.8545\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1563 - acc: 0.9500 - val_loss: 0.4072 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1278 - acc: 0.9620 - val_loss: 0.4338 - val_acc: 0.8456\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1033 - acc: 0.9685 - val_loss: 0.4561 - val_acc: 0.8398\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0883 - acc: 0.9743 - val_loss: 0.4861 - val_acc: 0.8373\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04284, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0801 - acc: 0.4170 - val_loss: 1.0428 - val_acc: 0.4735\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04284 to 0.80890, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9332 - acc: 0.5641 - val_loss: 0.8089 - val_acc: 0.6650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80890 to 0.55920, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6413 - acc: 0.7594 - val_loss: 0.5592 - val_acc: 0.7824\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.55920 to 0.44570, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4107 - acc: 0.8592 - val_loss: 0.4457 - val_acc: 0.8302\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44570 to 0.41096, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2843 - acc: 0.9053 - val_loss: 0.4110 - val_acc: 0.8392\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41096 to 0.40804, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2116 - acc: 0.9308 - val_loss: 0.4080 - val_acc: 0.8417\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9532 - val_loss: 0.4214 - val_acc: 0.8373\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1261 - acc: 0.9626 - val_loss: 0.4385 - val_acc: 0.8341\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1042 - acc: 0.9674 - val_loss: 0.4664 - val_acc: 0.8392\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0863 - acc: 0.9747 - val_loss: 0.4871 - val_acc: 0.8328\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04152, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0805 - acc: 0.4141 - val_loss: 1.0415 - val_acc: 0.4844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04152 to 0.80109, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9353 - acc: 0.5706 - val_loss: 0.8011 - val_acc: 0.6682\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.80109 to 0.53008, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6120 - acc: 0.7788 - val_loss: 0.5301 - val_acc: 0.7971\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53008 to 0.42788, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3818 - acc: 0.8689 - val_loss: 0.4279 - val_acc: 0.8328\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42788 to 0.39585, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2699 - acc: 0.9103 - val_loss: 0.3958 - val_acc: 0.8488\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2011 - acc: 0.9355 - val_loss: 0.3963 - val_acc: 0.8513\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1566 - acc: 0.9499 - val_loss: 0.4115 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1252 - acc: 0.9598 - val_loss: 0.4245 - val_acc: 0.8405\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1018 - acc: 0.9704 - val_loss: 0.4484 - val_acc: 0.8341\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0845 - acc: 0.9755 - val_loss: 0.4794 - val_acc: 0.8354\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03806, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0804 - acc: 0.4166 - val_loss: 1.0381 - val_acc: 0.4805\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03806 to 0.77199, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9151 - acc: 0.5872 - val_loss: 0.7720 - val_acc: 0.7179\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77199 to 0.51762, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5917 - acc: 0.7903 - val_loss: 0.5176 - val_acc: 0.8079\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51762 to 0.42590, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3804 - acc: 0.8707 - val_loss: 0.4259 - val_acc: 0.8437\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42590 to 0.39328, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2656 - acc: 0.9107 - val_loss: 0.3933 - val_acc: 0.8513\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2026 - acc: 0.9339 - val_loss: 0.3957 - val_acc: 0.8545\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1583 - acc: 0.9510 - val_loss: 0.4084 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1280 - acc: 0.9610 - val_loss: 0.4245 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1026 - acc: 0.9709 - val_loss: 0.4516 - val_acc: 0.8488\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0857 - acc: 0.9745 - val_loss: 0.4777 - val_acc: 0.8481\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05214, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0824 - acc: 0.4113 - val_loss: 1.0521 - val_acc: 0.4595\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05214 to 0.73460, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9014 - acc: 0.6042 - val_loss: 0.7346 - val_acc: 0.7588\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.73460 to 0.51334, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5599 - acc: 0.8068 - val_loss: 0.5133 - val_acc: 0.8073\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51334 to 0.43391, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3718 - acc: 0.8718 - val_loss: 0.4339 - val_acc: 0.8398\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43391 to 0.40896, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2696 - acc: 0.9121 - val_loss: 0.4090 - val_acc: 0.8417\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40896 to 0.40809, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2058 - acc: 0.9347 - val_loss: 0.4081 - val_acc: 0.8488\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1632 - acc: 0.9476 - val_loss: 0.4222 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1324 - acc: 0.9594 - val_loss: 0.4441 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1084 - acc: 0.9684 - val_loss: 0.4648 - val_acc: 0.8475\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0901 - acc: 0.9742 - val_loss: 0.4966 - val_acc: 0.8398\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04247, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0814 - acc: 0.4114 - val_loss: 1.0425 - val_acc: 0.4722\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04247 to 0.76075, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9090 - acc: 0.5913 - val_loss: 0.7608 - val_acc: 0.7052\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76075 to 0.52987, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5992 - acc: 0.7824 - val_loss: 0.5299 - val_acc: 0.8009\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52987 to 0.44758, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3884 - acc: 0.8649 - val_loss: 0.4476 - val_acc: 0.8290\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44758 to 0.42018, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2793 - acc: 0.9075 - val_loss: 0.4202 - val_acc: 0.8443\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.42018 to 0.41326, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2156 - acc: 0.9292 - val_loss: 0.4133 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1704 - acc: 0.9450 - val_loss: 0.4197 - val_acc: 0.8449\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1360 - acc: 0.9575 - val_loss: 0.4369 - val_acc: 0.8437\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1135 - acc: 0.9653 - val_loss: 0.4589 - val_acc: 0.8398\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0933 - acc: 0.9726 - val_loss: 0.4869 - val_acc: 0.8347\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04668, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0814 - acc: 0.4128 - val_loss: 1.0467 - val_acc: 0.4620\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04668 to 0.71125, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8892 - acc: 0.6161 - val_loss: 0.7113 - val_acc: 0.7390\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71125 to 0.48398, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5409 - acc: 0.8112 - val_loss: 0.4840 - val_acc: 0.8168\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48398 to 0.41490, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3556 - acc: 0.8802 - val_loss: 0.4149 - val_acc: 0.8411\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41490 to 0.39112, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2560 - acc: 0.9163 - val_loss: 0.3911 - val_acc: 0.8507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.1986 - acc: 0.9356 - val_loss: 0.3942 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1535 - acc: 0.9523 - val_loss: 0.4091 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1258 - acc: 0.9628 - val_loss: 0.4280 - val_acc: 0.8475\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1019 - acc: 0.9680 - val_loss: 0.4530 - val_acc: 0.8430\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0858 - acc: 0.9757 - val_loss: 0.4833 - val_acc: 0.8398\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04801, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0823 - acc: 0.4104 - val_loss: 1.0480 - val_acc: 0.4697\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04801 to 0.72873, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9004 - acc: 0.6036 - val_loss: 0.7287 - val_acc: 0.7352\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72873 to 0.49813, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5661 - acc: 0.7975 - val_loss: 0.4981 - val_acc: 0.8168\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49813 to 0.42128, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3732 - acc: 0.8717 - val_loss: 0.4213 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42128 to 0.39723, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2706 - acc: 0.9111 - val_loss: 0.3972 - val_acc: 0.8456\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39723 to 0.39531, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2051 - acc: 0.9320 - val_loss: 0.3953 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1597 - acc: 0.9476 - val_loss: 0.4041 - val_acc: 0.8513\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1327 - acc: 0.9582 - val_loss: 0.4206 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1064 - acc: 0.9679 - val_loss: 0.4452 - val_acc: 0.8443\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0874 - acc: 0.9749 - val_loss: 0.4683 - val_acc: 0.8430\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03731, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0797 - acc: 0.4188 - val_loss: 1.0373 - val_acc: 0.4761\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03731 to 0.76522, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9043 - acc: 0.5902 - val_loss: 0.7652 - val_acc: 0.6930\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76522 to 0.52702, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5940 - acc: 0.7872 - val_loss: 0.5270 - val_acc: 0.8015\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52702 to 0.43765, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3854 - acc: 0.8697 - val_loss: 0.4377 - val_acc: 0.8322\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43765 to 0.40415, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2752 - acc: 0.9069 - val_loss: 0.4042 - val_acc: 0.8456\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40415 to 0.40407, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2085 - acc: 0.9329 - val_loss: 0.4041 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1628 - acc: 0.9482 - val_loss: 0.4072 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1319 - acc: 0.9589 - val_loss: 0.4282 - val_acc: 0.8443\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1046 - acc: 0.9702 - val_loss: 0.4516 - val_acc: 0.8449\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0872 - acc: 0.9748 - val_loss: 0.4703 - val_acc: 0.8488\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03884, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0798 - acc: 0.4191 - val_loss: 1.0388 - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03884 to 0.86417, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9460 - acc: 0.5426 - val_loss: 0.8642 - val_acc: 0.5775\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.86417 to 0.76366, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7790 - acc: 0.6237 - val_loss: 0.7637 - val_acc: 0.6126\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.76366 to 0.72085, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6717 - acc: 0.6532 - val_loss: 0.7209 - val_acc: 0.6388\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72085 to 0.71214, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5982 - acc: 0.7018 - val_loss: 0.7121 - val_acc: 0.6682\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.71214 to 0.68898, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5137 - acc: 0.7802 - val_loss: 0.6890 - val_acc: 0.6918\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss improved from 0.68898 to 0.65556, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4128 - acc: 0.8421 - val_loss: 0.6556 - val_acc: 0.7358\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss improved from 0.65556 to 0.60520, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3131 - acc: 0.8920 - val_loss: 0.6052 - val_acc: 0.7709\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.60520 to 0.58587, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2361 - acc: 0.9254 - val_loss: 0.5859 - val_acc: 0.7817\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1792 - acc: 0.9444 - val_loss: 0.5894 - val_acc: 0.7945\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03590, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0792 - acc: 0.4207 - val_loss: 1.0359 - val_acc: 0.4876\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03590 to 0.75013, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9072 - acc: 0.5969 - val_loss: 0.7501 - val_acc: 0.7096\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75013 to 0.50911, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5719 - acc: 0.7962 - val_loss: 0.5091 - val_acc: 0.8054\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50911 to 0.42555, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3702 - acc: 0.8749 - val_loss: 0.4256 - val_acc: 0.8443\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42555 to 0.40311, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2651 - acc: 0.9108 - val_loss: 0.4031 - val_acc: 0.8513\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40311 to 0.40120, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2046 - acc: 0.9326 - val_loss: 0.4012 - val_acc: 0.8488\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9506 - val_loss: 0.4088 - val_acc: 0.8488\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1292 - acc: 0.9602 - val_loss: 0.4325 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1078 - acc: 0.9664 - val_loss: 0.4509 - val_acc: 0.8430\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0904 - acc: 0.9725 - val_loss: 0.4781 - val_acc: 0.8398\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.95193, saving model to /tmp/nn_model.h5\n",
      " - 25s - loss: 1.0550 - acc: 0.4420 - val_loss: 0.9519 - val_acc: 0.6063\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.95193 to 0.58686, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.7437 - acc: 0.6821 - val_loss: 0.5869 - val_acc: 0.7652\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.58686 to 0.47848, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.4201 - acc: 0.8411 - val_loss: 0.4785 - val_acc: 0.8162\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.47848 to 0.45038, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2665 - acc: 0.9039 - val_loss: 0.4504 - val_acc: 0.8328\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1867 - acc: 0.9314 - val_loss: 0.5015 - val_acc: 0.8245\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 20s - loss: 0.1409 - acc: 0.9509 - val_loss: 0.5382 - val_acc: 0.8385\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93492, saving model to /tmp/nn_model.h5\n",
      " - 23s - loss: 1.0512 - acc: 0.4518 - val_loss: 0.9349 - val_acc: 0.5935\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93492 to 0.56494, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.7215 - acc: 0.6908 - val_loss: 0.5649 - val_acc: 0.7703\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.56494 to 0.46026, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.3949 - acc: 0.8463 - val_loss: 0.4603 - val_acc: 0.8175\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 19s - loss: 0.2530 - acc: 0.9071 - val_loss: 0.4674 - val_acc: 0.8245\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 20s - loss: 0.1724 - acc: 0.9408 - val_loss: 0.5088 - val_acc: 0.8251\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.1263 - acc: 0.9572 - val_loss: 0.5775 - val_acc: 0.8226\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.94644, saving model to /tmp/nn_model.h5\n",
      " - 25s - loss: 1.0587 - acc: 0.4360 - val_loss: 0.9464 - val_acc: 0.5303\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.94644 to 0.73909, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.8112 - acc: 0.6010 - val_loss: 0.7391 - val_acc: 0.6324\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.73909 to 0.63697, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.6012 - acc: 0.7311 - val_loss: 0.6370 - val_acc: 0.7205\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.63697 to 0.55244, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.4078 - acc: 0.8375 - val_loss: 0.5524 - val_acc: 0.7709\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss improved from 0.55244 to 0.52739, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.2628 - acc: 0.9024 - val_loss: 0.5274 - val_acc: 0.8066\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.1735 - acc: 0.9358 - val_loss: 0.5673 - val_acc: 0.8149\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93014, saving model to /tmp/nn_model.h5\n",
      " - 25s - loss: 1.0536 - acc: 0.4496 - val_loss: 0.9301 - val_acc: 0.5986\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93014 to 0.55048, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.7138 - acc: 0.7050 - val_loss: 0.5505 - val_acc: 0.7817\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.55048 to 0.46990, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.3847 - acc: 0.8546 - val_loss: 0.4699 - val_acc: 0.8092\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.46990 to 0.45497, saving model to /tmp/nn_model.h5\n",
      " - 20s - loss: 0.2482 - acc: 0.9099 - val_loss: 0.4550 - val_acc: 0.8302\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 19s - loss: 0.1745 - acc: 0.9395 - val_loss: 0.4892 - val_acc: 0.8290\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.1268 - acc: 0.9557 - val_loss: 0.5251 - val_acc: 0.8309\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.93195, saving model to /tmp/nn_model.h5\n",
      " - 25s - loss: 1.0524 - acc: 0.4440 - val_loss: 0.9319 - val_acc: 0.5897\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.93195 to 0.59893, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.7619 - acc: 0.6601 - val_loss: 0.5989 - val_acc: 0.7537\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.59893 to 0.45242, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.4267 - acc: 0.8322 - val_loss: 0.4524 - val_acc: 0.8143\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss improved from 0.45242 to 0.44187, saving model to /tmp/nn_model.h5\n",
      " - 19s - loss: 0.2693 - acc: 0.9020 - val_loss: 0.4419 - val_acc: 0.8309\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 19s - loss: 0.1877 - acc: 0.9327 - val_loss: 0.4727 - val_acc: 0.8322\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 19s - loss: 0.1381 - acc: 0.9504 - val_loss: 0.5282 - val_acc: 0.8283\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=256, epochs=6, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 256) (8392, 256)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id', 'text','tag_txt','ne_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2,train_cvec3,train_cvec4,train_tf5,train_tf6])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2,test_cvec3,test_cvec4,test_tf5,test_tf6])\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,\n",
    "                       help_train_feat3,lstm_train1, lstm_train2, cnn_train1, cnn_train2,\n",
    "                       cnn_train3, cnn_train4,cnn_train5, cnn_train6])\n",
    "f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,\n",
    "                      lstm_test1, lstm_test2, cnn_test1, cnn_test2,\n",
    "                      cnn_test3, cnn_test4,cnn_test5, cnn_test6\n",
    "                     ])\n",
    "f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.03,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.06572\tvalid-mlogloss:1.0665\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.246856\tvalid-mlogloss:0.293983\n",
      "[400]\ttrain-mlogloss:0.209031\tvalid-mlogloss:0.281892\n",
      "[600]\ttrain-mlogloss:0.183464\tvalid-mlogloss:0.278967\n",
      "[800]\ttrain-mlogloss:0.16237\tvalid-mlogloss:0.278355\n",
      "Stopping. Best iteration:\n",
      "[790]\ttrain-mlogloss:0.163347\tvalid-mlogloss:0.278191\n",
      "\n",
      "train log loss 0.15870163603 valid log loss 0.278285241527\n",
      "rev 3.59343526273\n",
      "[0]\ttrain-mlogloss:1.06588\tvalid-mlogloss:1.06627\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.248869\tvalid-mlogloss:0.286643\n",
      "[400]\ttrain-mlogloss:0.210247\tvalid-mlogloss:0.277318\n",
      "Stopping. Best iteration:\n",
      "[524]\ttrain-mlogloss:0.193438\tvalid-mlogloss:0.27593\n",
      "\n",
      "train log loss 0.18754816338 valid log loss 0.276002976232\n",
      "rev 3.62314933576\n",
      "[0]\ttrain-mlogloss:1.06615\tvalid-mlogloss:1.06584\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.256692\tvalid-mlogloss:0.259971\n",
      "[400]\ttrain-mlogloss:0.218107\tvalid-mlogloss:0.246827\n",
      "[600]\ttrain-mlogloss:0.191814\tvalid-mlogloss:0.243315\n",
      "[800]\ttrain-mlogloss:0.170147\tvalid-mlogloss:0.2419\n",
      "Stopping. Best iteration:\n",
      "[838]\ttrain-mlogloss:0.166558\tvalid-mlogloss:0.241602\n",
      "\n",
      "train log loss 0.161891978771 valid log loss 0.241781018514\n",
      "rev 4.13597397407\n",
      "[0]\ttrain-mlogloss:1.066\tvalid-mlogloss:1.06617\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.25192\tvalid-mlogloss:0.276492\n",
      "[400]\ttrain-mlogloss:0.213934\tvalid-mlogloss:0.264926\n",
      "[600]\ttrain-mlogloss:0.188111\tvalid-mlogloss:0.262176\n",
      "[800]\ttrain-mlogloss:0.16697\tvalid-mlogloss:0.260918\n",
      "Stopping. Best iteration:\n",
      "[775]\ttrain-mlogloss:0.169504\tvalid-mlogloss:0.260766\n",
      "\n",
      "train log loss 0.164590539872 valid log loss 0.260820048061\n",
      "rev 3.83406109858\n",
      "[0]\ttrain-mlogloss:1.06596\tvalid-mlogloss:1.06613\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.250346\tvalid-mlogloss:0.282783\n",
      "[400]\ttrain-mlogloss:0.212465\tvalid-mlogloss:0.271867\n",
      "[600]\ttrain-mlogloss:0.186439\tvalid-mlogloss:0.269627\n",
      "[800]\ttrain-mlogloss:0.165159\tvalid-mlogloss:0.268399\n",
      "Stopping. Best iteration:\n",
      "[831]\ttrain-mlogloss:0.162311\tvalid-mlogloss:0.268133\n",
      "\n",
      "train log loss 0.157695883038 valid log loss 0.268572186876\n",
      "rev 3.72339374241\n",
      "        id    EAP     HPL     MWS\n",
      "0  id02310  0.011  0.0031  0.9859\n",
      "1  id24541  0.999  0.0006  0.0004\n",
      "2  id00134  0.003  0.9959  0.0011\n",
      "3  id27757  0.711  0.2856  0.0034\n",
      "4  id04081  0.784  0.1363  0.0797\n",
      "--------------\n",
      "18.9100134135\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0111  0.0031  0.9857\n",
      "1  id24541  0.9990  0.0006  0.0004\n",
      "2  id00134  0.0030  0.9959  0.0011\n",
      "3  id27757  0.7099  0.2867  0.0034\n",
      "4  id04081  0.7829  0.1367  0.0804\n",
      "---------------\n",
      "local average valid loss 0.265092294242\n",
      "train log loss 0.178282596233\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
