{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:27<00:00,  1.05s/it]\n",
      "100%|██████████| 26/26 [01:06<00:00,  2.55s/it]\n",
      "100%|██████████| 26/26 [07:54<00:00, 18.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:11<00:00,  2.33it/s]\n",
      "100%|██████████| 26/26 [00:27<00:00,  1.05s/it]\n",
      "100%|██████████| 26/26 [03:35<00:00,  8.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 26685) (8392, 26685)\n"
     ]
    }
   ],
   "source": [
    "# Clean text\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "punctuation = ['.', '..', '...', ',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def clean_text(x):\n",
    "    x.lower()\n",
    "    for p in punctuation:\n",
    "        x.replace(p, '')\n",
    "    return x\n",
    "\n",
    "def extract_features(in_df, train_flag=False):\n",
    "    df = in_df.copy()\n",
    "    df['text_cleaned'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['n_.'] = df['text'].str.count('\\.')\n",
    "    df['n_...'] = df['text'].str.count('\\...')\n",
    "    df['n_,'] = df['text'].str.count('\\,')\n",
    "    df['n_:'] = df['text'].str.count('\\:')\n",
    "    df['n_;'] = df['text'].str.count('\\;')\n",
    "    df['n_-'] = df['text'].str.count('\\-')\n",
    "    df['n_?'] = df['text'].str.count('\\?')\n",
    "    df['n_!'] = df['text'].str.count('\\!')\n",
    "    df['n_\\''] = df['text'].str.count('\\'')\n",
    "    df['n_\"'] = df['text'].str.count('\\\"')\n",
    "\n",
    "    # First words in a sentence\n",
    "    df['n_The '] = df['text'].str.count('The ')\n",
    "    df['n_I '] = df['text'].str.count('I ')\n",
    "    df['n_It '] = df['text'].str.count('It ')\n",
    "    df['n_He '] = df['text'].str.count('He ')\n",
    "    df['n_Me '] = df['text'].str.count('Me ')\n",
    "    df['n_She '] = df['text'].str.count('She ')\n",
    "    df['n_We '] = df['text'].str.count('We ')\n",
    "    df['n_They '] = df['text'].str.count('They ')\n",
    "    df['n_You '] = df['text'].str.count('You ')\n",
    "    df['n_the'] = df['text_cleaned'].str.count('the ')\n",
    "    df['n_ a '] = df['text_cleaned'].str.count(' a ')\n",
    "    df['n_appear'] = df['text_cleaned'].str.count('appear')\n",
    "    df['n_little'] = df['text_cleaned'].str.count('little')\n",
    "    df['n_was '] = df['text_cleaned'].str.count('was ')\n",
    "    df['n_one '] = df['text_cleaned'].str.count('one ')\n",
    "    df['n_two '] = df['text_cleaned'].str.count('two ')\n",
    "    df['n_three '] = df['text_cleaned'].str.count('three ')\n",
    "    df['n_ten '] = df['text_cleaned'].str.count('ten ')\n",
    "    df['n_is '] = df['text_cleaned'].str.count('is ')\n",
    "    df['n_are '] = df['text_cleaned'].str.count('are ')\n",
    "    df['n_ed'] = df['text_cleaned'].str.count('ed ')\n",
    "    df['n_however'] = df['text_cleaned'].str.count('however')\n",
    "    df['n_ to '] = df['text_cleaned'].str.count(' to ')\n",
    "    df['n_into'] = df['text_cleaned'].str.count('into')\n",
    "    df['n_about '] = df['text_cleaned'].str.count('about ')\n",
    "    df['n_th'] = df['text_cleaned'].str.count('th')\n",
    "    df['n_er'] = df['text_cleaned'].str.count('er')\n",
    "    df['n_ex'] = df['text_cleaned'].str.count('ex')\n",
    "    df['n_an '] = df['text_cleaned'].str.count('an ')\n",
    "    df['n_ground'] = df['text_cleaned'].str.count('ground')\n",
    "    df['n_any'] = df['text_cleaned'].str.count('any')\n",
    "    df['n_silence'] = df['text_cleaned'].str.count('silence')\n",
    "    df['n_wall'] = df['text_cleaned'].str.count('wall')\n",
    "    \n",
    "    # Find numbers of different combinations\n",
    "    for c in tqdm(alphabet.upper()):\n",
    "        df['n_' + c] = df['text'].str.count(c)\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c + '.'] = df['text'].str.count(c + '\\.')\n",
    "        df['n_' + c + ','] = df['text'].str.count(c + '\\,')\n",
    "        df['n_' + c + '?'] = df['text'].str.count(c + '\\?')\n",
    "        df['n_' + c + ';'] = df['text'].str.count(c + '\\;')\n",
    "        df['n_' + c + ':'] = df['text'].str.count(c + '\\:')\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2 + '.'] = df['text'].str.count(c + c2 + '\\.')\n",
    "            df['n_' + c + c2 + ','] = df['text'].str.count(c + c2 + '\\,')\n",
    "            df['n_' + c + c2 + '?'] = df['text'].str.count(c + c2 + '\\?')\n",
    "            df['n_' + c + c2 + ';'] = df['text'].str.count(c + c2 + '\\;')\n",
    "            df['n_' + c + c2 + ':'] = df['text'].str.count(c + c2 + '\\:')\n",
    "            df['n_' + c + ', ' + c2] = df['text'].str.count(c + '\\, ' + c2)\n",
    "\n",
    "    # And now starting processing of cleaned text\n",
    "    for c in tqdm(alphabet):\n",
    "        df['n_' + c] = df['text_cleaned'].str.count(c)\n",
    "        df['n_' + c + ' '] = df['text_cleaned'].str.count(c + ' ')\n",
    "        df['n_' + ' ' + c] = df['text_cleaned'].str.count(' ' + c)\n",
    "\n",
    "        for c2 in alphabet:\n",
    "            df['n_' + c + c2] = df['text_cleaned'].str.count(c + c2)\n",
    "            df['n_' + c + c2 + ' '] = df['text_cleaned'].str.count(c + c2 + ' ')\n",
    "            df['n_' + ' ' + c + c2] = df['text_cleaned'].str.count(' ' + c + c2)\n",
    "            df['n_' + c + ' ' + c2] = df['text_cleaned'].str.count(c + ' ' + c2)\n",
    "\n",
    "            for c3 in alphabet:\n",
    "                df['n_' + c + c2 + c3] = df['text_cleaned'].str.count(c + c2 + c3)\n",
    "                \n",
    "    if train_flag:\n",
    "        df.drop(['text_cleaned','text','author','id'], axis=1, inplace=True)\n",
    "    else:\n",
    "        df.drop(['text_cleaned','text','id'], axis=1, inplace=True)\n",
    "    return df.values\n",
    "    \n",
    "print('Processing train...')\n",
    "train_hand_features = extract_features(train_df,train_flag=True)\n",
    "print('Processing test...')\n",
    "test_hand_features = extract_features(test_df)\n",
    "print(train_hand_features.shape,test_hand_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' DET VERB NOUN ADJ NOUN PUNCT', ' DT VBZ NN JJ NNS .', ' nsubj ROOT nmod amod attr punct')\n",
      "train done 271.81710720062256\n",
      "test done 117.49479866027832\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/usage/models#usage-import\n",
    "# https://spacy.io/usage/models\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "# change ne to tag\n",
    "def get_spacy_text(s):\n",
    "    pos,tag,dep = '','',''\n",
    "    for token in spacy_nlp(s):\n",
    "        pos = pos + ' ' + token.pos_\n",
    "        tag = tag + ' ' + token.tag_\n",
    "        dep = dep + ' ' + token.dep_\n",
    "\n",
    "    return pos,tag,dep\n",
    "\n",
    "print(get_spacy_text('this is kaggle spooky games.'))\n",
    "\n",
    "import time\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in train_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "train_df['pos_txt'],train_df['tag_txt'],train_df['dep_txt'] = poss, tags, deps\n",
    "print('train done',time.time() - start_t)\n",
    "\n",
    "\n",
    "start_t = time.time()\n",
    "poss,tags,deps = [],[],[]\n",
    "for s in test_df[\"text\"].values:\n",
    "    pos,tag,dep = get_spacy_text(s)\n",
    "    poss.append(pos)\n",
    "    tags.append(tag)\n",
    "    deps.append(dep)\n",
    "test_df['pos_txt'],test_df['tag_txt'],test_df['dep_txt'] = poss, tags, deps\n",
    "print('test done', time.time() - start_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n",
      "(19579, 38) (8392, 38)\n",
      "(19579, 186) (8392, 186)\n",
      "(19579, 45) (8392, 45)\n"
     ]
    }
   ],
   "source": [
    "# cnt on tag\n",
    "c_vec3 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec3.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_cvec3 = c_vec3.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_cvec3 = c_vec3.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_cvec3.shape,test_cvec3.shape)\n",
    "\n",
    "# cnt on ne\n",
    "c_vec4 = CountVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "c_vec4.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_cvec4 = c_vec4.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_cvec4 = c_vec4.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_cvec4.shape,test_cvec4.shape)\n",
    "\n",
    "# cnt on dep\n",
    "c_vec7 = CountVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "c_vec7.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_cvec7 = c_vec7.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_cvec7 = c_vec7.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_cvec7.shape,test_cvec7.shape)\n",
    "\n",
    "# tfidf on tag\n",
    "tf_vec5 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec5.fit(train_df['tag_txt'].values.tolist() + test_df['tag_txt'].values.tolist())\n",
    "train_tf5 = tf_vec5.transform(train_df['tag_txt'].values.tolist()).toarray()\n",
    "test_tf5 = tf_vec5.transform(test_df['tag_txt'].values.tolist()).toarray()\n",
    "print(train_tf5.shape,test_tf5.shape)\n",
    "\n",
    "# tfidf on ne\n",
    "tf_vec6 = TfidfVectorizer(lowercase=False,ngram_range=(1,2))\n",
    "tf_vec6.fit(train_df['pos_txt'].values.tolist() + test_df['pos_txt'].values.tolist())\n",
    "train_tf6 = tf_vec6.transform(train_df['pos_txt'].values.tolist()).toarray()\n",
    "test_tf6 = tf_vec6.transform(test_df['pos_txt'].values.tolist()).toarray()\n",
    "print(train_tf6.shape,test_tf6.shape)\n",
    "\n",
    "# tfidf on dep\n",
    "tf_vec8 = TfidfVectorizer(lowercase=False,ngram_range=(1,1))\n",
    "tf_vec8.fit(train_df['dep_txt'].values.tolist() + test_df['dep_txt'].values.tolist())\n",
    "train_tf8 = tf_vec8.transform(train_df['dep_txt'].values.tolist()).toarray()\n",
    "test_tf8 = tf_vec8.transform(test_df['dep_txt'].values.tolist()).toarray()\n",
    "print(train_tf8.shape,test_tf8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp feat done\n"
     ]
    }
   ],
   "source": [
    "all_nlp_train = np.hstack([train_cvec3,train_cvec4,train_tf5,train_tf6,train_cvec7, train_tf8]) \n",
    "all_nlp_test = np.hstack([test_cvec3,test_cvec4,test_tf5,test_tf6, test_cvec7, test_tf8]) \n",
    "print('nlp feat done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'text', 'author', 'pos_txt', 'tag_txt', 'dep_txt', 'num_words',\n",
      "       'num_unique_words', 'num_chars', 'num_stopwords', 'num_punctuations',\n",
      "       'num_words_upper', 'num_words_title', 'mean_word_len', 'unique_r',\n",
      "       'w_p', 'w_p_r', 'stop_r', 'w_p_stop', 'w_p_stop_r', 'num_words_upper_r',\n",
      "       'num_words_title_r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# add features\n",
    "def add_feat(df):\n",
    "    df['unique_r'] = df['num_unique_words'] / df['num_words']\n",
    "    df['w_p'] = df['num_words'] - df['num_punctuations']\n",
    "    df['w_p_r'] = df['w_p'] / df['num_words']\n",
    "    df['stop_r'] = df['num_stopwords'] / df['num_words']\n",
    "    df['w_p_stop'] = df['w_p'] - df['num_stopwords']\n",
    "    df['w_p_stop_r'] = df['w_p_stop'] / df['num_words']\n",
    "    df['num_words_upper_r'] = df['num_words_upper'] / df['num_words']\n",
    "    df['num_words_title_r'] = df['num_words_title'] / df['num_words']\n",
    "\n",
    "add_feat(train_df)\n",
    "add_feat(test_df)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "full_cvec1 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "## add svd3\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec1)\n",
    "train_svd3 = pd.DataFrame(svd_obj.transform(train_cvec))\n",
    "test_svd3 = pd.DataFrame(svd_obj.transform(test_cvec))\n",
    "\n",
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "full_cvec2 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "## add svd4\n",
    "svd_obj = TruncatedSVD(n_components=n_comp)\n",
    "svd_obj.fit(full_cvec2)\n",
    "train_svd4 = pd.DataFrame(svd_obj.transform(train_cvec2))\n",
    "test_svd4 = pd.DataFrame(svd_obj.transform(test_cvec2))\n",
    "\n",
    "# add cnt char\n",
    "c_vec = CountVectorizer(ngram_range=(1,1), analyzer='char',max_df=0.8)\n",
    "full_cvec3 = c_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec3 = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec3 = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec3.shape,test_cvec3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_svd_train = np.hstack([train_svd,train_svd2,train_svd3,train_svd4,train_cvec3.toarray()])\n",
    "all_svd_test = np.hstack([test_svd,test_svd2,test_svd3,test_svd4,test_cvec3.toarray()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 15) (8392, 15)\n"
     ]
    }
   ],
   "source": [
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "def gen_nb_feats(rnd=1):\n",
    "    help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    hand_train, hand_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "    kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        tmp_model = MultinomialNB(alpha=0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "        help_tfidf_train[test_index] = tmp_train_feat\n",
    "        help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # tfidf to nb\n",
    "        X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "        help_tfidf_train2[test_index] = tmp_train_feat\n",
    "        help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec to nb\n",
    "        X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "        help_cnt1_train[test_index] = tmp_train_feat\n",
    "        help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "\n",
    "        # count vec2 to nb \n",
    "        X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "        help_cnt2_train[test_index] = tmp_train_feat\n",
    "        help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "        \n",
    "        # hand feature to nb\n",
    "        X_train, X_test = train_hand_features[train_index], train_hand_features[test_index]\n",
    "        tmp_model = MultinomialNB(0.025,fit_prior=False)\n",
    "        tmp_model.fit(X_train,y_train)\n",
    "        tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "        tmp_test_feat = tmp_model.predict_proba(test_hand_features)\n",
    "        hand_train[test_index] = tmp_train_feat\n",
    "        hand_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    help_train_feat = np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train,hand_train])\n",
    "    help_test_feat = np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test,hand_test])\n",
    "\n",
    "    return help_train_feat,help_test_feat\n",
    "\n",
    "help_train_feat,help_test_feat = gen_nb_feats(1)\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "help_train_feat2,help_test_feat2 = gen_nb_feats(2)\n",
    "help_train_feat3,help_test_feat3 = gen_nb_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04313, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0787 - acc: 0.4097 - val_loss: 1.0431 - val_acc: 0.4710\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04313 to 0.76624, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9084 - acc: 0.5828 - val_loss: 0.7662 - val_acc: 0.7045\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76624 to 0.56726, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6142 - acc: 0.7701 - val_loss: 0.5673 - val_acc: 0.7951\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.56726 to 0.47139, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4174 - acc: 0.8587 - val_loss: 0.4714 - val_acc: 0.8258\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.47139 to 0.43201, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2999 - acc: 0.8991 - val_loss: 0.4320 - val_acc: 0.8315\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.43201 to 0.42545, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2243 - acc: 0.9278 - val_loss: 0.4254 - val_acc: 0.8379\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1726 - acc: 0.9454 - val_loss: 0.4426 - val_acc: 0.8430\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1365 - acc: 0.9608 - val_loss: 0.4663 - val_acc: 0.8449\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1144 - acc: 0.9672 - val_loss: 0.4904 - val_acc: 0.8430\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0905 - acc: 0.9757 - val_loss: 0.5272 - val_acc: 0.8360\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04960, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0813 - acc: 0.4125 - val_loss: 1.0496 - val_acc: 0.4678\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04960 to 0.78425, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9279 - acc: 0.5862 - val_loss: 0.7842 - val_acc: 0.7058\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.78425 to 0.52750, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5948 - acc: 0.7882 - val_loss: 0.5275 - val_acc: 0.8028\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52750 to 0.44285, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3785 - acc: 0.8739 - val_loss: 0.4429 - val_acc: 0.8207\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44285 to 0.41596, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2717 - acc: 0.9108 - val_loss: 0.4160 - val_acc: 0.8354\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41596 to 0.41496, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2068 - acc: 0.9333 - val_loss: 0.4150 - val_acc: 0.8411\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1653 - acc: 0.9471 - val_loss: 0.4288 - val_acc: 0.8392\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1326 - acc: 0.9592 - val_loss: 0.4485 - val_acc: 0.8443\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1082 - acc: 0.9669 - val_loss: 0.4727 - val_acc: 0.8411\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0933 - acc: 0.9729 - val_loss: 0.4978 - val_acc: 0.8373\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03618, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0782 - acc: 0.4227 - val_loss: 1.0362 - val_acc: 0.4869\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03618 to 0.77683, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9127 - acc: 0.5909 - val_loss: 0.7768 - val_acc: 0.7058\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77683 to 0.52993, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5919 - acc: 0.7877 - val_loss: 0.5299 - val_acc: 0.8028\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52993 to 0.45013, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3799 - acc: 0.8698 - val_loss: 0.4501 - val_acc: 0.8258\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.45013 to 0.42059, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2750 - acc: 0.9086 - val_loss: 0.4206 - val_acc: 0.8385\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.42059 to 0.42029, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2093 - acc: 0.9323 - val_loss: 0.4203 - val_acc: 0.8341\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1627 - acc: 0.9498 - val_loss: 0.4331 - val_acc: 0.8322\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1325 - acc: 0.9587 - val_loss: 0.4489 - val_acc: 0.8322\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1071 - acc: 0.9684 - val_loss: 0.4760 - val_acc: 0.8334\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0865 - acc: 0.9753 - val_loss: 0.5054 - val_acc: 0.8341\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04200, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0817 - acc: 0.4117 - val_loss: 1.0420 - val_acc: 0.4754\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04200 to 0.76569, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9127 - acc: 0.5846 - val_loss: 0.7657 - val_acc: 0.7084\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76569 to 0.50975, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5932 - acc: 0.7835 - val_loss: 0.5097 - val_acc: 0.8079\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50975 to 0.42777, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3799 - acc: 0.8688 - val_loss: 0.4278 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42777 to 0.40286, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2753 - acc: 0.9075 - val_loss: 0.4029 - val_acc: 0.8417\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2110 - acc: 0.9314 - val_loss: 0.4063 - val_acc: 0.8443\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss improved from 0.40286 to 0.40004, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1677 - acc: 0.9470 - val_loss: 0.4000 - val_acc: 0.8494\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1347 - acc: 0.9594 - val_loss: 0.4089 - val_acc: 0.8513\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1092 - acc: 0.9695 - val_loss: 0.4320 - val_acc: 0.8519\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0919 - acc: 0.9725 - val_loss: 0.4434 - val_acc: 0.8475\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03800, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0799 - acc: 0.4170 - val_loss: 1.0380 - val_acc: 0.4888\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03800 to 0.72844, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9002 - acc: 0.6015 - val_loss: 0.7284 - val_acc: 0.7237\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72844 to 0.48985, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5667 - acc: 0.8008 - val_loss: 0.4899 - val_acc: 0.8226\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48985 to 0.41480, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3674 - acc: 0.8744 - val_loss: 0.4148 - val_acc: 0.8475\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41480 to 0.39298, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2682 - acc: 0.9099 - val_loss: 0.3930 - val_acc: 0.8545\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39298 to 0.39080, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2060 - acc: 0.9325 - val_loss: 0.3908 - val_acc: 0.8564\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1579 - acc: 0.9489 - val_loss: 0.4044 - val_acc: 0.8519\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1290 - acc: 0.9617 - val_loss: 0.4316 - val_acc: 0.8488\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1043 - acc: 0.9689 - val_loss: 0.4548 - val_acc: 0.8411\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0890 - acc: 0.9742 - val_loss: 0.4860 - val_acc: 0.8392\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04547, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0804 - acc: 0.4161 - val_loss: 1.0455 - val_acc: 0.4665\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04547 to 0.81015, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9353 - acc: 0.5649 - val_loss: 0.8102 - val_acc: 0.6605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.81015 to 0.55608, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6406 - acc: 0.7605 - val_loss: 0.5561 - val_acc: 0.7830\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.55608 to 0.44470, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4092 - acc: 0.8610 - val_loss: 0.4447 - val_acc: 0.8315\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44470 to 0.41083, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2840 - acc: 0.9052 - val_loss: 0.4108 - val_acc: 0.8405\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41083 to 0.40680, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2117 - acc: 0.9313 - val_loss: 0.4068 - val_acc: 0.8417\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9537 - val_loss: 0.4205 - val_acc: 0.8411\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1259 - acc: 0.9627 - val_loss: 0.4379 - val_acc: 0.8354\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1040 - acc: 0.9673 - val_loss: 0.4673 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0864 - acc: 0.9745 - val_loss: 0.4852 - val_acc: 0.8341\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04151, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0805 - acc: 0.4142 - val_loss: 1.0415 - val_acc: 0.4844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04151 to 0.79852, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9349 - acc: 0.5705 - val_loss: 0.7985 - val_acc: 0.6739\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.79852 to 0.53034, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6128 - acc: 0.7794 - val_loss: 0.5303 - val_acc: 0.7958\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.53034 to 0.42888, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3847 - acc: 0.8682 - val_loss: 0.4289 - val_acc: 0.8341\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42888 to 0.39550, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2729 - acc: 0.9105 - val_loss: 0.3955 - val_acc: 0.8513\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39550 to 0.39397, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2036 - acc: 0.9357 - val_loss: 0.3940 - val_acc: 0.8519\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1596 - acc: 0.9496 - val_loss: 0.4073 - val_acc: 0.8437\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1280 - acc: 0.9602 - val_loss: 0.4236 - val_acc: 0.8417\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1047 - acc: 0.9684 - val_loss: 0.4504 - val_acc: 0.8315\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0873 - acc: 0.9740 - val_loss: 0.4778 - val_acc: 0.8328\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03946, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0806 - acc: 0.4158 - val_loss: 1.0395 - val_acc: 0.4773\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03946 to 0.77823, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9202 - acc: 0.5857 - val_loss: 0.7782 - val_acc: 0.7128\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.77823 to 0.52349, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5974 - acc: 0.7850 - val_loss: 0.5235 - val_acc: 0.8079\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52349 to 0.42417, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3802 - acc: 0.8715 - val_loss: 0.4242 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42417 to 0.39273, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2655 - acc: 0.9112 - val_loss: 0.3927 - val_acc: 0.8494\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2026 - acc: 0.9347 - val_loss: 0.3953 - val_acc: 0.8532\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1584 - acc: 0.9513 - val_loss: 0.4079 - val_acc: 0.8532\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1281 - acc: 0.9611 - val_loss: 0.4240 - val_acc: 0.8539\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1031 - acc: 0.9691 - val_loss: 0.4514 - val_acc: 0.8519\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0872 - acc: 0.9739 - val_loss: 0.4726 - val_acc: 0.8494\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.05450, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0824 - acc: 0.4115 - val_loss: 1.0545 - val_acc: 0.4422\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.05450 to 0.72547, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8990 - acc: 0.6068 - val_loss: 0.7255 - val_acc: 0.7492\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.72547 to 0.50770, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5516 - acc: 0.8102 - val_loss: 0.5077 - val_acc: 0.8073\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.50770 to 0.43074, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3659 - acc: 0.8744 - val_loss: 0.4307 - val_acc: 0.8392\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43074 to 0.40797, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2650 - acc: 0.9130 - val_loss: 0.4080 - val_acc: 0.8424\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2025 - acc: 0.9350 - val_loss: 0.4085 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1606 - acc: 0.9493 - val_loss: 0.4238 - val_acc: 0.8462\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1304 - acc: 0.9602 - val_loss: 0.4467 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1067 - acc: 0.9687 - val_loss: 0.4661 - val_acc: 0.8462\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0887 - acc: 0.9745 - val_loss: 0.4996 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04217, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0814 - acc: 0.4111 - val_loss: 1.0422 - val_acc: 0.4716\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04217 to 0.75898, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9062 - acc: 0.5952 - val_loss: 0.7590 - val_acc: 0.7128\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.75898 to 0.52641, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5943 - acc: 0.7841 - val_loss: 0.5264 - val_acc: 0.8015\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52641 to 0.44541, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3843 - acc: 0.8670 - val_loss: 0.4454 - val_acc: 0.8302\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.44541 to 0.41918, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2758 - acc: 0.9091 - val_loss: 0.4192 - val_acc: 0.8443\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.41918 to 0.41309, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2125 - acc: 0.9297 - val_loss: 0.4131 - val_acc: 0.8488\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1677 - acc: 0.9454 - val_loss: 0.4212 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1337 - acc: 0.9581 - val_loss: 0.4396 - val_acc: 0.8430\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1115 - acc: 0.9657 - val_loss: 0.4619 - val_acc: 0.8379\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0917 - acc: 0.9730 - val_loss: 0.4902 - val_acc: 0.8366\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04468, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0813 - acc: 0.4127 - val_loss: 1.0447 - val_acc: 0.4608\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04468 to 0.71144, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8829 - acc: 0.6216 - val_loss: 0.7114 - val_acc: 0.7243\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.71144 to 0.48792, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5441 - acc: 0.8088 - val_loss: 0.4879 - val_acc: 0.8149\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.48792 to 0.41694, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3592 - acc: 0.8798 - val_loss: 0.4169 - val_acc: 0.8417\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.41694 to 0.39200, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2588 - acc: 0.9149 - val_loss: 0.3920 - val_acc: 0.8481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 1s - loss: 0.2013 - acc: 0.9344 - val_loss: 0.3931 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1558 - acc: 0.9519 - val_loss: 0.4068 - val_acc: 0.8468\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1279 - acc: 0.9629 - val_loss: 0.4249 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1036 - acc: 0.9672 - val_loss: 0.4512 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0874 - acc: 0.9754 - val_loss: 0.4791 - val_acc: 0.8379\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04839, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0825 - acc: 0.4101 - val_loss: 1.0484 - val_acc: 0.4703\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04839 to 0.73035, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8980 - acc: 0.6041 - val_loss: 0.7303 - val_acc: 0.7384\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.73035 to 0.49848, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5672 - acc: 0.7965 - val_loss: 0.4985 - val_acc: 0.8111\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.49848 to 0.42222, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3737 - acc: 0.8727 - val_loss: 0.4222 - val_acc: 0.8347\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42222 to 0.39665, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2698 - acc: 0.9113 - val_loss: 0.3966 - val_acc: 0.8462\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.39665 to 0.39355, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2040 - acc: 0.9325 - val_loss: 0.3935 - val_acc: 0.8507\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1585 - acc: 0.9480 - val_loss: 0.4016 - val_acc: 0.8526\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1313 - acc: 0.9586 - val_loss: 0.4173 - val_acc: 0.8532\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1051 - acc: 0.9684 - val_loss: 0.4394 - val_acc: 0.8462\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0861 - acc: 0.9746 - val_loss: 0.4652 - val_acc: 0.8443\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03749, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0797 - acc: 0.4188 - val_loss: 1.0375 - val_acc: 0.4773\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03749 to 0.76877, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9062 - acc: 0.5873 - val_loss: 0.7688 - val_acc: 0.6892\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76877 to 0.52303, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5926 - acc: 0.7883 - val_loss: 0.5230 - val_acc: 0.8034\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52303 to 0.43528, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3815 - acc: 0.8710 - val_loss: 0.4353 - val_acc: 0.8328\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.43528 to 0.40332, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2724 - acc: 0.9079 - val_loss: 0.4033 - val_acc: 0.8475\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40332 to 0.39764, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2066 - acc: 0.9329 - val_loss: 0.3976 - val_acc: 0.8475\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1609 - acc: 0.9490 - val_loss: 0.4088 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1305 - acc: 0.9598 - val_loss: 0.4303 - val_acc: 0.8449\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1034 - acc: 0.9707 - val_loss: 0.4799 - val_acc: 0.8360\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0865 - acc: 0.9755 - val_loss: 0.4723 - val_acc: 0.8443\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03910, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0798 - acc: 0.4193 - val_loss: 1.0391 - val_acc: 0.4844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03910 to 0.86375, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9449 - acc: 0.5446 - val_loss: 0.8637 - val_acc: 0.5788\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.86375 to 0.76230, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7794 - acc: 0.6239 - val_loss: 0.7623 - val_acc: 0.6177\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.76230 to 0.72519, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6766 - acc: 0.6509 - val_loss: 0.7252 - val_acc: 0.6356\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.72519 to 0.71972, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6068 - acc: 0.6953 - val_loss: 0.7197 - val_acc: 0.6535\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.71972 to 0.71523, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5314 - acc: 0.7667 - val_loss: 0.7152 - val_acc: 0.6669\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.4449 - acc: 0.8255 - val_loss: 0.7199 - val_acc: 0.6975\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss improved from 0.71523 to 0.70620, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3609 - acc: 0.8681 - val_loss: 0.7062 - val_acc: 0.7218\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss improved from 0.70620 to 0.69004, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2880 - acc: 0.9027 - val_loss: 0.6900 - val_acc: 0.7294\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.2292 - acc: 0.9226 - val_loss: 0.6938 - val_acc: 0.7473\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.03512, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0791 - acc: 0.4200 - val_loss: 1.0351 - val_acc: 0.4863\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.03512 to 0.76188, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9116 - acc: 0.5916 - val_loss: 0.7619 - val_acc: 0.7096\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.76188 to 0.51508, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5846 - acc: 0.7922 - val_loss: 0.5151 - val_acc: 0.7958\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.51508 to 0.42696, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3734 - acc: 0.8736 - val_loss: 0.4270 - val_acc: 0.8437\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.42696 to 0.40393, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2658 - acc: 0.9091 - val_loss: 0.4039 - val_acc: 0.8500\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss improved from 0.40393 to 0.40243, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2051 - acc: 0.9318 - val_loss: 0.4024 - val_acc: 0.8468\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 1s - loss: 0.1611 - acc: 0.9497 - val_loss: 0.4102 - val_acc: 0.8475\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1296 - acc: 0.9597 - val_loss: 0.4326 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1083 - acc: 0.9662 - val_loss: 0.4512 - val_acc: 0.8424\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.0910 - acc: 0.9718 - val_loss: 0.4786 - val_acc: 0.8398\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)\n",
    "cnn_train3,cnn_test3,cnn_train4,cnn_test4 = get_cnn_feats(2)\n",
    "cnn_train5,cnn_test5,cnn_train6,cnn_test6 = get_cnn_feats(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.79683, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 1.0284 - acc: 0.4729 - val_loss: 0.7968 - val_acc: 0.6682\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.79683 to 0.48785, saving model to /tmp/nn_model.h5\n",
      " - 39s - loss: 0.5572 - acc: 0.7762 - val_loss: 0.4879 - val_acc: 0.8079\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.48785 to 0.43386, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.3120 - acc: 0.8851 - val_loss: 0.4339 - val_acc: 0.8315\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 40s - loss: 0.1997 - acc: 0.9293 - val_loss: 0.4689 - val_acc: 0.8379\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 39s - loss: 0.1420 - acc: 0.9496 - val_loss: 0.4953 - val_acc: 0.8366\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 39s - loss: 0.1065 - acc: 0.9652 - val_loss: 0.5518 - val_acc: 0.8398\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.80874, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 1.0041 - acc: 0.4781 - val_loss: 0.8087 - val_acc: 0.6758\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.80874 to 0.49368, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.5804 - acc: 0.7638 - val_loss: 0.4937 - val_acc: 0.8028\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.49368 to 0.44960, saving model to /tmp/nn_model.h5\n",
      " - 39s - loss: 0.3116 - acc: 0.8849 - val_loss: 0.4496 - val_acc: 0.8264\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 39s - loss: 0.2039 - acc: 0.9261 - val_loss: 0.5194 - val_acc: 0.8137\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 40s - loss: 0.1404 - acc: 0.9507 - val_loss: 0.5405 - val_acc: 0.8188\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 41s - loss: 0.1021 - acc: 0.9672 - val_loss: 0.6275 - val_acc: 0.8181\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.77977, saving model to /tmp/nn_model.h5\n",
      " - 42s - loss: 1.0080 - acc: 0.4841 - val_loss: 0.7798 - val_acc: 0.6726\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.77977 to 0.46322, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 0.5553 - acc: 0.7821 - val_loss: 0.4632 - val_acc: 0.8143\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.46322 to 0.44051, saving model to /tmp/nn_model.h5\n",
      " - 41s - loss: 0.3007 - acc: 0.8904 - val_loss: 0.4405 - val_acc: 0.8290\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 41s - loss: 0.1966 - acc: 0.9293 - val_loss: 0.4897 - val_acc: 0.8251\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 40s - loss: 0.1380 - acc: 0.9519 - val_loss: 0.5446 - val_acc: 0.8162\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 40s - loss: 0.0996 - acc: 0.9652 - val_loss: 0.6125 - val_acc: 0.8156\n",
      "------------------\n",
      "Train on 14096 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.77743, saving model to /tmp/nn_model.h5\n",
      " - 42s - loss: 1.0009 - acc: 0.4949 - val_loss: 0.7774 - val_acc: 0.6618\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.77743 to 0.49838, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.5492 - acc: 0.7798 - val_loss: 0.4984 - val_acc: 0.8041\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.49838 to 0.46274, saving model to /tmp/nn_model.h5\n",
      " - 39s - loss: 0.3009 - acc: 0.8900 - val_loss: 0.4627 - val_acc: 0.8200\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 38s - loss: 0.1951 - acc: 0.9299 - val_loss: 0.4998 - val_acc: 0.8226\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 39s - loss: 0.1373 - acc: 0.9566 - val_loss: 0.5097 - val_acc: 0.8277\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 38s - loss: 0.1035 - acc: 0.9657 - val_loss: 0.6139 - val_acc: 0.8239\n",
      "------------------\n",
      "Train on 14097 samples, validate on 1567 samples\n",
      "Epoch 1/6\n",
      "Epoch 00001: val_loss improved from inf to 0.81915, saving model to /tmp/nn_model.h5\n",
      " - 42s - loss: 0.9944 - acc: 0.4833 - val_loss: 0.8192 - val_acc: 0.6318\n",
      "Epoch 2/6\n",
      "Epoch 00002: val_loss improved from 0.81915 to 0.54375, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.6525 - acc: 0.7129 - val_loss: 0.5437 - val_acc: 0.7779\n",
      "Epoch 3/6\n",
      "Epoch 00003: val_loss improved from 0.54375 to 0.44030, saving model to /tmp/nn_model.h5\n",
      " - 40s - loss: 0.3518 - acc: 0.8662 - val_loss: 0.4403 - val_acc: 0.8251\n",
      "Epoch 4/6\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 41s - loss: 0.2201 - acc: 0.9208 - val_loss: 0.4679 - val_acc: 0.8271\n",
      "Epoch 5/6\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 41s - loss: 0.1537 - acc: 0.9451 - val_loss: 0.5319 - val_acc: 0.8194\n",
      "Epoch 6/6\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 41s - loss: 0.1108 - acc: 0.9606 - val_loss: 0.6095 - val_acc: 0.8251\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=128, epochs=6, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07739, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0881 - acc: 0.3928 - val_loss: 1.0774 - val_acc: 0.4035\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07739 to 0.99465, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0473 - acc: 0.4470 - val_loss: 0.9946 - val_acc: 0.5482\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99465 to 0.79467, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8818 - acc: 0.6535 - val_loss: 0.7947 - val_acc: 0.6995\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79467 to 0.64289, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6579 - acc: 0.7761 - val_loss: 0.6429 - val_acc: 0.7529\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64289 to 0.55402, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5033 - acc: 0.8332 - val_loss: 0.5540 - val_acc: 0.7851\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55402 to 0.50130, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3986 - acc: 0.8703 - val_loss: 0.5013 - val_acc: 0.8040\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50130 to 0.46850, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3240 - acc: 0.8996 - val_loss: 0.4685 - val_acc: 0.8119\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46850 to 0.44607, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2688 - acc: 0.9154 - val_loss: 0.4461 - val_acc: 0.8202\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44607 to 0.43427, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2263 - acc: 0.9333 - val_loss: 0.4343 - val_acc: 0.8225\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43427 to 0.42630, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1922 - acc: 0.9437 - val_loss: 0.4263 - val_acc: 0.8295\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1635 - acc: 0.9529 - val_loss: 0.4276 - val_acc: 0.8285\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1420 - acc: 0.9607 - val_loss: 0.4280 - val_acc: 0.8289\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1236 - acc: 0.9673 - val_loss: 0.4293 - val_acc: 0.8325\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1092 - acc: 0.9704 - val_loss: 0.4345 - val_acc: 0.8327\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0954 - acc: 0.9756 - val_loss: 0.4400 - val_acc: 0.8315\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0842 - acc: 0.9775 - val_loss: 0.4518 - val_acc: 0.8293\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0760 - acc: 0.9808 - val_loss: 0.4615 - val_acc: 0.8287\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0661 - acc: 0.9826 - val_loss: 0.4723 - val_acc: 0.8300\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0591 - acc: 0.9859 - val_loss: 0.4844 - val_acc: 0.8278\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0530 - acc: 0.9865 - val_loss: 0.4940 - val_acc: 0.8276\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07494, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0866 - acc: 0.4001 - val_loss: 1.0749 - val_acc: 0.4033\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07494 to 0.98334, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0402 - acc: 0.4470 - val_loss: 0.9833 - val_acc: 0.5150\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98334 to 0.81530, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8829 - acc: 0.6240 - val_loss: 0.8153 - val_acc: 0.6548\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81530 to 0.67514, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6999 - acc: 0.7527 - val_loss: 0.6751 - val_acc: 0.7425\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67514 to 0.57708, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5460 - acc: 0.8242 - val_loss: 0.5771 - val_acc: 0.7844\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57708 to 0.51980, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4367 - acc: 0.8597 - val_loss: 0.5198 - val_acc: 0.8029\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51980 to 0.48392, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3585 - acc: 0.8874 - val_loss: 0.4839 - val_acc: 0.8106\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48392 to 0.46105, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2996 - acc: 0.9061 - val_loss: 0.4610 - val_acc: 0.8168\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46105 to 0.44595, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2523 - acc: 0.9239 - val_loss: 0.4460 - val_acc: 0.8223\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44595 to 0.43791, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2162 - acc: 0.9346 - val_loss: 0.4379 - val_acc: 0.8240\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43791 to 0.43369, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1860 - acc: 0.9449 - val_loss: 0.4337 - val_acc: 0.8263\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss improved from 0.43369 to 0.43097, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1611 - acc: 0.9518 - val_loss: 0.4310 - val_acc: 0.8289\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss improved from 0.43097 to 0.42942, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1400 - acc: 0.9620 - val_loss: 0.4294 - val_acc: 0.8291\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1220 - acc: 0.9672 - val_loss: 0.4356 - val_acc: 0.8306\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1071 - acc: 0.9713 - val_loss: 0.4444 - val_acc: 0.8302\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0926 - acc: 0.9756 - val_loss: 0.4463 - val_acc: 0.8336\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0830 - acc: 0.9792 - val_loss: 0.4541 - val_acc: 0.8336\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0734 - acc: 0.9823 - val_loss: 0.4673 - val_acc: 0.8321\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0650 - acc: 0.9850 - val_loss: 0.4732 - val_acc: 0.8344\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0579 - acc: 0.9868 - val_loss: 0.4861 - val_acc: 0.8332\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07425, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0856 - acc: 0.4037 - val_loss: 1.0743 - val_acc: 0.4103\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07425 to 0.98765, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0424 - acc: 0.4519 - val_loss: 0.9876 - val_acc: 0.5305\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98765 to 0.80707, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8840 - acc: 0.6090 - val_loss: 0.8071 - val_acc: 0.6829\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80707 to 0.64777, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6787 - acc: 0.7640 - val_loss: 0.6478 - val_acc: 0.7672\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64777 to 0.55485, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5161 - acc: 0.8353 - val_loss: 0.5548 - val_acc: 0.7931\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55485 to 0.50197, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4094 - acc: 0.8658 - val_loss: 0.5020 - val_acc: 0.8080\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50197 to 0.46930, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3353 - acc: 0.8939 - val_loss: 0.4693 - val_acc: 0.8138\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46930 to 0.45012, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2810 - acc: 0.9118 - val_loss: 0.4501 - val_acc: 0.8180\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45012 to 0.43663, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2319 - acc: 0.9293 - val_loss: 0.4366 - val_acc: 0.8225\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43663 to 0.43357, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1990 - acc: 0.9424 - val_loss: 0.4336 - val_acc: 0.8289\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43357 to 0.43098, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1715 - acc: 0.9508 - val_loss: 0.4310 - val_acc: 0.8295\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss improved from 0.43098 to 0.43067, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1483 - acc: 0.9587 - val_loss: 0.4307 - val_acc: 0.8342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1278 - acc: 0.9667 - val_loss: 0.4349 - val_acc: 0.8329\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1121 - acc: 0.9709 - val_loss: 0.4409 - val_acc: 0.8317\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0987 - acc: 0.9754 - val_loss: 0.4523 - val_acc: 0.8300\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0867 - acc: 0.9774 - val_loss: 0.4610 - val_acc: 0.8295\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0763 - acc: 0.9818 - val_loss: 0.4722 - val_acc: 0.8287\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0677 - acc: 0.9845 - val_loss: 0.4830 - val_acc: 0.8278\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0601 - acc: 0.9870 - val_loss: 0.4980 - val_acc: 0.8268\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0524 - acc: 0.9881 - val_loss: 0.5062 - val_acc: 0.8259\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07499, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0865 - acc: 0.4002 - val_loss: 1.0750 - val_acc: 0.4088\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07499 to 0.97456, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0395 - acc: 0.4544 - val_loss: 0.9746 - val_acc: 0.5356\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.97456 to 0.79533, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8712 - acc: 0.6276 - val_loss: 0.7953 - val_acc: 0.6614\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79533 to 0.64731, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6698 - acc: 0.7710 - val_loss: 0.6473 - val_acc: 0.7578\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64731 to 0.55672, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5119 - acc: 0.8357 - val_loss: 0.5567 - val_acc: 0.7876\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55672 to 0.50428, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4032 - acc: 0.8695 - val_loss: 0.5043 - val_acc: 0.8023\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50428 to 0.47299, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3291 - acc: 0.8935 - val_loss: 0.4730 - val_acc: 0.8080\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47299 to 0.45474, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2697 - acc: 0.9148 - val_loss: 0.4547 - val_acc: 0.8155\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45474 to 0.44064, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2272 - acc: 0.9348 - val_loss: 0.4406 - val_acc: 0.8195\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44064 to 0.43611, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1939 - acc: 0.9445 - val_loss: 0.4361 - val_acc: 0.8215\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43611 to 0.43296, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1639 - acc: 0.9559 - val_loss: 0.4330 - val_acc: 0.8232\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1430 - acc: 0.9625 - val_loss: 0.4330 - val_acc: 0.8268\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1232 - acc: 0.9672 - val_loss: 0.4402 - val_acc: 0.8257\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1090 - acc: 0.9717 - val_loss: 0.4446 - val_acc: 0.8272\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0940 - acc: 0.9775 - val_loss: 0.4539 - val_acc: 0.8274\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0853 - acc: 0.9787 - val_loss: 0.4622 - val_acc: 0.8268\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0736 - acc: 0.9820 - val_loss: 0.4703 - val_acc: 0.8255\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0659 - acc: 0.9844 - val_loss: 0.4846 - val_acc: 0.8242\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0577 - acc: 0.9869 - val_loss: 0.4990 - val_acc: 0.8249\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0524 - acc: 0.9881 - val_loss: 0.5075 - val_acc: 0.8219\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07609, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0849 - acc: 0.4055 - val_loss: 1.0761 - val_acc: 0.4006\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07609 to 0.98492, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0388 - acc: 0.4423 - val_loss: 0.9849 - val_acc: 0.5109\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98492 to 0.81646, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8785 - acc: 0.6375 - val_loss: 0.8165 - val_acc: 0.6672\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81646 to 0.67350, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6847 - acc: 0.7630 - val_loss: 0.6735 - val_acc: 0.7387\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67350 to 0.58190, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5333 - acc: 0.8274 - val_loss: 0.5819 - val_acc: 0.7745\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.58190 to 0.52588, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4276 - acc: 0.8595 - val_loss: 0.5259 - val_acc: 0.7930\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52588 to 0.49171, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3518 - acc: 0.8855 - val_loss: 0.4917 - val_acc: 0.8032\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.49171 to 0.47530, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2949 - acc: 0.9081 - val_loss: 0.4753 - val_acc: 0.8089\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.47530 to 0.45666, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2485 - acc: 0.9228 - val_loss: 0.4567 - val_acc: 0.8130\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.45666 to 0.45200, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2131 - acc: 0.9348 - val_loss: 0.4520 - val_acc: 0.8166\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.45200 to 0.44449, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1857 - acc: 0.9455 - val_loss: 0.4445 - val_acc: 0.8202\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1600 - acc: 0.9567 - val_loss: 0.4504 - val_acc: 0.8202\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1400 - acc: 0.9617 - val_loss: 0.4489 - val_acc: 0.8202\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1212 - acc: 0.9664 - val_loss: 0.4508 - val_acc: 0.8219\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1056 - acc: 0.9722 - val_loss: 0.4655 - val_acc: 0.8202\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0966 - acc: 0.9748 - val_loss: 0.4646 - val_acc: 0.8221\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0830 - acc: 0.9791 - val_loss: 0.4739 - val_acc: 0.8215\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0734 - acc: 0.9823 - val_loss: 0.4845 - val_acc: 0.8196\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0655 - acc: 0.9843 - val_loss: 0.4935 - val_acc: 0.8211\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0588 - acc: 0.9860 - val_loss: 0.5085 - val_acc: 0.8198\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07681, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0869 - acc: 0.3992 - val_loss: 1.0768 - val_acc: 0.4024\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07681 to 0.98257, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0427 - acc: 0.4496 - val_loss: 0.9826 - val_acc: 0.5418\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98257 to 0.79468, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8740 - acc: 0.6352 - val_loss: 0.7947 - val_acc: 0.6797\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79468 to 0.64514, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6700 - acc: 0.7748 - val_loss: 0.6451 - val_acc: 0.7546\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64514 to 0.55854, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5195 - acc: 0.8273 - val_loss: 0.5585 - val_acc: 0.7836\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.55854 to 0.50758, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4129 - acc: 0.8659 - val_loss: 0.5076 - val_acc: 0.8004\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: val_loss improved from 0.50758 to 0.47506, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3385 - acc: 0.8933 - val_loss: 0.4751 - val_acc: 0.8074\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47506 to 0.45422, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2809 - acc: 0.9129 - val_loss: 0.4542 - val_acc: 0.8159\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45422 to 0.43940, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2359 - acc: 0.9303 - val_loss: 0.4394 - val_acc: 0.8221\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43940 to 0.43599, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2010 - acc: 0.9412 - val_loss: 0.4360 - val_acc: 0.8261\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43599 to 0.43303, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1731 - acc: 0.9506 - val_loss: 0.4330 - val_acc: 0.8295\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1497 - acc: 0.9578 - val_loss: 0.4386 - val_acc: 0.8283\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1331 - acc: 0.9641 - val_loss: 0.4372 - val_acc: 0.8315\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1139 - acc: 0.9718 - val_loss: 0.4453 - val_acc: 0.8293\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1011 - acc: 0.9730 - val_loss: 0.4545 - val_acc: 0.8289\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0900 - acc: 0.9761 - val_loss: 0.4698 - val_acc: 0.8249\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0800 - acc: 0.9798 - val_loss: 0.4721 - val_acc: 0.8270\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0709 - acc: 0.9817 - val_loss: 0.4870 - val_acc: 0.8240\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0616 - acc: 0.9853 - val_loss: 0.5035 - val_acc: 0.8229\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0570 - acc: 0.9858 - val_loss: 0.5230 - val_acc: 0.8212\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07780, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0866 - acc: 0.4003 - val_loss: 1.0778 - val_acc: 0.4022\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07780 to 0.99507, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0469 - acc: 0.4354 - val_loss: 0.9951 - val_acc: 0.5099\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99507 to 0.81215, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8887 - acc: 0.6322 - val_loss: 0.8122 - val_acc: 0.6899\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81215 to 0.65991, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6800 - acc: 0.7744 - val_loss: 0.6599 - val_acc: 0.7580\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65991 to 0.56640, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5207 - acc: 0.8296 - val_loss: 0.5664 - val_acc: 0.7887\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56640 to 0.51281, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4155 - acc: 0.8648 - val_loss: 0.5128 - val_acc: 0.8014\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51281 to 0.47915, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3406 - acc: 0.8902 - val_loss: 0.4791 - val_acc: 0.8104\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47915 to 0.46028, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2846 - acc: 0.9085 - val_loss: 0.4603 - val_acc: 0.8166\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46028 to 0.44642, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2398 - acc: 0.9269 - val_loss: 0.4464 - val_acc: 0.8183\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44642 to 0.44123, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2051 - acc: 0.9398 - val_loss: 0.4412 - val_acc: 0.8215\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44123 to 0.43592, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1753 - acc: 0.9495 - val_loss: 0.4359 - val_acc: 0.8238\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1538 - acc: 0.9561 - val_loss: 0.4388 - val_acc: 0.8261\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1328 - acc: 0.9637 - val_loss: 0.4401 - val_acc: 0.8259\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1163 - acc: 0.9666 - val_loss: 0.4446 - val_acc: 0.8268\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1017 - acc: 0.9744 - val_loss: 0.4519 - val_acc: 0.8251\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0883 - acc: 0.9777 - val_loss: 0.4603 - val_acc: 0.8244\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0809 - acc: 0.9775 - val_loss: 0.4655 - val_acc: 0.8238\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0687 - acc: 0.9840 - val_loss: 0.4779 - val_acc: 0.8229\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0626 - acc: 0.9861 - val_loss: 0.4869 - val_acc: 0.8221\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0568 - acc: 0.9862 - val_loss: 0.4995 - val_acc: 0.8219\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07671, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0868 - acc: 0.4004 - val_loss: 1.0767 - val_acc: 0.4035\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07671 to 0.98501, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0421 - acc: 0.4483 - val_loss: 0.9850 - val_acc: 0.5376\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98501 to 0.80502, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8762 - acc: 0.6287 - val_loss: 0.8050 - val_acc: 0.6604\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80502 to 0.65892, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6767 - acc: 0.7703 - val_loss: 0.6589 - val_acc: 0.7510\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65892 to 0.56476, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5200 - acc: 0.8367 - val_loss: 0.5648 - val_acc: 0.7827\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56476 to 0.50753, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4104 - acc: 0.8682 - val_loss: 0.5075 - val_acc: 0.8053\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50753 to 0.47219, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3341 - acc: 0.8978 - val_loss: 0.4722 - val_acc: 0.8149\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47219 to 0.45274, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2779 - acc: 0.9125 - val_loss: 0.4527 - val_acc: 0.8206\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.45274 to 0.44043, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2334 - acc: 0.9300 - val_loss: 0.4404 - val_acc: 0.8263\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44043 to 0.43513, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1987 - acc: 0.9411 - val_loss: 0.4351 - val_acc: 0.8274\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43513 to 0.43177, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1715 - acc: 0.9525 - val_loss: 0.4318 - val_acc: 0.8308\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1463 - acc: 0.9610 - val_loss: 0.4325 - val_acc: 0.8323\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1282 - acc: 0.9651 - val_loss: 0.4357 - val_acc: 0.8327\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1105 - acc: 0.9703 - val_loss: 0.4386 - val_acc: 0.8336\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0969 - acc: 0.9750 - val_loss: 0.4495 - val_acc: 0.8334\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0857 - acc: 0.9794 - val_loss: 0.4578 - val_acc: 0.8329\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0755 - acc: 0.9821 - val_loss: 0.4681 - val_acc: 0.8321\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0668 - acc: 0.9843 - val_loss: 0.4767 - val_acc: 0.8300\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0590 - acc: 0.9857 - val_loss: 0.4884 - val_acc: 0.8319\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0543 - acc: 0.9875 - val_loss: 0.5019 - val_acc: 0.8300\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07375, saving model to /tmp/nn_model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 4s - loss: 1.0853 - acc: 0.4052 - val_loss: 1.0738 - val_acc: 0.4126\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07375 to 0.99611, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0467 - acc: 0.4424 - val_loss: 0.9961 - val_acc: 0.5099\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99611 to 0.81897, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9006 - acc: 0.6018 - val_loss: 0.8190 - val_acc: 0.6914\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81897 to 0.66220, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6957 - acc: 0.7592 - val_loss: 0.6622 - val_acc: 0.7674\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66220 to 0.56738, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5343 - acc: 0.8253 - val_loss: 0.5674 - val_acc: 0.7880\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56738 to 0.51313, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4251 - acc: 0.8628 - val_loss: 0.5131 - val_acc: 0.7978\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.51313 to 0.48126, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3448 - acc: 0.8892 - val_loss: 0.4813 - val_acc: 0.8104\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48126 to 0.46346, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2874 - acc: 0.9109 - val_loss: 0.4635 - val_acc: 0.8187\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46346 to 0.44697, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2414 - acc: 0.9265 - val_loss: 0.4470 - val_acc: 0.8219\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44697 to 0.44578, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2058 - acc: 0.9376 - val_loss: 0.4458 - val_acc: 0.8232\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44578 to 0.43712, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1768 - acc: 0.9495 - val_loss: 0.4371 - val_acc: 0.8242\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1523 - acc: 0.9587 - val_loss: 0.4451 - val_acc: 0.8244\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1345 - acc: 0.9642 - val_loss: 0.4414 - val_acc: 0.8263\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1172 - acc: 0.9687 - val_loss: 0.4497 - val_acc: 0.8238\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1031 - acc: 0.9738 - val_loss: 0.4574 - val_acc: 0.8200\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0914 - acc: 0.9774 - val_loss: 0.4567 - val_acc: 0.8246\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0795 - acc: 0.9815 - val_loss: 0.4685 - val_acc: 0.8229\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0708 - acc: 0.9833 - val_loss: 0.4718 - val_acc: 0.8244\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0620 - acc: 0.9855 - val_loss: 0.4891 - val_acc: 0.8221\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0555 - acc: 0.9877 - val_loss: 0.4957 - val_acc: 0.8221\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07620, saving model to /tmp/nn_model.h5\n",
      " - 4s - loss: 1.0855 - acc: 0.4029 - val_loss: 1.0762 - val_acc: 0.4045\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07620 to 0.98869, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0442 - acc: 0.4435 - val_loss: 0.9887 - val_acc: 0.5085\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98869 to 0.80550, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8849 - acc: 0.6257 - val_loss: 0.8055 - val_acc: 0.6666\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.80550 to 0.65397, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6817 - acc: 0.7666 - val_loss: 0.6540 - val_acc: 0.7453\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.65397 to 0.56387, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5276 - acc: 0.8274 - val_loss: 0.5639 - val_acc: 0.7762\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.56387 to 0.50716, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4173 - acc: 0.8646 - val_loss: 0.5072 - val_acc: 0.8019\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.50716 to 0.47106, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3392 - acc: 0.8915 - val_loss: 0.4711 - val_acc: 0.8111\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.47106 to 0.44904, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2814 - acc: 0.9144 - val_loss: 0.4490 - val_acc: 0.8181\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44904 to 0.43426, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2365 - acc: 0.9261 - val_loss: 0.4343 - val_acc: 0.8236\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43426 to 0.42739, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2007 - acc: 0.9383 - val_loss: 0.4274 - val_acc: 0.8249\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42739 to 0.42262, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1721 - acc: 0.9491 - val_loss: 0.4226 - val_acc: 0.8289\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1497 - acc: 0.9570 - val_loss: 0.4246 - val_acc: 0.8281\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1299 - acc: 0.9637 - val_loss: 0.4250 - val_acc: 0.8313\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1141 - acc: 0.9684 - val_loss: 0.4293 - val_acc: 0.8336\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0974 - acc: 0.9746 - val_loss: 0.4386 - val_acc: 0.8321\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0889 - acc: 0.9769 - val_loss: 0.4418 - val_acc: 0.8330\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0785 - acc: 0.9806 - val_loss: 0.4477 - val_acc: 0.8323\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0690 - acc: 0.9839 - val_loss: 0.4590 - val_acc: 0.8300\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0613 - acc: 0.9840 - val_loss: 0.4703 - val_acc: 0.8317\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0543 - acc: 0.9867 - val_loss: 0.4850 - val_acc: 0.8302\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07686, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0865 - acc: 0.4012 - val_loss: 1.0769 - val_acc: 0.4046\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07686 to 0.97885, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0424 - acc: 0.4500 - val_loss: 0.9788 - val_acc: 0.5173\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.97885 to 0.79047, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8728 - acc: 0.6314 - val_loss: 0.7905 - val_acc: 0.6929\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79047 to 0.63889, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6699 - acc: 0.7691 - val_loss: 0.6389 - val_acc: 0.7610\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.63889 to 0.54802, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5171 - acc: 0.8324 - val_loss: 0.5480 - val_acc: 0.7880\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54802 to 0.49421, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4127 - acc: 0.8682 - val_loss: 0.4942 - val_acc: 0.8072\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49421 to 0.46495, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3388 - acc: 0.8922 - val_loss: 0.4650 - val_acc: 0.8142\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46495 to 0.44449, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2827 - acc: 0.9129 - val_loss: 0.4445 - val_acc: 0.8206\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44449 to 0.43035, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2357 - acc: 0.9302 - val_loss: 0.4303 - val_acc: 0.8274\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43035 to 0.42556, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2029 - acc: 0.9416 - val_loss: 0.4256 - val_acc: 0.8298\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42556 to 0.42301, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1747 - acc: 0.9509 - val_loss: 0.4230 - val_acc: 0.8321\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1508 - acc: 0.9598 - val_loss: 0.4255 - val_acc: 0.8325\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1331 - acc: 0.9666 - val_loss: 0.4265 - val_acc: 0.8325\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1150 - acc: 0.9699 - val_loss: 0.4333 - val_acc: 0.8317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1020 - acc: 0.9744 - val_loss: 0.4379 - val_acc: 0.8300\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0906 - acc: 0.9786 - val_loss: 0.4479 - val_acc: 0.8306\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0805 - acc: 0.9806 - val_loss: 0.4575 - val_acc: 0.8298\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0702 - acc: 0.9844 - val_loss: 0.4644 - val_acc: 0.8323\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0630 - acc: 0.9866 - val_loss: 0.4792 - val_acc: 0.8287\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0553 - acc: 0.9869 - val_loss: 0.4943 - val_acc: 0.8289\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07575, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0850 - acc: 0.4062 - val_loss: 1.0758 - val_acc: 0.4046\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07575 to 0.99016, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0435 - acc: 0.4439 - val_loss: 0.9902 - val_acc: 0.5408\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99016 to 0.81517, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8868 - acc: 0.6232 - val_loss: 0.8152 - val_acc: 0.6697\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81517 to 0.66873, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6886 - acc: 0.7586 - val_loss: 0.6687 - val_acc: 0.7474\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66873 to 0.57160, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5316 - acc: 0.8257 - val_loss: 0.5716 - val_acc: 0.7800\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57160 to 0.52137, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4202 - acc: 0.8646 - val_loss: 0.5214 - val_acc: 0.7912\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52137 to 0.48339, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3423 - acc: 0.8889 - val_loss: 0.4834 - val_acc: 0.8055\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48339 to 0.46288, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2824 - acc: 0.9127 - val_loss: 0.4629 - val_acc: 0.8112\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46288 to 0.44629, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2374 - acc: 0.9294 - val_loss: 0.4463 - val_acc: 0.8180\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44629 to 0.43866, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2036 - acc: 0.9404 - val_loss: 0.4387 - val_acc: 0.8202\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.43866 to 0.43425, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1734 - acc: 0.9503 - val_loss: 0.4343 - val_acc: 0.8232\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1508 - acc: 0.9597 - val_loss: 0.4378 - val_acc: 0.8240\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1304 - acc: 0.9658 - val_loss: 0.4356 - val_acc: 0.8285\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1143 - acc: 0.9702 - val_loss: 0.4425 - val_acc: 0.8266\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0981 - acc: 0.9757 - val_loss: 0.4533 - val_acc: 0.8246\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0854 - acc: 0.9803 - val_loss: 0.4634 - val_acc: 0.8238\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0769 - acc: 0.9819 - val_loss: 0.4721 - val_acc: 0.8246\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0686 - acc: 0.9854 - val_loss: 0.4918 - val_acc: 0.8208\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0608 - acc: 0.9854 - val_loss: 0.5078 - val_acc: 0.8170\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0543 - acc: 0.9869 - val_loss: 0.5111 - val_acc: 0.8189\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07653, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0861 - acc: 0.4013 - val_loss: 1.0765 - val_acc: 0.4067\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07653 to 0.98705, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0439 - acc: 0.4474 - val_loss: 0.9871 - val_acc: 0.5225\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98705 to 0.79858, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8771 - acc: 0.6338 - val_loss: 0.7986 - val_acc: 0.6808\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.79858 to 0.64343, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6701 - acc: 0.7723 - val_loss: 0.6434 - val_acc: 0.7593\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.64343 to 0.54966, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5105 - acc: 0.8337 - val_loss: 0.5497 - val_acc: 0.7951\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.54966 to 0.49772, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4042 - acc: 0.8688 - val_loss: 0.4977 - val_acc: 0.8085\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.49772 to 0.46510, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3291 - acc: 0.8961 - val_loss: 0.4651 - val_acc: 0.8153\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.46510 to 0.44466, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2768 - acc: 0.9120 - val_loss: 0.4447 - val_acc: 0.8212\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.44466 to 0.43077, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2311 - acc: 0.9285 - val_loss: 0.4308 - val_acc: 0.8240\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.43077 to 0.42494, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1991 - acc: 0.9406 - val_loss: 0.4249 - val_acc: 0.8289\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.42494 to 0.42277, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1704 - acc: 0.9513 - val_loss: 0.4228 - val_acc: 0.8274\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1465 - acc: 0.9582 - val_loss: 0.4250 - val_acc: 0.8293\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1276 - acc: 0.9657 - val_loss: 0.4263 - val_acc: 0.8287\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1119 - acc: 0.9698 - val_loss: 0.4315 - val_acc: 0.8304\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0979 - acc: 0.9743 - val_loss: 0.4406 - val_acc: 0.8304\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0874 - acc: 0.9764 - val_loss: 0.4450 - val_acc: 0.8310\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0767 - acc: 0.9807 - val_loss: 0.4556 - val_acc: 0.8300\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0678 - acc: 0.9843 - val_loss: 0.4649 - val_acc: 0.8302\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0597 - acc: 0.9858 - val_loss: 0.4758 - val_acc: 0.8293\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0535 - acc: 0.9869 - val_loss: 0.4876 - val_acc: 0.8270\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4699 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07631, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0867 - acc: 0.3990 - val_loss: 1.0763 - val_acc: 0.4020\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07631 to 0.99405, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0453 - acc: 0.4421 - val_loss: 0.9941 - val_acc: 0.5308\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.99405 to 0.82562, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8939 - acc: 0.6094 - val_loss: 0.8256 - val_acc: 0.6631\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.82562 to 0.67338, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6951 - acc: 0.7602 - val_loss: 0.6734 - val_acc: 0.7489\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.67338 to 0.57817, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5367 - acc: 0.8273 - val_loss: 0.5782 - val_acc: 0.7789\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57817 to 0.52287, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4312 - acc: 0.8584 - val_loss: 0.5229 - val_acc: 0.7976\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52287 to 0.49024, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3524 - acc: 0.8902 - val_loss: 0.4902 - val_acc: 0.8055\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.49024 to 0.47006, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2948 - acc: 0.9082 - val_loss: 0.4701 - val_acc: 0.8119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.47006 to 0.44913, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2508 - acc: 0.9226 - val_loss: 0.4491 - val_acc: 0.8215\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.44913 to 0.44596, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2152 - acc: 0.9372 - val_loss: 0.4460 - val_acc: 0.8238\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44596 to 0.44069, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1853 - acc: 0.9448 - val_loss: 0.4407 - val_acc: 0.8274\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1601 - acc: 0.9558 - val_loss: 0.4417 - val_acc: 0.8291\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss improved from 0.44069 to 0.43350, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1388 - acc: 0.9619 - val_loss: 0.4335 - val_acc: 0.8315\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1206 - acc: 0.9673 - val_loss: 0.4426 - val_acc: 0.8291\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.1074 - acc: 0.9713 - val_loss: 0.4471 - val_acc: 0.8302\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0941 - acc: 0.9752 - val_loss: 0.4531 - val_acc: 0.8310\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0842 - acc: 0.9780 - val_loss: 0.4605 - val_acc: 0.8289\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0743 - acc: 0.9819 - val_loss: 0.4712 - val_acc: 0.8283\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0652 - acc: 0.9844 - val_loss: 0.4851 - val_acc: 0.8283\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0589 - acc: 0.9850 - val_loss: 0.4908 - val_acc: 0.8266\n",
      "------------------\n",
      "Train on 10964 samples, validate on 4700 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.07477, saving model to /tmp/nn_model.h5\n",
      " - 5s - loss: 1.0859 - acc: 0.4006 - val_loss: 1.0748 - val_acc: 0.4074\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.07477 to 0.98646, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 1.0421 - acc: 0.4490 - val_loss: 0.9865 - val_acc: 0.5360\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.98646 to 0.81356, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.8814 - acc: 0.6301 - val_loss: 0.8136 - val_acc: 0.6472\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.81356 to 0.66556, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.6818 - acc: 0.7656 - val_loss: 0.6656 - val_acc: 0.7404\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.66556 to 0.57368, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5233 - acc: 0.8328 - val_loss: 0.5737 - val_acc: 0.7753\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.57368 to 0.52054, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.4172 - acc: 0.8630 - val_loss: 0.5205 - val_acc: 0.7953\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.52054 to 0.48752, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3397 - acc: 0.8917 - val_loss: 0.4875 - val_acc: 0.8049\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.48752 to 0.46890, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2819 - acc: 0.9126 - val_loss: 0.4689 - val_acc: 0.8121\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.46890 to 0.45293, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2391 - acc: 0.9270 - val_loss: 0.4529 - val_acc: 0.8168\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.45293 to 0.44494, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2010 - acc: 0.9406 - val_loss: 0.4449 - val_acc: 0.8200\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss improved from 0.44494 to 0.44045, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1744 - acc: 0.9512 - val_loss: 0.4405 - val_acc: 0.8247\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1498 - acc: 0.9580 - val_loss: 0.4476 - val_acc: 0.8230\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.1304 - acc: 0.9639 - val_loss: 0.4438 - val_acc: 0.8268\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.1127 - acc: 0.9703 - val_loss: 0.4507 - val_acc: 0.8230\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0995 - acc: 0.9745 - val_loss: 0.4586 - val_acc: 0.8251\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0882 - acc: 0.9767 - val_loss: 0.4673 - val_acc: 0.8272\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0802 - acc: 0.9797 - val_loss: 0.4768 - val_acc: 0.8234\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0697 - acc: 0.9829 - val_loss: 0.4811 - val_acc: 0.8257\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0629 - acc: 0.9845 - val_loss: 0.4887 - val_acc: 0.8268\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0552 - acc: 0.9870 - val_loss: 0.4996 - val_acc: 0.8249\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.3,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        best_val_test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(4)\n",
    "nn_train3,nn_test3,nn_train4,nn_test4 = get_nn_feats(5)\n",
    "nn_train5,nn_test5,nn_train6,nn_test6 = get_nn_feats(6)\n",
    "\n",
    "\n",
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                          cnn_train1, cnn_train2,cnn_train3, cnn_train4,cnn_train5, cnn_train6,\n",
    "                          nn_train1,nn_train2,nn_train3,nn_train4,nn_train5,nn_train6\n",
    "                         ])\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                         cnn_test1, cnn_test2,cnn_test3, cnn_test4,cnn_test5, cnn_test6,\n",
    "                         nn_test1,nn_test2,nn_test3,nn_test4,nn_test5,nn_test6\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 762) (8392, 762)\n"
     ]
    }
   ],
   "source": [
    "# combine feats\n",
    "cols_to_drop = ['id','text','tag_txt','pos_txt','dep_txt']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "train_X = np.hstack([train_X, all_svd_train, all_nlp_train])\n",
    "test_X = np.hstack([test_X, all_svd_test, all_nlp_test])\n",
    "\n",
    "f_train_X = np.hstack([train_X, help_train_feat,help_train_feat2,help_train_feat3,all_nn_train])\n",
    "#f_train_X = np.round(f_train_X,4)\n",
    "f_test_X = np.hstack([test_X, help_test_feat,help_test_feat2,help_test_feat3,all_nn_test])\n",
    "#f_test_X = np.round(f_test_X,4)\n",
    "print(f_train_X.shape, f_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump for xgb\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('feat.pkl','wb') as fout:\n",
    "    pickle.dump([f_train_X,f_test_X],fout)\n",
    "print('dump for xgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = np.round(best_single_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.055\tvalid-mlogloss:1.05602\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.22196\tvalid-mlogloss:0.280404\n",
      "[400]\ttrain-mlogloss:0.179357\tvalid-mlogloss:0.270178\n",
      "Stopping. Best iteration:\n",
      "[527]\ttrain-mlogloss:0.160523\tvalid-mlogloss:0.268975\n",
      "\n",
      "train log loss 0.153875230434 valid log loss 0.269190198116\n",
      "rev 3.71484551442\n",
      "[0]\ttrain-mlogloss:1.05507\tvalid-mlogloss:1.05581\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 50 rounds.\n",
      "[200]\ttrain-mlogloss:0.22299\tvalid-mlogloss:0.272222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3e9af5a179d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-4919720dd770>\u001b[0m in \u001b[0;36mcv_test\u001b[0;34m(k_cnt, s_flag)\u001b[0m\n\u001b[1;32m     35\u001b[0m         m = xgb.train(params, d_train, 2000, watchlist, \n\u001b[1;32m     36\u001b[0m                         \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                         verbose_eval=200)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# get res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 896\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_test(10, True)\n",
    "# 276xx not good"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
