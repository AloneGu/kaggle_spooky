{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the text ##\n",
    "train_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the text ##\n",
    "train_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.658537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.481481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0         41                35        231             23                 7   \n",
       "1         14                14         71             10                 1   \n",
       "2         36                32        200             16                 5   \n",
       "3         34                32        206             14                 4   \n",
       "4         27                25        174             13                 4   \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0                2                3       4.658537  \n",
       "1                0                1       4.142857  \n",
       "2                0                1       4.583333  \n",
       "3                0                4       5.088235  \n",
       "4                0                2       5.481481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 30) (8392, 30)\n",
      "(19579, 885129) (8392, 885129)\n",
      "(19579, 1354551) (8392, 1354551)\n",
      "(19579, 8) (8392, 8)\n",
      "(19579, 68) (8392, 68)\n",
      "(19579, 12) (8392, 12)\n",
      "[[ 0.844  0.024  0.131  1.     0.     0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.401  0.355  0.244  0.671  0.296  0.033  0.726  0.247  0.027  0.978\n",
      "   0.022  0.   ]\n",
      " [ 0.758  0.23   0.012  0.997  0.003  0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.009  0.034  0.957  0.     0.     1.     0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.291  0.404  0.305  0.947  0.039  0.014  0.336  0.125  0.539  1.     0.\n",
      "   0.   ]]\n",
      "[[ 0.158  0.06   0.781  0.     0.     1.     0.     0.     1.     0.     0.\n",
      "   1.   ]\n",
      " [ 0.765  0.167  0.068  0.999  0.     0.     1.     0.     0.     1.     0.\n",
      "   0.   ]\n",
      " [ 0.017  0.956  0.027  0.002  0.998  0.     0.     1.     0.     0.     1.\n",
      "   0.   ]\n",
      " [ 0.471  0.5    0.029  0.173  0.827  0.     0.689  0.311  0.     0.2    0.8\n",
      "   0.   ]\n",
      " [ 0.54   0.299  0.161  0.734  0.036  0.23   0.82   0.179  0.002  0.992  0.\n",
      "   0.008]]\n"
     ]
    }
   ],
   "source": [
    "## Prepare the data for modeling ###\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_id = train_df['id'].values\n",
    "test_id = test_df['id'].values\n",
    "\n",
    "## add tfidf and svd\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,3), max_df=0.8,lowercase=False, sublinear_tf=True)\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['text'].values.tolist())\n",
    "\n",
    "print(train_tfidf.shape,test_tfidf.shape)\n",
    "\n",
    "# svd1\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "print(train_svd.shape,test_svd.shape)\n",
    "\n",
    "## add tfidf char\n",
    "tfidf_vec2 = TfidfVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8, sublinear_tf=True)\n",
    "full_tfidf2 = tfidf_vec2.fit_transform(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_tfidf2 = tfidf_vec2.transform(train_df['text'].values.tolist())\n",
    "test_tfidf2 = tfidf_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_tfidf2.shape,test_tfidf2.shape)\n",
    "\n",
    "## add svd2\n",
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf2)\n",
    "train_svd2 = pd.DataFrame(svd_obj.transform(train_tfidf2))\n",
    "test_svd2 = pd.DataFrame(svd_obj.transform(test_tfidf2))\n",
    "print(train_svd2.shape,test_svd2.shape)\n",
    "\n",
    "\n",
    "## add cnt vec\n",
    "c_vec = CountVectorizer(ngram_range=(1,3),max_df=0.8, lowercase=False)\n",
    "c_vec.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec = c_vec.transform(train_df['text'].values.tolist())\n",
    "test_cvec = c_vec.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec.shape,test_cvec.shape)\n",
    "\n",
    "# add cnt char\n",
    "c_vec2 = CountVectorizer(ngram_range=(3,7), analyzer='char',max_df=0.8)\n",
    "c_vec2.fit(train_df['text'].values.tolist() + test_df['text'].values.tolist())\n",
    "train_cvec2 = c_vec2.transform(train_df['text'].values.tolist())\n",
    "test_cvec2 = c_vec2.transform(test_df['text'].values.tolist())\n",
    "print(train_cvec2.shape,test_cvec2.shape)\n",
    "\n",
    "# add tfidf to svd\n",
    "cols_to_drop = ['id', 'text']\n",
    "train_X = train_df.drop(cols_to_drop+['author'], axis=1).values\n",
    "test_X = test_df.drop(cols_to_drop, axis=1).values\n",
    "print(train_X.shape, test_X.shape)\n",
    "train_X = np.hstack([train_X,train_svd,train_svd2])\n",
    "test_X = np.hstack([test_X,test_svd,test_svd2])\n",
    "print(train_X.shape, test_X.shape)\n",
    "\n",
    "# add naive feature\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "feat_cnt = 5\n",
    "train_Y = train_y\n",
    "\n",
    "help_tfidf_train,help_tfidf_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_tfidf_train2,help_tfidf_test2 = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt1_train,help_cnt1_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "help_cnt2_train,help_cnt2_test = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "\n",
    "kf = KFold(n_splits=feat_cnt, shuffle=True, random_state=23)\n",
    "for train_index, test_index in kf.split(train_tfidf):\n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf[train_index], train_tfidf[test_index]\n",
    "    y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "    tmp_model = MultinomialNB(alpha=0.02,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf)\n",
    "    help_tfidf_train[test_index] = tmp_train_feat\n",
    "    help_tfidf_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # tfidf to nb\n",
    "    X_train, X_test = train_tfidf2[train_index], train_tfidf2[test_index]\n",
    "    tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_tfidf2)\n",
    "    help_tfidf_train2[test_index] = tmp_train_feat\n",
    "    help_tfidf_test2 += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec to nb\n",
    "    X_train, X_test = train_cvec[train_index], train_cvec[test_index]\n",
    "    tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec)\n",
    "    help_cnt1_train[test_index] = tmp_train_feat\n",
    "    help_cnt1_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "    # count vec2 to nb \n",
    "    X_train, X_test = train_cvec2[train_index], train_cvec2[test_index]\n",
    "    tmp_model = MultinomialNB(0.02,fit_prior=False)\n",
    "    tmp_model.fit(X_train,y_train)\n",
    "    tmp_train_feat = tmp_model.predict_proba(X_test)\n",
    "    tmp_test_feat = tmp_model.predict_proba(test_cvec2)\n",
    "    help_cnt2_train[test_index] = tmp_train_feat\n",
    "    help_cnt2_test += tmp_test_feat/feat_cnt\n",
    "    \n",
    "help_train_feat = np.round(np.hstack([help_tfidf_train,help_tfidf_train2,help_cnt1_train,help_cnt2_train]),3)\n",
    "help_test_feat = np.round(np.hstack([help_tfidf_test,help_tfidf_test2,help_cnt1_test,help_cnt2_test]),3)\n",
    "\n",
    "print(help_train_feat.shape,help_test_feat.shape)\n",
    "print(help_train_feat[:5])\n",
    "print(help_test_feat[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Epoch 00001: val_loss improved from inf to 1.06302, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.06302 to 0.97965, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.97965 to 0.88026, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.88026 to 0.79915, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.79915 to 0.75300, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.75300 to 0.73080, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.73080 to 0.71119, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.71119 to 0.70166, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.70166 to 0.69786, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 1.06939, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.06939 to 0.96834, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.96834 to 0.79978, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.79978 to 0.63123, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.63123 to 0.52860, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.52860 to 0.47470, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.47470 to 0.44594, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.44594 to 0.43132, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.43132 to 0.42692, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss improved from 0.42692 to 0.42088, saving model to /tmp/nn_model.h5\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 1.06635, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.06635 to 0.98484, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.98484 to 0.86653, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.86653 to 0.72391, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.72391 to 0.60301, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.60301 to 0.52686, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.52686 to 0.48501, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.48501 to 0.45878, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.45878 to 0.44489, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss improved from 0.44489 to 0.43641, saving model to /tmp/nn_model.h5\n",
      "Epoch 00011: val_loss improved from 0.43641 to 0.43335, saving model to /tmp/nn_model.h5\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 1.07228, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.07228 to 0.97308, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.97308 to 0.79120, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.79120 to 0.61771, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.61771 to 0.51668, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.51668 to 0.46219, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.46219 to 0.43080, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.43080 to 0.41475, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.41475 to 0.40386, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 1.06652, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 1.06652 to 0.96450, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss improved from 0.96450 to 0.80460, saving model to /tmp/nn_model.h5\n",
      "Epoch 00004: val_loss improved from 0.80460 to 0.65430, saving model to /tmp/nn_model.h5\n",
      "Epoch 00005: val_loss improved from 0.65430 to 0.55061, saving model to /tmp/nn_model.h5\n",
      "Epoch 00006: val_loss improved from 0.55061 to 0.48402, saving model to /tmp/nn_model.h5\n",
      "Epoch 00007: val_loss improved from 0.48402 to 0.44818, saving model to /tmp/nn_model.h5\n",
      "Epoch 00008: val_loss improved from 0.44818 to 0.42524, saving model to /tmp/nn_model.h5\n",
      "Epoch 00009: val_loss improved from 0.42524 to 0.41432, saving model to /tmp/nn_model.h5\n",
      "Epoch 00010: val_loss improved from 0.41432 to 0.41081, saving model to /tmp/nn_model.h5\n",
      "Epoch 00011: val_loss improved from 0.41081 to 0.41064, saving model to /tmp/nn_model.h5\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 00020: val_loss did not improve\n",
      "------------------\n",
      "[[ 0.998  0.001  0.   ]\n",
      " [ 0.577  0.384  0.039]\n",
      " [ 0.891  0.108  0.001]\n",
      " [ 0.     0.     1.   ]\n",
      " [ 0.909  0.074  0.018]]\n",
      "[[ 0.076  0.055  0.87 ]\n",
      " [ 0.994  0.006  0.   ]\n",
      " [ 0.14   0.847  0.013]\n",
      " [ 0.745  0.247  0.007]\n",
      " [ 0.824  0.109  0.067]]\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=20, \n",
    "                  verbose=0,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "\n",
    "        model = load_model(MODEL_P)\n",
    "        \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "cnn_train,cnn_test = get_cnn_feats()\n",
    "cnn_train = np.round(cnn_train,3)\n",
    "cnn_test = np.round(cnn_test,3)\n",
    "print(cnn_train[:5])\n",
    "print(cnn_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Epoch 00001: val_loss improved from inf to 0.51247, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.51247 to 0.45328, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 0.49967, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.49967 to 0.43847, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 0.52920, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.52920 to 0.45662, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 0.52944, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.52944 to 0.45607, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "------------------\n",
      "Epoch 00001: val_loss improved from inf to 0.50960, saving model to /tmp/nn_model.h5\n",
      "Epoch 00002: val_loss improved from 0.50960 to 0.46567, saving model to /tmp/nn_model.h5\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 00004: val_loss did not improve\n",
      "------------------\n",
      "[[ 0.997  0.003  0.   ]\n",
      " [ 0.801  0.156  0.043]\n",
      " [ 0.987  0.012  0.   ]\n",
      " [ 0.     0.     1.   ]\n",
      " [ 0.896  0.1    0.004]]\n",
      "[[ 0.128  0.022  0.85 ]\n",
      " [ 0.997  0.003  0.   ]\n",
      " [ 0.177  0.82   0.003]\n",
      " [ 0.357  0.637  0.005]\n",
      " [ 0.862  0.065  0.072]]\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 5\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = train_df['text']\n",
    "    tmp_Y = train_df['author']\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "\n",
    "    ttrain_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    ttrain_x = pad_sequences(ttrain_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    ttest_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    ttest_x = pad_sequences(ttest_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(tmp_Y)\n",
    "\n",
    "    ttrain_y = lb.transform(tmp_Y)\n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333)\n",
    "    for train_index, test_index in kf.split(train_tfidf):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        np.random.seed(42) # for model train\n",
    "        model.fit(ttrain_x[train_index], ttrain_y[train_index], \n",
    "                  validation_split=0.1,\n",
    "                  batch_size=64, epochs=4, \n",
    "                  verbose=0,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    "\n",
    "        model = load_model(MODEL_P)\n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(ttrain_x[test_index])\n",
    "        test_pred += model.predict(ttest_x)/feat_cnt\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train,lstm_test = get_lstm_feats()\n",
    "lstm_train = np.round(lstm_train,3)\n",
    "lstm_test = np.round(lstm_test,3)\n",
    "print(lstm_train[:5])\n",
    "print(lstm_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 86) (8392, 86)\n",
      "def done\n"
     ]
    }
   ],
   "source": [
    "f_train_X = np.hstack([train_X,help_train_feat,cnn_train,lstm_train])\n",
    "f_test_X = np.hstack([test_X,help_test_feat,cnn_test,lstm_test])\n",
    "# f_train_X = np.hstack([train_X,help_train_feat,cnn_train])\n",
    "# f_test_X = np.hstack([test_X,help_test_feat,cnn_test])\n",
    "print(f_train_X.shape, f_test_X.shape)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 42\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.02,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':3,\n",
    "                'gamma':0.2\n",
    "                }\n",
    "        \n",
    "        # def mat\n",
    "        d_train = xgb.DMatrix(X_train, y_train)\n",
    "        d_valid = xgb.DMatrix(X_test, y_test)\n",
    "        d_test = xgb.DMatrix(f_test_X)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        # train model\n",
    "        m = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=100,\n",
    "                        verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(d_train)\n",
    "        valid_pred = m.predict(d_valid)\n",
    "        tmp_train_pred = m.predict(xgb.DMatrix(f_train_X))\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(d_test)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(d_test)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    test_pred = np.round(test_pred,4)\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_xgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07685\tvalid-mlogloss:1.07728\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.283227\tvalid-mlogloss:0.319551\n",
      "[400]\ttrain-mlogloss:0.245683\tvalid-mlogloss:0.300558\n",
      "[600]\ttrain-mlogloss:0.22394\tvalid-mlogloss:0.295192\n",
      "[800]\ttrain-mlogloss:0.206842\tvalid-mlogloss:0.29351\n",
      "[1000]\ttrain-mlogloss:0.192262\tvalid-mlogloss:0.293001\n",
      "[1200]\ttrain-mlogloss:0.179252\tvalid-mlogloss:0.292589\n",
      "Stopping. Best iteration:\n",
      "[1198]\ttrain-mlogloss:0.179383\tvalid-mlogloss:0.292543\n",
      "\n",
      "train log loss 0.173414773729 valid log loss 0.292948070175\n",
      "rev 3.41357428777\n",
      "[0]\ttrain-mlogloss:1.07695\tvalid-mlogloss:1.07701\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.287212\tvalid-mlogloss:0.303931\n",
      "[400]\ttrain-mlogloss:0.249269\tvalid-mlogloss:0.284555\n",
      "[600]\ttrain-mlogloss:0.227126\tvalid-mlogloss:0.280192\n",
      "[800]\ttrain-mlogloss:0.210022\tvalid-mlogloss:0.278736\n",
      "Stopping. Best iteration:\n",
      "[894]\ttrain-mlogloss:0.202862\tvalid-mlogloss:0.27848\n",
      "\n",
      "train log loss 0.195776391019 valid log loss 0.278842727787\n",
      "rev 3.58625095923\n",
      "[0]\ttrain-mlogloss:1.07713\tvalid-mlogloss:1.07684\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.293402\tvalid-mlogloss:0.28384\n",
      "[400]\ttrain-mlogloss:0.255302\tvalid-mlogloss:0.261387\n",
      "[600]\ttrain-mlogloss:0.233515\tvalid-mlogloss:0.255336\n",
      "[800]\ttrain-mlogloss:0.216424\tvalid-mlogloss:0.25286\n",
      "[1000]\ttrain-mlogloss:0.201341\tvalid-mlogloss:0.25174\n",
      "[1200]\ttrain-mlogloss:0.18806\tvalid-mlogloss:0.251291\n",
      "Stopping. Best iteration:\n",
      "[1140]\ttrain-mlogloss:0.191807\tvalid-mlogloss:0.251245\n",
      "\n",
      "train log loss 0.185650598291 valid log loss 0.25134089181\n",
      "rev 3.97866018855\n",
      "[0]\ttrain-mlogloss:1.07697\tvalid-mlogloss:1.07715\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.288006\tvalid-mlogloss:0.303622\n",
      "[400]\ttrain-mlogloss:0.250111\tvalid-mlogloss:0.281623\n",
      "[600]\ttrain-mlogloss:0.22833\tvalid-mlogloss:0.277315\n",
      "[800]\ttrain-mlogloss:0.21104\tvalid-mlogloss:0.275688\n",
      "[1000]\ttrain-mlogloss:0.196239\tvalid-mlogloss:0.275191\n",
      "[1200]\ttrain-mlogloss:0.183301\tvalid-mlogloss:0.274979\n",
      "[1400]\ttrain-mlogloss:0.171428\tvalid-mlogloss:0.274825\n",
      "Stopping. Best iteration:\n",
      "[1301]\ttrain-mlogloss:0.177116\tvalid-mlogloss:0.274674\n",
      "\n",
      "train log loss 0.171360303541 valid log loss 0.274838263701\n",
      "rev 3.63850355673\n",
      "[0]\ttrain-mlogloss:1.077\tvalid-mlogloss:1.07722\n",
      "Multiple eval metrics have been passed: 'valid-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-mlogloss hasn't improved in 100 rounds.\n",
      "[200]\ttrain-mlogloss:0.28691\tvalid-mlogloss:0.306294\n",
      "[400]\ttrain-mlogloss:0.249367\tvalid-mlogloss:0.286991\n",
      "[600]\ttrain-mlogloss:0.227646\tvalid-mlogloss:0.282473\n",
      "[800]\ttrain-mlogloss:0.210445\tvalid-mlogloss:0.280495\n",
      "[1000]\ttrain-mlogloss:0.195492\tvalid-mlogloss:0.279873\n",
      "Stopping. Best iteration:\n",
      "[1027]\ttrain-mlogloss:0.193613\tvalid-mlogloss:0.279823\n",
      "\n",
      "train log loss 0.18701520658 valid log loss 0.280004205307\n",
      "rev 3.57137493311\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0167  0.0041  0.9792\n",
      "1  id24541  0.9989  0.0006  0.0005\n",
      "2  id00134  0.0033  0.9958  0.0009\n",
      "3  id27757  0.6335  0.3613  0.0052\n",
      "4  id04081  0.8271  0.1132  0.0597\n",
      "--------------\n",
      "18.1883639254\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0169  0.0041  0.9791\n",
      "1  id24541  0.9989  0.0006  0.0005\n",
      "2  id00134  0.0033  0.9958  0.0009\n",
      "3  id27757  0.6308  0.3640  0.0052\n",
      "4  id04081  0.8264  0.1135  0.0601\n",
      "---------------\n",
      "local average valid loss 0.275594831756\n",
      "train log loss 0.19472020012\n"
     ]
    }
   ],
   "source": [
    "cv_test(5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
