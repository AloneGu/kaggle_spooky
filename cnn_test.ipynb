{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "#     model.add(Conv1D(16,\n",
    "#                      3,\n",
    "#                      padding='valid',\n",
    "#                      activation='relu',\n",
    "#                      strides=1))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.33,\n",
    "              batch_size=32, epochs=20, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "    model = load_model(MODEL_P)\n",
    "    train_pred = model.predict(train_x)\n",
    "    test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(log_loss(train_y,train_pred))\n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 10)           300000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 300,423\n",
      "Trainable params: 300,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13117 samples, validate on 6462 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.01641, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 1.0716 - acc: 0.4125 - val_loss: 1.0164 - val_acc: 0.4847\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.01641 to 0.69507, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.8431 - acc: 0.6610 - val_loss: 0.6951 - val_acc: 0.7447\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.69507 to 0.53846, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.5505 - acc: 0.8035 - val_loss: 0.5385 - val_acc: 0.7951\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.53846 to 0.46855, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.3949 - acc: 0.8623 - val_loss: 0.4686 - val_acc: 0.8135\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.46855 to 0.43774, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.2999 - acc: 0.9001 - val_loss: 0.4377 - val_acc: 0.8227\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.43774 to 0.42557, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.2346 - acc: 0.9240 - val_loss: 0.4256 - val_acc: 0.8275\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.42557 to 0.41457, saving model to /tmp/lstm.h5\n",
      " - 1s - loss: 0.1886 - acc: 0.9429 - val_loss: 0.4146 - val_acc: 0.8338\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 1s - loss: 0.1530 - acc: 0.9527 - val_loss: 0.4163 - val_acc: 0.8340\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 1s - loss: 0.1254 - acc: 0.9640 - val_loss: 0.4223 - val_acc: 0.8366\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1051 - acc: 0.9700 - val_loss: 0.4337 - val_acc: 0.8366\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.0894 - acc: 0.9741 - val_loss: 0.4482 - val_acc: 0.8355\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.0742 - acc: 0.9799 - val_loss: 0.4665 - val_acc: 0.8338\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.0639 - acc: 0.9830 - val_loss: 0.4856 - val_acc: 0.8341\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0551 - acc: 0.9853 - val_loss: 0.5061 - val_acc: 0.8323\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0474 - acc: 0.9873 - val_loss: 0.5385 - val_acc: 0.8309\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 1s - loss: 0.0405 - acc: 0.9896 - val_loss: 0.5651 - val_acc: 0.8293\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 1s - loss: 0.0351 - acc: 0.9914 - val_loss: 0.5821 - val_acc: 0.8279\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 1s - loss: 0.0306 - acc: 0.9931 - val_loss: 0.6340 - val_acc: 0.8240\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 1s - loss: 0.0258 - acc: 0.9936 - val_loss: 0.6631 - val_acc: 0.8216\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 1s - loss: 0.0222 - acc: 0.9952 - val_loss: 0.6792 - val_acc: 0.8202\n",
      "0.236071064896\n"
     ]
    }
   ],
   "source": [
    "cnn_train,cnn_test = get_cnn_feats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
