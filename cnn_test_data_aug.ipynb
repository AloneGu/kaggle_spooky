{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19579.000000\n",
       "mean        26.730477\n",
       "std         19.048353\n",
       "min          2.000000\n",
       "25%         15.000000\n",
       "50%         23.000000\n",
       "75%         34.000000\n",
       "max        861.000000\n",
       "Name: word_cnt, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "train_df['word_cnt'] = train_df['text'].apply(lambda x:len(str(x).split(' ')))\n",
    "train_df['word_cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "7672\n",
      "2123\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(train_df[train_df.word_cnt<20]))\n",
    "print(len(train_df[train_df.word_cnt<10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 12\n",
      "kkkk\n"
     ]
    }
   ],
   "source": [
    "test_x = 'sdf .ksdfkl .'\n",
    "print(test_x.find('.'),test_x.rfind('.'))\n",
    "\n",
    "def find_p(s):\n",
    "    punctuation = ['.',',', ':', ';', '-', '*', '\"', '!', '?']\n",
    "    s_len = len(s)\n",
    "    for i in range(s_len):\n",
    "        ch = s[i]\n",
    "        if ch in punctuation:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "tmp_s = 'hello world. kkkk'\n",
    "idx = find_p(tmp_s)\n",
    "print(tmp_s[idx+2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_aug(x,y,aug_cnt=3):\n",
    "    new_x,new_y = [],[]\n",
    "    print(len(x))\n",
    "    for tmp_x,tmp_y in zip(x,y):\n",
    "        new_x.append(tmp_x)\n",
    "        new_y.append(tmp_y)\n",
    "        \n",
    "        for i in range(aug_cnt):\n",
    "            tmp_idx = find_p(tmp_x)\n",
    "            if tmp_x is not None:\n",
    "                tmp_x = tmp_x[tmp_idx+2:]\n",
    "                word_cnt = len(tmp_x.split(' '))\n",
    "                if word_cnt >= 20:\n",
    "                    new_x.append(tmp_x)\n",
    "                    new_y.append(tmp_y)\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    print(len(new_x))\n",
    "    return new_x,new_y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 150\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text'].values\n",
    "    Y = train_df['author'].values\n",
    "    X_test = test_df['text'].values\n",
    "    \n",
    "    X,Y = text_aug(X,Y,10)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "#     model.add(Conv1D(16,\n",
    "#                      3,\n",
    "#                      padding='valid',\n",
    "#                      activation='relu',\n",
    "#                      strides=1))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.1,\n",
    "              batch_size=64, epochs=20, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "    model = load_model(MODEL_P)\n",
    "    train_pred = model.predict(train_x)\n",
    "    test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print(log_loss(train_y,train_pred))\n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19579\n",
      "39387\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 150, 10)           300000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 300,423\n",
      "Trainable params: 300,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35448 samples, validate on 3939 samples\n",
      "Epoch 1/20\n",
      "Epoch 00001: val_loss improved from inf to 1.03565, saving model to /tmp/lstm.h5\n",
      " - 3s - loss: 1.0671 - acc: 0.4295 - val_loss: 1.0356 - val_acc: 0.4306\n",
      "Epoch 2/20\n",
      "Epoch 00002: val_loss improved from 1.03565 to 0.74433, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.8854 - acc: 0.6030 - val_loss: 0.7443 - val_acc: 0.7179\n",
      "Epoch 3/20\n",
      "Epoch 00003: val_loss improved from 0.74433 to 0.54466, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.5747 - acc: 0.7992 - val_loss: 0.5447 - val_acc: 0.7847\n",
      "Epoch 4/20\n",
      "Epoch 00004: val_loss improved from 0.54466 to 0.45671, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.4007 - acc: 0.8601 - val_loss: 0.4567 - val_acc: 0.8233\n",
      "Epoch 5/20\n",
      "Epoch 00005: val_loss improved from 0.45671 to 0.40523, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.3043 - acc: 0.8946 - val_loss: 0.4052 - val_acc: 0.8441\n",
      "Epoch 6/20\n",
      "Epoch 00006: val_loss improved from 0.40523 to 0.37662, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.2404 - acc: 0.9185 - val_loss: 0.3766 - val_acc: 0.8535\n",
      "Epoch 7/20\n",
      "Epoch 00007: val_loss improved from 0.37662 to 0.35713, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.1929 - acc: 0.9350 - val_loss: 0.3571 - val_acc: 0.8606\n",
      "Epoch 8/20\n",
      "Epoch 00008: val_loss improved from 0.35713 to 0.34477, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.1589 - acc: 0.9467 - val_loss: 0.3448 - val_acc: 0.8660\n",
      "Epoch 9/20\n",
      "Epoch 00009: val_loss improved from 0.34477 to 0.33970, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.1325 - acc: 0.9566 - val_loss: 0.3397 - val_acc: 0.8733\n",
      "Epoch 10/20\n",
      "Epoch 00010: val_loss improved from 0.33970 to 0.33693, saving model to /tmp/lstm.h5\n",
      " - 2s - loss: 0.1104 - acc: 0.9650 - val_loss: 0.3369 - val_acc: 0.8797\n",
      "Epoch 11/20\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 2s - loss: 0.0936 - acc: 0.9708 - val_loss: 0.3419 - val_acc: 0.8799\n",
      "Epoch 12/20\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 2s - loss: 0.0808 - acc: 0.9751 - val_loss: 0.3478 - val_acc: 0.8807\n",
      "Epoch 13/20\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 2s - loss: 0.0690 - acc: 0.9796 - val_loss: 0.3590 - val_acc: 0.8794\n",
      "Epoch 14/20\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 2s - loss: 0.0598 - acc: 0.9830 - val_loss: 0.3739 - val_acc: 0.8746\n",
      "Epoch 15/20\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 2s - loss: 0.0531 - acc: 0.9838 - val_loss: 0.3867 - val_acc: 0.8736\n",
      "Epoch 16/20\n",
      "Epoch 00016: val_loss did not improve\n",
      " - 2s - loss: 0.0452 - acc: 0.9865 - val_loss: 0.4098 - val_acc: 0.8662\n",
      "Epoch 17/20\n",
      "Epoch 00017: val_loss did not improve\n",
      " - 2s - loss: 0.0406 - acc: 0.9883 - val_loss: 0.4260 - val_acc: 0.8662\n",
      "Epoch 18/20\n",
      "Epoch 00018: val_loss did not improve\n",
      " - 2s - loss: 0.0351 - acc: 0.9900 - val_loss: 0.4472 - val_acc: 0.8621\n",
      "Epoch 19/20\n",
      "Epoch 00019: val_loss did not improve\n",
      " - 2s - loss: 0.0315 - acc: 0.9908 - val_loss: 0.4662 - val_acc: 0.8614\n",
      "Epoch 20/20\n",
      "Epoch 00020: val_loss did not improve\n",
      " - 2s - loss: 0.0288 - acc: 0.9918 - val_loss: 0.4817 - val_acc: 0.8611\n",
      "0.111838003098\n"
     ]
    }
   ],
   "source": [
    "cnn_train,cnn_test = get_cnn_feats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
