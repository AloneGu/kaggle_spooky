{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_feats(a=20000,b=10,c=300,bat=32):\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = a\n",
    "    N = b\n",
    "    MAX_LEN = c\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(LSTM(N, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    #model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.1,\n",
    "              batch_size=bat, epochs=5, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "#     model = load_model(MODEL_P)\n",
    "#     train_pred = model.predict(train_x)\n",
    "#     test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "#     print(log_loss(train_y,train_pred))\n",
    "#     return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.72420, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 0.9873 - acc: 0.5076 - val_loss: 0.7242 - val_acc: 0.7007\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.72420 to 0.48216, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5579 - acc: 0.7772 - val_loss: 0.4822 - val_acc: 0.7988\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.48216 to 0.45165, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3661 - acc: 0.8595 - val_loss: 0.4516 - val_acc: 0.8166\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 26s - loss: 0.2788 - acc: 0.8936 - val_loss: 0.4679 - val_acc: 0.8238\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 25s - loss: 0.2254 - acc: 0.9138 - val_loss: 0.5126 - val_acc: 0.8253\n",
      "----------------------\n",
      "12000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.80862, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 1.0170 - acc: 0.4757 - val_loss: 0.8086 - val_acc: 0.6675\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.80862 to 0.46784, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5868 - acc: 0.7602 - val_loss: 0.4678 - val_acc: 0.8126\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.46784 to 0.43014, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3449 - acc: 0.8679 - val_loss: 0.4301 - val_acc: 0.8279\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 25s - loss: 0.2415 - acc: 0.9071 - val_loss: 0.4407 - val_acc: 0.8391\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 25s - loss: 0.1836 - acc: 0.9313 - val_loss: 0.4895 - val_acc: 0.8381\n",
      "----------------------\n",
      "16000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.79881, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 1.0151 - acc: 0.4791 - val_loss: 0.7988 - val_acc: 0.6736\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.79881 to 0.45867, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5724 - acc: 0.7659 - val_loss: 0.4587 - val_acc: 0.8161\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.45867 to 0.41977, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3255 - acc: 0.8767 - val_loss: 0.4198 - val_acc: 0.8315\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 25s - loss: 0.2214 - acc: 0.9160 - val_loss: 0.4446 - val_acc: 0.8412\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 26s - loss: 0.1647 - acc: 0.9386 - val_loss: 0.4942 - val_acc: 0.8386\n",
      "----------------------\n",
      "20000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.79917, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 1.0140 - acc: 0.4795 - val_loss: 0.7992 - val_acc: 0.6747\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.79917 to 0.46213, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5716 - acc: 0.7678 - val_loss: 0.4621 - val_acc: 0.8151\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.46213 to 0.41978, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 0.3194 - acc: 0.8788 - val_loss: 0.4198 - val_acc: 0.8304\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 26s - loss: 0.2112 - acc: 0.9197 - val_loss: 0.4479 - val_acc: 0.8437\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 26s - loss: 0.1586 - acc: 0.9405 - val_loss: 0.4881 - val_acc: 0.8432\n",
      "----------------------\n",
      "24000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.79931, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 1.0147 - acc: 0.4785 - val_loss: 0.7993 - val_acc: 0.6742\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.79931 to 0.46126, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5682 - acc: 0.7686 - val_loss: 0.4613 - val_acc: 0.8151\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.46126 to 0.42066, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3118 - acc: 0.8820 - val_loss: 0.4207 - val_acc: 0.8335\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 25s - loss: 0.2019 - acc: 0.9237 - val_loss: 0.4490 - val_acc: 0.8432\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 25s - loss: 0.1506 - acc: 0.9432 - val_loss: 0.4913 - val_acc: 0.8412\n",
      "----------------------\n",
      "28000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n",
      "Epoch 00001: val_loss improved from inf to 0.79967, saving model to /tmp/lstm.h5\n",
      " - 27s - loss: 1.0149 - acc: 0.4783 - val_loss: 0.7997 - val_acc: 0.6777\n",
      "Epoch 2/5\n",
      "Epoch 00002: val_loss improved from 0.79967 to 0.46123, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.5671 - acc: 0.7694 - val_loss: 0.4612 - val_acc: 0.8126\n",
      "Epoch 3/5\n",
      "Epoch 00003: val_loss improved from 0.46123 to 0.42135, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3097 - acc: 0.8823 - val_loss: 0.4213 - val_acc: 0.8366\n",
      "Epoch 4/5\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 26s - loss: 0.1992 - acc: 0.9243 - val_loss: 0.4546 - val_acc: 0.8407\n",
      "Epoch 5/5\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 25s - loss: 0.1483 - acc: 0.9439 - val_loss: 0.4966 - val_acc: 0.8386\n",
      "----------------------\n",
      "32000 12 300\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-afcab47e5414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             \u001b[0mget_lstm_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1e3129b8bdb8>\u001b[0m in \u001b[0;36mget_lstm_feats\u001b[0;34m(a, b, c, bat)\u001b[0m\n\u001b[1;32m     44\u001b[0m               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_chk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m              )\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for a in [8000, 12000,16000,20000,24000,28000,32000]:\n",
    "    for b in [12]:\n",
    "        for c in [300]:\n",
    "            print(a,b,c)\n",
    "            get_lstm_feats(a,b,c,256)\n",
    "            print('----------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
