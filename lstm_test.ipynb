{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "import gc\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "# train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "# test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n"
     ]
    }
   ],
   "source": [
    "def get_lstm_feats(a=20000,b=10,c=300,bat=32):\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = a\n",
    "    N = b\n",
    "    MAX_LEN = c\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(N))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    np.random.seed(42)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.1,\n",
    "              batch_size=bat, epochs=10, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk],\n",
    "              shuffle=False\n",
    "             )\n",
    "    \n",
    "#     model = load_model(MODEL_P)\n",
    "#     train_pred = model.predict(train_x)\n",
    "#     test_pred = model.predict(test_x)\n",
    "    del model\n",
    "    gc.collect()\n",
    "#     print(log_loss(train_y,train_pred))\n",
    "#     return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 12)           192000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12)                1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 39        \n",
      "=================================================================\n",
      "Total params: 193,395\n",
      "Trainable params: 193,395\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17621 samples, validate on 1958 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 1.04360, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 1.0788 - acc: 0.4032 - val_loss: 1.0436 - val_acc: 0.4316\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 1.04360 to 0.79559, saving model to /tmp/lstm.h5\n",
      " - 26s - loss: 0.9350 - acc: 0.5791 - val_loss: 0.7956 - val_acc: 0.6925\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.79559 to 0.57880, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.6519 - acc: 0.7472 - val_loss: 0.5788 - val_acc: 0.7778\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.57880 to 0.48075, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.4382 - acc: 0.8426 - val_loss: 0.4807 - val_acc: 0.8136\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss improved from 0.48075 to 0.45953, saving model to /tmp/lstm.h5\n",
      " - 25s - loss: 0.3118 - acc: 0.8918 - val_loss: 0.4595 - val_acc: 0.8248\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 25s - loss: 0.2333 - acc: 0.9209 - val_loss: 0.4682 - val_acc: 0.8243\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 25s - loss: 0.1938 - acc: 0.9371 - val_loss: 0.4955 - val_acc: 0.8264\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "get_lstm_feats(16000,12,300,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for a in [8000, 12000,16000,20000,24000,28000,32000]:\n",
    "#     for b in [12]:\n",
    "#         for c in [300]:\n",
    "#             print(a,b,c)\n",
    "#             get_lstm_feats(a,b,c,256)\n",
    "#             print('----------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
