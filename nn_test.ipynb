{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding, LSTM, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "# replace\n",
    "train_df['text'] = train_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "test_df['text'] =test_df['text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "def get_lstm_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = 10000\n",
    "    N = 10\n",
    "    MAX_LEN = 200\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.2,\n",
    "              batch_size=256, epochs=6, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk]\n",
    "             )\n",
    "    \n",
    "    model = load_model(MODEL_P)\n",
    "    train_pred = model.predict(train_x)\n",
    "    test_pred = model.predict(test_x)\n",
    "    print(log_loss(train_y,train_pred))\n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def lstm done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats():\n",
    "    # return train pred prob and test pred prob \n",
    "    NUM_WORDS = 20000\n",
    "    N = 10\n",
    "    MAX_LEN = 80\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/lstm.h5'\n",
    "    \n",
    "    X = train_df['text']\n",
    "    Y = train_df['author']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "\n",
    "    train_x = tokenizer.texts_to_sequences(X)\n",
    "    train_x = pad_sequences(train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    test_x = tokenizer.texts_to_sequences(X_test)\n",
    "    test_x = pad_sequences(test_x, maxlen=MAX_LEN)\n",
    "\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(Y)\n",
    "\n",
    "    train_y = lb.transform(Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "    model.add(Conv1D(16,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    model.fit(train_x, train_y, \n",
    "              validation_split=0.2,\n",
    "              batch_size=256, epochs=20, \n",
    "              verbose=2,\n",
    "              callbacks=[model_chk]\n",
    "             )\n",
    "    \n",
    "    model = load_model(MODEL_P)\n",
    "    train_pred = model.predict(train_x)\n",
    "    test_pred = model.predict(test_x)\n",
    "    print(log_loss(train_y,train_pred))\n",
    "    return train_pred,test_pred\n",
    "\n",
    "print('def cnn done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 80, 10)            200000    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 78, 16)            496       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_11  (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 200,819\n",
      "Trainable params: 200,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 1.08288, saving model to /tmp/lstm.h5\n",
      "0s - loss: 1.0876 - acc: 0.4043 - val_loss: 1.0829 - val_acc: 0.3999\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 1.08288 to 1.05394, saving model to /tmp/lstm.h5\n",
      "0s - loss: 1.0729 - acc: 0.4071 - val_loss: 1.0539 - val_acc: 0.4122\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 1.05394 to 0.95547, saving model to /tmp/lstm.h5\n",
      "0s - loss: 1.0088 - acc: 0.4844 - val_loss: 0.9555 - val_acc: 0.5534\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss improved from 0.95547 to 0.81802, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.8639 - acc: 0.6266 - val_loss: 0.8180 - val_acc: 0.6473\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss improved from 0.81802 to 0.70369, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.7189 - acc: 0.7255 - val_loss: 0.7037 - val_acc: 0.7360\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss improved from 0.70369 to 0.60824, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.5930 - acc: 0.7896 - val_loss: 0.6082 - val_acc: 0.7919\n",
      "Epoch 7/20\n",
      "Epoch 00006: val_loss improved from 0.60824 to 0.52943, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.4805 - acc: 0.8402 - val_loss: 0.5294 - val_acc: 0.8087\n",
      "Epoch 8/20\n",
      "Epoch 00007: val_loss improved from 0.52943 to 0.48197, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.3924 - acc: 0.8727 - val_loss: 0.4820 - val_acc: 0.8223\n",
      "Epoch 9/20\n",
      "Epoch 00008: val_loss improved from 0.48197 to 0.44825, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.3236 - acc: 0.8975 - val_loss: 0.4482 - val_acc: 0.8304\n",
      "Epoch 10/20\n",
      "Epoch 00009: val_loss improved from 0.44825 to 0.43602, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2757 - acc: 0.9139 - val_loss: 0.4360 - val_acc: 0.8361\n",
      "Epoch 11/20\n",
      "Epoch 00010: val_loss improved from 0.43602 to 0.43362, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2350 - acc: 0.9298 - val_loss: 0.4336 - val_acc: 0.8309\n",
      "Epoch 12/20\n",
      "Epoch 00011: val_loss improved from 0.43362 to 0.42612, saving model to /tmp/lstm.h5\n",
      "0s - loss: 0.2073 - acc: 0.9365 - val_loss: 0.4261 - val_acc: 0.8381\n",
      "Epoch 13/20\n",
      "Epoch 00012: val_loss did not improve\n",
      "0s - loss: 0.1744 - acc: 0.9485 - val_loss: 0.4295 - val_acc: 0.8376\n",
      "Epoch 14/20\n",
      "Epoch 00013: val_loss did not improve\n",
      "0s - loss: 0.1588 - acc: 0.9545 - val_loss: 0.4307 - val_acc: 0.8399\n",
      "Epoch 15/20\n",
      "Epoch 00014: val_loss did not improve\n",
      "0s - loss: 0.1419 - acc: 0.9591 - val_loss: 0.4650 - val_acc: 0.8294\n",
      "Epoch 16/20\n",
      "Epoch 00015: val_loss did not improve\n",
      "0s - loss: 0.1287 - acc: 0.9630 - val_loss: 0.4500 - val_acc: 0.8396\n",
      "Epoch 17/20\n",
      "Epoch 00016: val_loss did not improve\n",
      "0s - loss: 0.1166 - acc: 0.9674 - val_loss: 0.4638 - val_acc: 0.8381\n",
      "Epoch 18/20\n",
      "Epoch 00017: val_loss did not improve\n",
      "0s - loss: 0.1066 - acc: 0.9712 - val_loss: 0.4925 - val_acc: 0.8322\n",
      "Epoch 19/20\n",
      "Epoch 00018: val_loss did not improve\n",
      "0s - loss: 0.1031 - acc: 0.9710 - val_loss: 0.5059 - val_acc: 0.8343\n",
      "Epoch 20/20\n",
      "Epoch 00019: val_loss did not improve\n",
      "0s - loss: 0.0891 - acc: 0.9768 - val_loss: 0.5008 - val_acc: 0.8368\n",
      "0.203909325887\n"
     ]
    }
   ],
   "source": [
    "cnn_train,cnn_test = get_cnn_feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train,lstm_test = get_lstm_feats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
