{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "test_df = pd.read_csv(\"./input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import keras done\n"
     ]
    }
   ],
   "source": [
    "# add cnn feat\n",
    "from keras.layers import Embedding, CuDNNLSTM, Dense, Flatten, Dropout, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "print('import keras done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_p(s):\n",
    "    punctuation = ['.', ';', '\"', '!', '?']\n",
    "    s_len = len(s)\n",
    "    for i in range(s_len):\n",
    "        ch = s[i]\n",
    "        if ch in punctuation:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def text_aug(x,y,aug_cnt=5):\n",
    "    new_x,new_y = [],[]\n",
    "    #print(len(x))\n",
    "    for tmp_x,tmp_y in zip(x,y):\n",
    "        new_x.append(tmp_x)\n",
    "        new_y.append(tmp_y)\n",
    "        \n",
    "        for i in range(aug_cnt):\n",
    "            tmp_idx = find_p(tmp_x)\n",
    "            if tmp_x is not None:\n",
    "                tmp_x = tmp_x[tmp_idx+2:]\n",
    "                word_cnt = len(tmp_x.split(' '))\n",
    "                if word_cnt >= 20:\n",
    "                    new_x.append(tmp_x)\n",
    "                    new_y.append(tmp_y)\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    #print(len(new_x))\n",
    "    return new_x,np.array(new_y)\n",
    "\n",
    "org_X, org_Y = train_df['text'].values,train_df['author'].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 16230 samples, validate on 6527 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.87968, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0309 - acc: 0.4682 - val_loss: 0.8797 - val_acc: 0.5761\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.87968 to 0.57381, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.6544 - acc: 0.7134 - val_loss: 0.5738 - val_acc: 0.7763\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.57381 to 0.46719, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.3622 - acc: 0.8697 - val_loss: 0.4672 - val_acc: 0.8209\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.46719 to 0.45567, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.2265 - acc: 0.9225 - val_loss: 0.4557 - val_acc: 0.8275\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 2s - loss: 0.1584 - acc: 0.9463 - val_loss: 0.4784 - val_acc: 0.8263\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 2s - loss: 0.1133 - acc: 0.9640 - val_loss: 0.5207 - val_acc: 0.8279\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 2s - loss: 0.0864 - acc: 0.9769 - val_loss: 0.5669 - val_acc: 0.8227\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.0666 - acc: 0.9818 - val_loss: 0.6123 - val_acc: 0.8204\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 2s - loss: 0.0513 - acc: 0.9874 - val_loss: 0.6884 - val_acc: 0.8183\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 2s - loss: 0.0454 - acc: 0.9880 - val_loss: 0.7272 - val_acc: 0.8123\n",
      "------------------\n",
      "Train on 16180 samples, validate on 6526 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.95257, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0500 - acc: 0.4491 - val_loss: 0.9526 - val_acc: 0.5513\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.95257 to 0.56914, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.7025 - acc: 0.7161 - val_loss: 0.5691 - val_acc: 0.7849\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.56914 to 0.45025, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.3774 - acc: 0.8676 - val_loss: 0.4503 - val_acc: 0.8200\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.45025 to 0.43964, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.2413 - acc: 0.9221 - val_loss: 0.4396 - val_acc: 0.8281\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 2s - loss: 0.1722 - acc: 0.9449 - val_loss: 0.4758 - val_acc: 0.8245\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 2s - loss: 0.1301 - acc: 0.9618 - val_loss: 0.4895 - val_acc: 0.8302\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 2s - loss: 0.1010 - acc: 0.9705 - val_loss: 0.5354 - val_acc: 0.8296\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.0794 - acc: 0.9765 - val_loss: 0.5789 - val_acc: 0.8259\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 2s - loss: 0.0642 - acc: 0.9805 - val_loss: 0.6392 - val_acc: 0.8218\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 2s - loss: 0.0526 - acc: 0.9845 - val_loss: 0.6889 - val_acc: 0.8219\n",
      "------------------\n",
      "Train on 16146 samples, validate on 6526 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.87576, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0299 - acc: 0.4617 - val_loss: 0.8758 - val_acc: 0.5844\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.87576 to 0.68274, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.7274 - acc: 0.6675 - val_loss: 0.6827 - val_acc: 0.6975\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.68274 to 0.56699, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.4990 - acc: 0.8095 - val_loss: 0.5670 - val_acc: 0.7694\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.56699 to 0.52249, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 0.3275 - acc: 0.8872 - val_loss: 0.5225 - val_acc: 0.7976\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 2s - loss: 0.2330 - acc: 0.9201 - val_loss: 0.5343 - val_acc: 0.8049\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 2s - loss: 0.1744 - acc: 0.9425 - val_loss: 0.5491 - val_acc: 0.8108\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 2s - loss: 0.1316 - acc: 0.9578 - val_loss: 0.5885 - val_acc: 0.8108\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 2s - loss: 0.1019 - acc: 0.9691 - val_loss: 0.6322 - val_acc: 0.8121\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 2s - loss: 0.0820 - acc: 0.9755 - val_loss: 0.6919 - val_acc: 0.8127\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 2s - loss: 0.0700 - acc: 0.9778 - val_loss: 0.7598 - val_acc: 0.8091\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 3\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = org_X\n",
    "    tmp_Y = org_Y\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    tmp_Y = lb.fit_transform(tmp_Y)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "    \n",
    "    # test\n",
    "    final_test_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    final_test_x = pad_sequences(final_test_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    # train + valid\n",
    "    all_train_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    all_train_x = pad_sequences(all_train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(tmp_X):\n",
    "        # prepare aug train, val\n",
    "        aug_X, aug_Y = text_aug(tmp_X[train_index],tmp_Y[train_index])\n",
    "        val_X, val_Y = all_train_x[test_index],tmp_Y[test_index]\n",
    "        \n",
    "        curr_train_x = tokenizer.texts_to_sequences(aug_X)\n",
    "        curr_train_x = pad_sequences(curr_train_x, maxlen=MAX_LEN)\n",
    "      \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(Conv1D(16,\n",
    "                         3,\n",
    "                         padding='valid',\n",
    "                         activation='relu',\n",
    "                         strides=1))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        model.fit(curr_train_x, aug_Y, \n",
    "                  validation_data=(val_X,val_Y),\n",
    "                  batch_size=32, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(val_X)\n",
    "        test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(val_X)\n",
    "        best_val_test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "cnn_train1,cnn_test1,cnn_train2,cnn_test2 = get_cnn_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def lstm done\n",
      "Train on 16145 samples, validate on 6527 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.81542, saving model to /tmp/nn_model.h5\n",
      " - 49s - loss: 1.0117 - acc: 0.4718 - val_loss: 0.8154 - val_acc: 0.6545\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.81542 to 0.49112, saving model to /tmp/nn_model.h5\n",
      " - 48s - loss: 0.5651 - acc: 0.7664 - val_loss: 0.4911 - val_acc: 0.8065\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.49112 to 0.45854, saving model to /tmp/nn_model.h5\n",
      " - 48s - loss: 0.2856 - acc: 0.8920 - val_loss: 0.4585 - val_acc: 0.8249\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 47s - loss: 0.1796 - acc: 0.9356 - val_loss: 0.4856 - val_acc: 0.8266\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 46s - loss: 0.1196 - acc: 0.9579 - val_loss: 0.5575 - val_acc: 0.8197\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 46s - loss: 0.0917 - acc: 0.9694 - val_loss: 0.5956 - val_acc: 0.8149\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 48s - loss: 0.0709 - acc: 0.9752 - val_loss: 0.6910 - val_acc: 0.8175\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 48s - loss: 0.0543 - acc: 0.9810 - val_loss: 0.7448 - val_acc: 0.8117\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 47s - loss: 0.0463 - acc: 0.9851 - val_loss: 0.7542 - val_acc: 0.8112\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 46s - loss: 0.0393 - acc: 0.9867 - val_loss: 0.8372 - val_acc: 0.8047\n",
      "------------------\n",
      "Train on 16153 samples, validate on 6526 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.78777, saving model to /tmp/nn_model.h5\n",
      " - 48s - loss: 1.0218 - acc: 0.4735 - val_loss: 0.7878 - val_acc: 0.6820\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.78777 to 0.49247, saving model to /tmp/nn_model.h5\n",
      " - 47s - loss: 0.5214 - acc: 0.7957 - val_loss: 0.4925 - val_acc: 0.8013\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.49247 to 0.47317, saving model to /tmp/nn_model.h5\n",
      " - 47s - loss: 0.2626 - acc: 0.9013 - val_loss: 0.4732 - val_acc: 0.8190\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss did not improve\n",
      " - 46s - loss: 0.1662 - acc: 0.9426 - val_loss: 0.5219 - val_acc: 0.8190\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 46s - loss: 0.1152 - acc: 0.9614 - val_loss: 0.5698 - val_acc: 0.8155\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 46s - loss: 0.0861 - acc: 0.9705 - val_loss: 0.6596 - val_acc: 0.8066\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 46s - loss: 0.0647 - acc: 0.9788 - val_loss: 0.7348 - val_acc: 0.7976\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 46s - loss: 0.0551 - acc: 0.9814 - val_loss: 0.7871 - val_acc: 0.8062\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 46s - loss: 0.0494 - acc: 0.9834 - val_loss: 0.7970 - val_acc: 0.8049\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 46s - loss: 0.0374 - acc: 0.9869 - val_loss: 0.9482 - val_acc: 0.7916\n",
      "------------------\n",
      "Train on 16258 samples, validate on 6526 samples\n",
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 0.83736, saving model to /tmp/nn_model.h5\n",
      " - 47s - loss: 1.0119 - acc: 0.4847 - val_loss: 0.8374 - val_acc: 0.6232\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 0.83736 to 0.52753, saving model to /tmp/nn_model.h5\n",
      " - 46s - loss: 0.5391 - acc: 0.7850 - val_loss: 0.5275 - val_acc: 0.7901\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 0.52753 to 0.52157, saving model to /tmp/nn_model.h5\n",
      " - 47s - loss: 0.2776 - acc: 0.8988 - val_loss: 0.5216 - val_acc: 0.8062\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 0.52157 to 0.52046, saving model to /tmp/nn_model.h5\n",
      " - 47s - loss: 0.1739 - acc: 0.9391 - val_loss: 0.5205 - val_acc: 0.8154\n",
      "Epoch 5/10\n",
      "Epoch 00005: val_loss did not improve\n",
      " - 47s - loss: 0.1170 - acc: 0.9595 - val_loss: 0.5815 - val_acc: 0.8124\n",
      "Epoch 6/10\n",
      "Epoch 00006: val_loss did not improve\n",
      " - 46s - loss: 0.0913 - acc: 0.9697 - val_loss: 0.6486 - val_acc: 0.8103\n",
      "Epoch 7/10\n",
      "Epoch 00007: val_loss did not improve\n",
      " - 47s - loss: 0.0701 - acc: 0.9762 - val_loss: 0.7290 - val_acc: 0.8043\n",
      "Epoch 8/10\n",
      "Epoch 00008: val_loss did not improve\n",
      " - 46s - loss: 0.0498 - acc: 0.9844 - val_loss: 0.8153 - val_acc: 0.8023\n",
      "Epoch 9/10\n",
      "Epoch 00009: val_loss did not improve\n",
      " - 46s - loss: 0.0404 - acc: 0.9869 - val_loss: 0.9111 - val_acc: 0.8032\n",
      "Epoch 10/10\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 47s - loss: 0.0344 - acc: 0.9883 - val_loss: 0.9265 - val_acc: 0.8042\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# add lstm feat\n",
    "def get_lstm_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 3\n",
    "    NUM_WORDS = 16000\n",
    "    N = 12\n",
    "    MAX_LEN = 300\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = org_X\n",
    "    tmp_Y = org_Y\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    tmp_Y = lb.fit_transform(tmp_Y)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "    \n",
    "    # test\n",
    "    final_test_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    final_test_x = pad_sequences(final_test_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    # train + valid\n",
    "    all_train_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    all_train_x = pad_sequences(all_train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=2333*rnd)\n",
    "    for train_index, test_index in kf.split(tmp_X):\n",
    "        # prepare aug train, val\n",
    "        aug_X, aug_Y = text_aug(tmp_X[train_index],tmp_Y[train_index])\n",
    "        val_X, val_Y = all_train_x[test_index],tmp_Y[test_index]\n",
    "        \n",
    "        curr_train_x = tokenizer.texts_to_sequences(aug_X)\n",
    "        curr_train_x = pad_sequences(curr_train_x, maxlen=MAX_LEN)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(LSTM(N, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        model.fit(curr_train_x, aug_Y, \n",
    "                  validation_data=(val_X,val_Y),\n",
    "                  batch_size=128, epochs=10, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(val_X)\n",
    "        test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(val_X)\n",
    "        best_val_test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def lstm done')\n",
    "lstm_train1,lstm_test1,lstm_train2,lstm_test2 = get_lstm_feats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def cnn done\n",
      "Train on 16191 samples, validate on 6527 samples\n",
      "Epoch 1/15\n",
      "Epoch 00001: val_loss improved from inf to 1.05355, saving model to /tmp/nn_model.h5\n",
      " - 2s - loss: 1.0805 - acc: 0.4075 - val_loss: 1.0536 - val_acc: 0.4521\n",
      "Epoch 2/15\n",
      "Epoch 00002: val_loss improved from 1.05355 to 0.87596, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9563 - acc: 0.5659 - val_loss: 0.8760 - val_acc: 0.5848\n",
      "Epoch 3/15\n",
      "Epoch 00003: val_loss improved from 0.87596 to 0.68718, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7226 - acc: 0.6995 - val_loss: 0.6872 - val_acc: 0.7371\n",
      "Epoch 4/15\n",
      "Epoch 00004: val_loss improved from 0.68718 to 0.55316, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5114 - acc: 0.8282 - val_loss: 0.5532 - val_acc: 0.7929\n",
      "Epoch 5/15\n",
      "Epoch 00005: val_loss improved from 0.55316 to 0.48710, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3731 - acc: 0.8812 - val_loss: 0.4871 - val_acc: 0.8114\n",
      "Epoch 6/15\n",
      "Epoch 00006: val_loss improved from 0.48710 to 0.44878, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2903 - acc: 0.9088 - val_loss: 0.4488 - val_acc: 0.8232\n",
      "Epoch 7/15\n",
      "Epoch 00007: val_loss improved from 0.44878 to 0.43292, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2297 - acc: 0.9298 - val_loss: 0.4329 - val_acc: 0.8261\n",
      "Epoch 8/15\n",
      "Epoch 00008: val_loss improved from 0.43292 to 0.41899, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1887 - acc: 0.9424 - val_loss: 0.4190 - val_acc: 0.8309\n",
      "Epoch 9/15\n",
      "Epoch 00009: val_loss improved from 0.41899 to 0.41175, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1559 - acc: 0.9542 - val_loss: 0.4117 - val_acc: 0.8382\n",
      "Epoch 10/15\n",
      "Epoch 00010: val_loss did not improve\n",
      " - 1s - loss: 0.1327 - acc: 0.9601 - val_loss: 0.4170 - val_acc: 0.8379\n",
      "Epoch 11/15\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1121 - acc: 0.9688 - val_loss: 0.4148 - val_acc: 0.8394\n",
      "Epoch 12/15\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.0947 - acc: 0.9728 - val_loss: 0.4238 - val_acc: 0.8378\n",
      "Epoch 13/15\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.0830 - acc: 0.9773 - val_loss: 0.4398 - val_acc: 0.8344\n",
      "Epoch 14/15\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0709 - acc: 0.9815 - val_loss: 0.4472 - val_acc: 0.8347\n",
      "Epoch 15/15\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0601 - acc: 0.9841 - val_loss: 0.4602 - val_acc: 0.8333\n",
      "------------------\n",
      "Train on 16220 samples, validate on 6526 samples\n",
      "Epoch 1/15\n",
      "Epoch 00001: val_loss improved from inf to 1.06131, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0772 - acc: 0.4100 - val_loss: 1.0613 - val_acc: 0.4260\n",
      "Epoch 2/15\n",
      "Epoch 00002: val_loss improved from 1.06131 to 0.90057, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9683 - acc: 0.5581 - val_loss: 0.9006 - val_acc: 0.5619\n",
      "Epoch 3/15\n",
      "Epoch 00003: val_loss improved from 0.90057 to 0.71372, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7338 - acc: 0.7023 - val_loss: 0.7137 - val_acc: 0.7113\n",
      "Epoch 4/15\n",
      "Epoch 00004: val_loss improved from 0.71372 to 0.58195, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5275 - acc: 0.8223 - val_loss: 0.5820 - val_acc: 0.7752\n",
      "Epoch 5/15\n",
      "Epoch 00005: val_loss improved from 0.58195 to 0.51125, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3868 - acc: 0.8766 - val_loss: 0.5113 - val_acc: 0.8003\n",
      "Epoch 6/15\n",
      "Epoch 00006: val_loss improved from 0.51125 to 0.47046, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2994 - acc: 0.9070 - val_loss: 0.4705 - val_acc: 0.8147\n",
      "Epoch 7/15\n",
      "Epoch 00007: val_loss improved from 0.47046 to 0.45272, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2395 - acc: 0.9248 - val_loss: 0.4527 - val_acc: 0.8183\n",
      "Epoch 8/15\n",
      "Epoch 00008: val_loss improved from 0.45272 to 0.44051, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1956 - acc: 0.9407 - val_loss: 0.4405 - val_acc: 0.8249\n",
      "Epoch 9/15\n",
      "Epoch 00009: val_loss improved from 0.44051 to 0.43459, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1630 - acc: 0.9517 - val_loss: 0.4346 - val_acc: 0.8270\n",
      "Epoch 10/15\n",
      "Epoch 00010: val_loss improved from 0.43459 to 0.43134, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1381 - acc: 0.9597 - val_loss: 0.4313 - val_acc: 0.8290\n",
      "Epoch 11/15\n",
      "Epoch 00011: val_loss did not improve\n",
      " - 1s - loss: 0.1176 - acc: 0.9657 - val_loss: 0.4329 - val_acc: 0.8305\n",
      "Epoch 12/15\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1000 - acc: 0.9716 - val_loss: 0.4376 - val_acc: 0.8301\n",
      "Epoch 13/15\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.0865 - acc: 0.9764 - val_loss: 0.4473 - val_acc: 0.8284\n",
      "Epoch 14/15\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0742 - acc: 0.9801 - val_loss: 0.4586 - val_acc: 0.8285\n",
      "Epoch 15/15\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0653 - acc: 0.9825 - val_loss: 0.4652 - val_acc: 0.8287\n",
      "------------------\n",
      "Train on 16145 samples, validate on 6526 samples\n",
      "Epoch 1/15\n",
      "Epoch 00001: val_loss improved from inf to 1.07362, saving model to /tmp/nn_model.h5\n",
      " - 3s - loss: 1.0783 - acc: 0.4116 - val_loss: 1.0736 - val_acc: 0.4219\n",
      "Epoch 2/15\n",
      "Epoch 00002: val_loss improved from 1.07362 to 0.89076, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.9790 - acc: 0.5404 - val_loss: 0.8908 - val_acc: 0.6076\n",
      "Epoch 3/15\n",
      "Epoch 00003: val_loss improved from 0.89076 to 0.67581, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.7254 - acc: 0.7341 - val_loss: 0.6758 - val_acc: 0.7548\n",
      "Epoch 4/15\n",
      "Epoch 00004: val_loss improved from 0.67581 to 0.55184, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.5160 - acc: 0.8279 - val_loss: 0.5518 - val_acc: 0.7951\n",
      "Epoch 5/15\n",
      "Epoch 00005: val_loss improved from 0.55184 to 0.48616, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3914 - acc: 0.8696 - val_loss: 0.4862 - val_acc: 0.8173\n",
      "Epoch 6/15\n",
      "Epoch 00006: val_loss improved from 0.48616 to 0.44122, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.3107 - acc: 0.8941 - val_loss: 0.4412 - val_acc: 0.8342\n",
      "Epoch 7/15\n",
      "Epoch 00007: val_loss improved from 0.44122 to 0.41567, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2555 - acc: 0.9141 - val_loss: 0.4157 - val_acc: 0.8431\n",
      "Epoch 8/15\n",
      "Epoch 00008: val_loss improved from 0.41567 to 0.39905, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.2127 - acc: 0.9333 - val_loss: 0.3991 - val_acc: 0.8462\n",
      "Epoch 9/15\n",
      "Epoch 00009: val_loss improved from 0.39905 to 0.38892, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1802 - acc: 0.9441 - val_loss: 0.3889 - val_acc: 0.8501\n",
      "Epoch 10/15\n",
      "Epoch 00010: val_loss improved from 0.38892 to 0.38444, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1527 - acc: 0.9532 - val_loss: 0.3844 - val_acc: 0.8501\n",
      "Epoch 11/15\n",
      "Epoch 00011: val_loss improved from 0.38444 to 0.38403, saving model to /tmp/nn_model.h5\n",
      " - 1s - loss: 0.1324 - acc: 0.9613 - val_loss: 0.3840 - val_acc: 0.8504\n",
      "Epoch 12/15\n",
      "Epoch 00012: val_loss did not improve\n",
      " - 1s - loss: 0.1119 - acc: 0.9692 - val_loss: 0.3862 - val_acc: 0.8511\n",
      "Epoch 13/15\n",
      "Epoch 00013: val_loss did not improve\n",
      " - 1s - loss: 0.0974 - acc: 0.9738 - val_loss: 0.3912 - val_acc: 0.8498\n",
      "Epoch 14/15\n",
      "Epoch 00014: val_loss did not improve\n",
      " - 1s - loss: 0.0845 - acc: 0.9782 - val_loss: 0.3986 - val_acc: 0.8501\n",
      "Epoch 15/15\n",
      "Epoch 00015: val_loss did not improve\n",
      " - 1s - loss: 0.0749 - acc: 0.9802 - val_loss: 0.4070 - val_acc: 0.8483\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def get_nn_feats(rnd=1):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    best_val_train_pred, best_val_test_pred = np.zeros((19579,3)),np.zeros((8392,3))\n",
    "    FEAT_CNT = 3\n",
    "    NUM_WORDS = 30000\n",
    "    N = 10\n",
    "    MAX_LEN = 100\n",
    "    NUM_CLASSES = 3\n",
    "    MODEL_P = '/tmp/nn_model.h5'\n",
    "    \n",
    "    tmp_X = org_X\n",
    "    tmp_Y = org_Y\n",
    "    tmp_X_test = test_df['text']\n",
    "    \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    tmp_Y = lb.fit_transform(tmp_Y)\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(tmp_X)\n",
    "    \n",
    "    # test\n",
    "    final_test_x = tokenizer.texts_to_sequences(tmp_X_test)\n",
    "    final_test_x = pad_sequences(final_test_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    # train + valid\n",
    "    all_train_x = tokenizer.texts_to_sequences(tmp_X)\n",
    "    all_train_x = pad_sequences(all_train_x, maxlen=MAX_LEN)\n",
    "    \n",
    "    kf = KFold(n_splits=FEAT_CNT, shuffle=True, random_state=233*rnd)\n",
    "    for train_index, test_index in kf.split(tmp_X):\n",
    "        # prepare aug train, val\n",
    "        aug_X, aug_Y = text_aug(tmp_X[train_index],tmp_Y[train_index])\n",
    "        val_X, val_Y = all_train_x[test_index],tmp_Y[test_index]\n",
    "        \n",
    "        curr_train_x = tokenizer.texts_to_sequences(aug_X)\n",
    "        curr_train_x = pad_sequences(curr_train_x, maxlen=MAX_LEN)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(NUM_WORDS, N, input_length=MAX_LEN))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #model.summary()\n",
    "\n",
    "        model_chk = ModelCheckpoint(filepath=MODEL_P, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        model.fit(curr_train_x, aug_Y, \n",
    "                  validation_data=(val_X,val_Y),\n",
    "                  batch_size=64, epochs=15, \n",
    "                  verbose=2,\n",
    "                  callbacks=[model_chk],\n",
    "                  shuffle=False\n",
    "                 )\n",
    " \n",
    "        # save feat\n",
    "        train_pred[test_index] = model.predict(val_X)\n",
    "        test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        # best val model\n",
    "        model = load_model(MODEL_P)\n",
    "        best_val_train_pred[test_index] = model.predict(val_X)\n",
    "        best_val_test_pred += model.predict(final_test_x)/FEAT_CNT\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred,test_pred,best_val_train_pred,best_val_test_pred\n",
    "\n",
    "print('def cnn done')\n",
    "\n",
    "nn_train1,nn_test1,nn_train2,nn_test2 = get_nn_feats(4)\n",
    "\n",
    "\n",
    "\n",
    "all_nn_train = np.hstack([lstm_train1, lstm_train2, \n",
    "                          cnn_train1, cnn_train2,\n",
    "                          nn_train1,nn_train2\n",
    "                         ])\n",
    "all_nn_test = np.hstack([lstm_test1, lstm_test2, \n",
    "                         cnn_test1, cnn_test2,\n",
    "                         nn_test1,nn_test2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nn_feat.pkl','wb') as fout:\n",
    "    pickle.dump([all_nn_train,all_nn_test],fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
