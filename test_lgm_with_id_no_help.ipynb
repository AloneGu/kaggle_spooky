{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26305]\n",
      " [17569]\n",
      " [11008]\n",
      " [27763]\n",
      " [12958]]\n",
      "[[ 2310]\n",
      " [24541]\n",
      " [  134]\n",
      " [27757]\n",
      " [ 4081]]\n",
      "(19579, 1866)\n",
      "(19579, 1867)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"./input/train.csv\")\n",
    "author_mapping_dict = {'EAP':0, 'HPL':1, 'MWS':2}\n",
    "train_y = train_df['author'].map(author_mapping_dict)\n",
    "train_Y = train_y\n",
    "test_df = pd.read_csv(\"./input/test.csv\")\n",
    "\n",
    "get_id = lambda x:int(str(x)[2:])\n",
    "\n",
    "train_id_feat = train_df['id'].values\n",
    "train_id_feat = np.array([get_id(x) for x in train_id_feat]).reshape(-1,1)\n",
    "print(train_id_feat[:5])\n",
    "t_id_feat = test_df['id'].values\n",
    "t_id_feat = np.array([get_id(x) for x in t_id_feat]).reshape(-1,1)\n",
    "print(t_id_feat[:5])\n",
    "\n",
    "\n",
    "with open('feat.pkl','rb') as fin:\n",
    "    f_train_X,f_test_X = pickle.load(fin)\n",
    "print(f_train_X.shape)\n",
    "\n",
    "f_train_X = np.hstack([f_train_X,train_id_feat])\n",
    "f_test_X = np.hstack([f_test_X,t_id_feat])\n",
    "print(f_train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "def cv_test(k_cnt=3, s_flag = False):\n",
    "    rnd = 420\n",
    "    if s_flag:\n",
    "        kf = StratifiedKFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k_cnt, shuffle=True, random_state=rnd)\n",
    "    test_pred = None\n",
    "    weighted_test_pred = None\n",
    "    org_train_pred = None\n",
    "    avg_k_score = 0\n",
    "    reverse_score = 0\n",
    "    best_loss = 100\n",
    "    best_single_pred = None\n",
    "    for train_index, test_index in kf.split(f_train_X,train_Y):\n",
    "        X_train, X_test = f_train_X[train_index], f_train_X[test_index]\n",
    "        y_train, y_test = train_Y[train_index], train_Y[test_index]\n",
    "        \n",
    "        dtx = lgb.Dataset(X_train, label=y_train)\n",
    "        dtv = lgb.Dataset(X_test, label=y_test)\n",
    "        \n",
    "        params = {\n",
    "            'learning_rate':0.04,'max_depth':4,'objective':'multiclass',\n",
    "            'num_class':3,'metric':{'multi_logloss'},\n",
    "            'feature_fraction':0.8,\n",
    "            'bagging_fraction':0.7,\n",
    "            'lambda_l2':1.0\n",
    "        }\n",
    "        \n",
    "        m = lgb.train(params, train_set=dtx, valid_sets=dtv, valid_names=['val'],\n",
    "                      num_boost_round=1000,\n",
    "                      early_stopping_rounds=50,\n",
    "                      verbose_eval=200)\n",
    "        \n",
    "        # get res\n",
    "        train_pred = m.predict(X_train)\n",
    "        print(train_pred.shape,y_train.shape)\n",
    "        valid_pred = m.predict(X_test)\n",
    "        tmp_train_pred = m.predict(f_train_X)\n",
    "        \n",
    "        # cal score\n",
    "        train_score = log_loss(y_train,train_pred)\n",
    "        valid_score = log_loss(y_test,valid_pred)\n",
    "        print('train log loss',train_score,'valid log loss',valid_score)\n",
    "        avg_k_score += valid_score\n",
    "        rev_valid_score = 1.0/valid_score # use for weighted\n",
    "        reverse_score += rev_valid_score # sum\n",
    "        print('rev',rev_valid_score)\n",
    "        \n",
    "        if test_pred is None:\n",
    "            test_pred = m.predict(f_test_X)\n",
    "            weighted_test_pred = test_pred*rev_valid_score\n",
    "            org_train_pred = tmp_train_pred\n",
    "            best_loss = valid_score\n",
    "            best_single_pred = test_pred\n",
    "        else:\n",
    "            curr_pred = m.predict(f_test_X)\n",
    "            test_pred += curr_pred\n",
    "            weighted_test_pred += curr_pred*rev_valid_score # fix bug here\n",
    "            org_train_pred += tmp_train_pred\n",
    "            # find better single model\n",
    "            if valid_score < best_loss:\n",
    "                print('BETTER')\n",
    "                best_loss = valid_score\n",
    "                best_single_pred = curr_pred\n",
    "\n",
    "    # avg\n",
    "    test_pred = test_pred / k_cnt\n",
    "    print(test_pred.shape)\n",
    "    print(test_pred[:5])\n",
    "    org_train_pred = org_train_pred / k_cnt\n",
    "    avg_k_score = avg_k_score/k_cnt\n",
    "\n",
    "\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    submiss['EAP']=test_pred[:,0]\n",
    "    submiss['HPL']=test_pred[:,1]\n",
    "    submiss['MWS']=test_pred[:,2]\n",
    "    submiss.to_csv(\"results/lgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('--------------')\n",
    "    print(reverse_score)\n",
    "    # weigthed\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = weighted_test_pred / reverse_score\n",
    "    weighted_test_pred = np.round(weighted_test_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/weighted_lgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    # best single\n",
    "    submiss=pd.read_csv(\"./input/sample_submission.csv\")\n",
    "    weighted_test_pred = np.round(best_single_pred,4)\n",
    "    submiss['EAP']=weighted_test_pred[:,0]\n",
    "    submiss['HPL']=weighted_test_pred[:,1]\n",
    "    submiss['MWS']=weighted_test_pred[:,2]\n",
    "    submiss.to_csv(\"results/single_lgb_res_{}_{}.csv\".format(k_cnt, s_flag),index=False)\n",
    "    print(submiss.head(5))\n",
    "    print('---------------')\n",
    "    \n",
    "    # train log loss\n",
    "    print('local average valid loss',avg_k_score)\n",
    "    print('train log loss', log_loss(train_Y,org_train_pred))\n",
    "    \n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cv_test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tval's multi_logloss: 0.326597\n",
      "[400]\tval's multi_logloss: 0.275389\n",
      "[600]\tval's multi_logloss: 0.266664\n",
      "[800]\tval's multi_logloss: 0.264719\n",
      "Early stopping, best iteration is:\n",
      "[894]\tval's multi_logloss: 0.26403\n",
      "(15663, 3) (15663,)\n",
      "train log loss 0.184413221587 valid log loss 0.265930216582\n",
      "rev 3.76038500947\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tval's multi_logloss: 0.332295\n",
      "[400]\tval's multi_logloss: 0.281923\n",
      "[600]\tval's multi_logloss: 0.272177\n",
      "[800]\tval's multi_logloss: 0.269532\n",
      "Early stopping, best iteration is:\n",
      "[782]\tval's multi_logloss: 0.269387\n",
      "(15663, 3) (15663,)\n",
      "train log loss 0.19144559898 valid log loss 0.271608736538\n",
      "rev 3.68176669406\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tval's multi_logloss: 0.330922\n",
      "[400]\tval's multi_logloss: 0.278269\n",
      "[600]\tval's multi_logloss: 0.268844\n",
      "[800]\tval's multi_logloss: 0.266519\n",
      "Early stopping, best iteration is:\n",
      "[894]\tval's multi_logloss: 0.265104\n",
      "(15663, 3) (15663,)\n",
      "train log loss 0.185229929214 valid log loss 0.2673422486\n",
      "rev 3.74052363679\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tval's multi_logloss: 0.332334\n",
      "[400]\tval's multi_logloss: 0.280577\n",
      "[600]\tval's multi_logloss: 0.271618\n",
      "[800]\tval's multi_logloss: 0.268786\n",
      "Early stopping, best iteration is:\n",
      "[894]\tval's multi_logloss: 0.267501\n",
      "(15663, 3) (15663,)\n",
      "train log loss 0.183447882722 valid log loss 0.269706804564\n",
      "rev 3.70772996113\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[200]\tval's multi_logloss: 0.326364\n",
      "[400]\tval's multi_logloss: 0.272051\n",
      "[600]\tval's multi_logloss: 0.261647\n",
      "[800]\tval's multi_logloss: 0.258033\n",
      "Early stopping, best iteration is:\n",
      "[894]\tval's multi_logloss: 0.256063\n",
      "(15664, 3) (15664,)\n",
      "train log loss 0.18675148451 valid log loss 0.258796568159\n",
      "rev 3.86403887467\n",
      "BETTER\n",
      "(8392, 3)\n",
      "[[ 0.02234406  0.00637025  0.97128569]\n",
      " [ 0.99510355  0.0026961   0.00220034]\n",
      " [ 0.00618748  0.99102694  0.00278558]\n",
      " [ 0.78070736  0.20817982  0.01111282]\n",
      " [ 0.73068558  0.17030886  0.09900556]]\n",
      "        id       EAP       HPL       MWS\n",
      "0  id02310  0.022344  0.006370  0.971286\n",
      "1  id24541  0.995104  0.002696  0.002200\n",
      "2  id00134  0.006187  0.991027  0.002786\n",
      "3  id27757  0.780707  0.208180  0.011113\n",
      "4  id04081  0.730686  0.170309  0.099006\n",
      "--------------\n",
      "18.7544441761\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0223  0.0064  0.9713\n",
      "1  id24541  0.9951  0.0027  0.0022\n",
      "2  id00134  0.0062  0.9910  0.0028\n",
      "3  id27757  0.7806  0.2083  0.0111\n",
      "4  id04081  0.7309  0.1701  0.0990\n",
      "---------------\n",
      "        id     EAP     HPL     MWS\n",
      "0  id02310  0.0171  0.0075  0.9754\n",
      "1  id24541  0.9937  0.0039  0.0024\n",
      "2  id00134  0.0068  0.9899  0.0033\n",
      "3  id27757  0.7578  0.2309  0.0113\n",
      "4  id04081  0.7606  0.1397  0.0998\n",
      "---------------\n",
      "local average valid loss 0.266676914889\n",
      "train log loss 0.196975672887\n"
     ]
    }
   ],
   "source": [
    "cv_test(5,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
